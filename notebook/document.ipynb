{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4dadc6",
   "metadata": {},
   "source": [
    "Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795b821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53111a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Krish Naik', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc=Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"exmaple.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Krish Naik\",\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b0e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85170c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6d3201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Gen AI Projects\\RAG 1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\n",
    "    \"../data/text_files/python_intro.txt\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4f9b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7eaa030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='arXiv:2505.18366v1  [cs.IR]  23 May 2025\\nAccepted in ACL 2025\\nHard Negative Mining for Domain-Specific Retrieval in Enterprise Systems\\nHansa Meghwani*, Amit Agarwal*, Priyaranjan Pattnayak,\\nHitesh Laxmichand Patel, Srikant Panda\\nOracle AI\\nCorrespondence: hansa.meghwani@oracle.com; amit.h.agarwal@oracle.com *\\nAbstract\\nEnterprise search systems often struggle to re-\\ntrieve accurate, domain-specific information\\ndue to semantic mismatches and overlapping\\nterminologies. These issues can degrade the\\nperformance of downstream applications such\\nas knowledge management, customer support,\\nand retrieval-augmented generation agents. To\\naddress this challenge, we propose a scal-\\nable hard-negative mining framework tailored\\nspecifically for domain-specific enterprise data.\\nOur approach dynamically selects semantically\\nchallenging but contextually irrelevant docu-\\nments to enhance deployed re-ranking models.\\nOur method integrates diverse embedding mod-\\nels, performs dimensionality reduction, and\\nuniquely selects hard negatives, ensuring com-\\nputational efficiency and semantic precision.\\nEvaluation on our proprietary enterprise corpus\\n(cloud services domain) demonstrates substan-\\ntial improvements of 15% in MRR@3 and 19%\\nin MRR@10 compared to state-of-the-art base-\\nlines and other negative sampling techniques.\\nFurther validation on public domain-specific\\ndatasets (FiQA, Climate Fever, TechQA) con-\\nfirms our method’s generalizability and readi-\\nness for real-world applications.\\n1\\nIntroduction\\nAccurate retrieval of domain-specific information\\nsignificantly impacts critical enterprise processes,\\nsuch as knowledge management, customer sup-\\nport, and Retrieval Augmented Generation (RAG)\\nAgents. However, achieving precise retrieval re-\\nmains challenging due to semantic mismatches,\\noverlapping terminologies, and ambiguous abbre-\\nviations common in specialized fields like finance,\\nand cloud computing. Traditional lexical retrieval\\ntechniques, such as BM25 (Robertson and Walker,\\n1994), struggle due to vocabulary mismatches, lead-\\ning to irrelevant results and poor user experience.\\n*The authors contributed equally to this work.\\nRecent dense retrieval approaches leveraging\\npre-trained language models, like BERT-based en-\\ncoders (Karpukhin et al., 2020; Xiong et al., 2020;\\nGuu et al., 2020), mitigate lexical limitations by\\ncapturing semantic relevance. Nevertheless, their\\nperformance heavily relies on the negative sam-\\nples—documents incorrectly retrieved due to se-\\nmantic similarity but lacking contextual relevance.\\nModels trained with negative sampling methods\\n(e.g., random sampling, BM25-based static sam-\\npling, or dynamic methods like ANCE (Xiong\\net al., 2020), STAR (Zhan et al., 2021)) either\\nlack sufficient semantic discrimination or incur\\nhigh computational costs, thus limiting scalability\\nand practical enterprise deployment. For instance,\\ngiven a query such as \"Steps to deploy a MySQL\\ndatabase on Cloud Infrastructure,\" most negative\\nsampling techniques select documents discussing\\nnon-MySQL database deployments. Conversely,\\nour method strategically selects a hard negative dis-\\ncussing MySQL deployment on-premises, which\\ndespite semantic overlap, is contextually distinct\\nand thus poses a stronger training challenge for the\\nretrieval and re-ranking models.\\nOur proposed framework addresses these by in-\\ntroducing a novel semantic selection criterion ex-\\nplicitly designed to curate high-quality hard nega-\\ntives. By uniquely formulating two semantic con-\\nditions that effectively select negatives that closely\\nresemble query semantics but remain contextually\\nirrelevant, significantly minimizing false negatives\\nencountered by existing techniques. The main con-\\ntributions of this paper are:\\n1. A negative mining framework for dynamically\\nselecting semantically challenging hard neg-\\natives, leveraging diverse embedding models\\nand semantic filtering criteria to significantly\\nimprove re-ranking models in domain-specific\\nretrieval scenarios.\\n2. Comprehensive evaluations demonstrating'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Accepted in ACL 2025\\nconsistent and significant improvements\\nacross both proprietary and publicly available\\ndatasets, verifying our method’s impact and\\nbroad applicability across domain-specific\\nusecases.\\n3. In-depth analysis, of critical challenges in han-\\ndling both short and long-form enterprise doc-\\numents, laying a clear foundation for targeted\\nfuture improvements.\\nOur work directly enhances the semantic dis-\\ncrimination capabilities of re-ranking models, re-\\nsulting in 15% improvement in MRR@3 and\\n19% improvement in MRR@10 on our in-house\\ncloud-services domain dataset. Further evaluations\\non public domain-specific benchmarks (FiQA, Cli-\\nmate Fever, TechQA) confirm generalizability and\\ntangible improvements of our proposed negative\\nmining framework.\\n2\\nRelated Work\\n2.1\\nHard Negatives in Retrieval Models\\nThe role of hard negatives in training dense re-\\ntrieval models has been widely studied.\\nStatic\\nnegatives, such as BM25 (Robertson and Walker,\\n1994), provide lexical similarity but fail to capture\\nsemantic relevance, often leading to overfitting (Qu\\net al., 2020). Dynamic negatives, introduced in\\nANCE (Xiong et al., 2020) and STAR (Zhan et al.,\\n2021), adapt during training to provide more chal-\\nlenging contrasts but require significant computa-\\ntional resources due to periodic re-indexing. Our\\nframework addresses these limitations by dynam-\\nically identifying semantically challenging nega-\\ntives using clustering and dimensionality reduction,\\nensuring scalability and adaptability.\\nFurther studies have explored advanced meth-\\nods for negative sampling in cross-encoder mod-\\nels (Meghwani, 2024). Localized Contrastive Es-\\ntimation (LCE) (Guo et al., 2023) integrates hard\\nnegatives into cross-encoder training, improving\\nthe reranking performance when negatives align\\nwith the output of the retriever. Similarly, (Pradeep\\net al., 2022) demonstrated the importance of hard\\nnegatives even when models undergo advanced pre-\\ntraining techniques, such as condenser (Gao and\\nCallan, 2021). Our work builds on these efforts by\\noffering a scalable approach, which can be applied\\nto any domain-heavy enterprise data.\\n2.2\\nNegative Sampling Strategies\\nEffective negative sampling significantly affects the\\nperformance of the retrieval model by challenging\\nthe model to differentiate between relevant and\\nirrelevant examples. Common strategies include:\\n• Random Negatives: Efficient but lacking se-\\nmantic contrast, leading to suboptimal perfor-\\nmance (Karpukhin et al., 2020).\\n• BM25 Negatives: Leverage lexical similar-\\nity, but often introduce biases, particularly\\nin semantically rich domains (Robertson and\\nWalker, 1994).\\n• In-Batch Negatives:\\nComputationally ef-\\nficient but limited to local semantic con-\\ntrasts, often underperforming in dense re-\\ntrieval tasks (Xiong et al., 2020).\\nOur framework complements these approaches\\nby dynamically generating negatives that balance\\nsemantic similarity and contextual irrelevance,\\navoiding the pitfalls of static or random methods.\\n2.3\\nDomain-Specific Retrieval Challenges\\nEnterprise retrieval systems face unique challenges,\\nsuch as ambiguous terminology, overlapping con-\\ncepts, and private datasets (Meghwani, 2024).\\nGeneral-purpose methods such as BM25 or dense\\nretrieval models (Qu et al., 2020) fail to capture\\ndomain-specific complexities effectively. Our ap-\\nproach addresses these gaps by curating hard nega-\\ntives that align with enterprise-specific semantics,\\nimproving retrieval precision and robustness for\\nproprietary datasets.\\nWe further discuss negative sampling techniques in\\nAppendix A.1.\\n3\\nMethodology\\nTo effectively train and finetune reranker models\\nfor domain-specific retrieval, it is essential to sys-\\ntematically handle technical ambiguities stemming\\nfrom specialized terminologies, overlapping con-\\ncepts, and abbreviations prevalent within enterprise\\ndomains.\\nWe propose a structured, modular framework\\nthat integrates diverse embedding models, dimen-\\nsionality reduction, and a novel semantic criterion\\nfor hard-negative selection. Figure 1 illustrates the\\nhigh-level pipeline, components and their interac-\\ntions. The re-ranking models fine-tuned using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Accepted in ACL 2025\\nFigure 1: Overview of the methodology pipeline for training reranker models, including embedding generation,\\nPCA-based dimensionality reduction and hard negative selection for fine-tuning.\\nhard negatives generated by our framework are di-\\nrectly deployed in downstream applications, such\\nas RAG, significantly improving the resolution of\\ncustomer queries through enhanced retrieval.\\nOur approach begins by encoding queries and\\ndocuments into semantically rich vector represen-\\ntations using an ensemble of state-of-the-art bi-\\nencoder embedding models. These embeddings are\\nstrategically selected based on multilingual sup-\\nport, embedding quality, training data diversity,\\ncontext length handling, and performance (details\\nprovided in Appendix A.2. To manage embed-\\nding dimensionality and improve computational\\nefficiency, Principal Component Analysis (PCA)\\n(Ma´ckiewicz and Ratajczak, 1993) is utilized to\\nproject the concatenated embeddings onto a lower-\\ndimensional space, maintaining 95% of the original\\nvariance.\\nWe then define two semantic conditions (Eq. 5\\nand Eq. 6) to dynamically select high-quality hard\\nnegatives, addressing semantic similarity chal-\\nlenges and minimizing false negatives. Together,\\nthese two equations ensure that the selected hard\\nnegative is not only close to the query (Eq. 5) but\\nalso contextually distinct from the true positive,\\nminimizing the risk of selecting topic duplicates\\nor noisy positives (Eq. 6). For example, a query\\nabout deploying MySQL on Oracle Cloud, PD is a\\nguide on that topic, and D is a doc about MySQL\\non-premise — semantically close to Q, but distant\\nfrom PD.\\nBelow we detail each methodological compo-\\nnent, emphasizing their contributions to enhancing\\nretrieval precision in domain-specific or enterprise\\nretrieval tasks.\\nTotal\\nTrain\\nTest\\n< Q, PD >\\n5250\\n1000\\n4250\\nTable 1: Dataset distribution of queries (Q) and positive\\ndocuments (PD).\\n3.1\\nDataset Statistics\\nOur experiments leverage a proprietary corpus con-\\ntaining 36,871 unannotated documents sourced\\nfrom over 30 enterprise cloud services. Addition-\\nally, we prepared 5250 annotated query-positive\\ndocument pairs (< Q, PD >) for training and\\ntesting. Notably, we adopted a non-standard train-\\ntest split (as summarized in Table 1), allocating\\nfour times more data to testing than training to\\nrigorously evaluate model robustness against vary-\\ning training data volumes (additional analyses in\\nAppendix A.4).\\nTo further validate generaliz-\\nability, we conduct evaluations on publicly avail-\\nable domain-specific benchmarks: FiQA (finance)\\n(TheFinAI, 2018), Climate Fever (climate science)\\n(Diggelmann et al., 2021), and TechQA (technol-\\nogy) (Castelli et al., 2019). Detailed dataset statis-\\ntics are provided in Appendix A.2.1.\\n3.2\\nEmbedding Generation\\nEmbeddings for queries, positive documents, and\\nthe corpus are computed via six diverse, high-\\nperformance bi-encoder models E1, E2, . . . , E6,\\neach selected strategically for capturing comple-\\nmentary semantic perspectives:\\nEk(x) ∈Rdk\\n(1)\\nwhere dk is the embedding dimension of the kth\\nmodel for textual input x. Concatenation of these'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Accepted in ACL 2025\\nembeddings yields a comprehensive representation:\\nXconcat = [e1(x); e2(x); . . . ; e6(x)]\\n(2)\\nwhere Xconcat ∈R\\nP6\\nk=1 dk represents the con-\\ncatenated embedding for the input x.\\n3.3\\nDimensionality Reduction\\nTo alleviate the computational overhead arising\\nfrom high-dimensional concatenated embeddings,\\nwe apply PCA to reduce dimensionality while pre-\\nserving semantic richness:\\nXPCA = XconcatP,\\n(3)\\nwhere P represents the PCA projection matrix.\\nWe specifically select PCA due to its computational\\nefficiency, and scalability, essential given our large\\nenterprise corpus and high-dimensional embedding\\nspace. While we empirically evaluated nonlinear\\ndimensionality reduction methods such as UMAP\\n(McInnes et al., 2020) and t-SNE (Van der Maaten\\nand Hinton, 2008), they offered negligible perfor-\\nmance improvements over PCA but incurred sub-\\nstantially higher computational costs, making them\\nimpractical for deployment at scale in enterprise\\nsystems.\\n3.4\\nHard Negative Selection Criteria\\nWe propose two semantic criteria to identify high-\\nquality hard negatives. PCA-reduced embeddings\\nXPCA are organized around each query Q. For each\\nquery-positive document pair (Q, PD), candidate\\ndocuments D from the corpus are evaluated via\\ncosine distances:\\nd(Q, PD),\\nd(Q, D),\\nd(PD, D)\\n(4)\\nA document D is selected as a hard negative\\nonly if it satisfies both criteria:\\nd(Q, D) < d(Q, PD)\\n(5)\\nd(Q, D) < d(PD, D)\\n(6)\\nEquation (5) ensures that the candidate negative\\ndocument is semantically closer to the query than\\nthe actual positive document, making it a challeng-\\ning negative example that potentially confuses the\\nreranking model. Equation (6), ensures that the se-\\nlected hard negative is not just query-confusing but\\nalso sufficiently dissimilar from the actual positive\\n(avoiding near-duplicates or false negatives).\\nThe candidate document DHN with minimal\\nd(Q, D) satisfying these conditions is chosen as\\nthe primary hard negative. Additional hard nega-\\ntives can similarly be selected based on semantic\\nproximity rankings.\\nFigure 2: Hard negative selection on the first two PCA\\ncomponents (78% variance). Q act as centroids, PD\\nguide selection of hard negatives; which are chosen\\nbased on semantic proximity.\\nFigure 2 illustrates an example embedding space,\\nclearly depicting the query Q, positive document\\nPD, and selected hard negative DHN, visualizing\\nthe semantic selection criteria. In cases where no\\ndocuments satisfy these conditions, no hard nega-\\ntives are selected for that particular query. Further\\ndetails on our embedding model & fine-tuning us-\\ning these hard negatives are provided in Appendix\\nA.2.\\n4\\nExperiments & Results\\nTo evaluate the effectiveness of our proposed hard-\\nnegative selection framework, we conduct exten-\\nsive experiments on our internal cloud-specific en-\\nterprise dataset, as well as domain-specific open-\\nsource benchmarks. We systematically compare\\nour approach against multiple competitive negative\\nsampling methods and perform detailed ablation\\nstudies to understand the contribution of individual\\nframework components. Complete details on exper-\\nimental setups and hyperparameters are provided\\nin Appendix A.3.\\n4.1\\nResults & Discussion\\nComparative Analysis of Negative Sampling\\nStrategies\\nTable 3 presents a detailed compar-\\nison of of our negative sampling technique against\\nseveral established methods, including Random,\\nBM25, In-batch, STAR, and ADORE+STAR. The'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Accepted in ACL 2025\\nRe-ranker (Fine-tuned w/)\\nInternal\\nFiQA\\nClimate-FEVER\\nTechQA\\nMRR@3\\nMRR@10\\nMRR@3\\nMRR@10\\nMRR@3\\nMRR@10\\nMRR@3\\nMRR@10\\nBaseline (No Fine-tuning)\\n0.42\\n0.45\\n0.45\\n0.48\\n0.44\\n0.46\\n0.57\\n0.61\\nIn-batch Negatives\\n0.47\\n0.52\\n0.46\\n0.52\\n0.44\\n0.47\\n0.57\\n0.62\\nSTAR\\n0.53\\n0.56\\n0.51\\n0.54\\n0.47\\n0.49\\n0.61\\n0.63\\nADORE+STAR\\n0.54\\n0.57\\n0.52\\n0.54\\n0.48\\n0.52\\n0.63\\n0.66\\nOur Proposed HN\\n0.57\\n0.64\\n0.54\\n0.56\\n0.52\\n0.55\\n0.65\\n0.69\\nTable 2: Comparative performance benchmarking of our in-house reranker across multiple domain-specific datasets.\\nThe reranker is fine-tuned (FT) with different negative sampling techniques, highlighting the effectiveness of our\\nproposed hard-negative mining method (HN).\\nNegative Sampling Method\\nMRR@3\\nMRR@10\\nBaseline\\n0.42\\n0.45\\nFT with Random Neg\\n0.47\\n0.51\\nFT with BM25 Neg\\n0.49\\n0.54\\nFT with In-batch Neg\\n0.47\\n0.52\\nFT with BM25+In-batch Neg\\n0.52\\n0.54\\nFT with STAR\\n0.53\\n0.56\\nFT with ADORE+STAR\\n0.54\\n0.57\\nFT with our HN\\n0.57\\n0.64\\nTable 3: Comparison of negative sampling methods for\\nfine-tuning(FT) in-house cross-encoder reranker model.\\nThe proposed framework achieves 15% and 19% im-\\nprovements in MRR@3 and MRR@10, respectively,\\nover baseline methods.\\nbaseline is defined as the performance of our inter-\\nnal reranker model without any fine-tuning. Our\\nmethod achieves notable relative improvements of\\n15% in MRR@3 and 19% in MRR@10 over this\\nbaseline. The semantic nature of our hard nega-\\ntives allows the reranker to distinguish contextually\\nirrelevant but semantically similar documents effec-\\ntively. In contrast, simpler baselines like Random\\nor BM25 negatives suffer due to no semantic con-\\nsideration, while advanced methods like STAR and\\nADORE+STAR occasionally miss subtle seman-\\ntic nuances that our formulated selection criteria\\naddress effectively.\\nGeneralization Across Open-source Models\\nTo\\nvalidate the robustness and versatility of our frame-\\nwork, we evaluated various open-source embed-\\nding and reranker models (Table 4), clearly demon-\\nstrating improvements across all models when fine-\\ntuned using our proposed negative sampling com-\\npared to ADORE+STAR and baseline (no fine-\\ntuning). Notably, rerankers with multilingual ca-\\npabilities, such as the BGE-Reranker and Jina\\nReranker, demonstrated pronounced improvements,\\nlikely benefiting from our embedding ensemble’s\\nmultilingual semantic richness. Similarly, larger\\nmodels like e5-mistral exhibit significant gains, re-\\nflecting their capacity to exploit nuanced semantic\\ndifferences provided by our negative samples. This\\nanalysis underscores the general applicability and\\nmodel-agnostic benefits of our approach.\\nModel\\nBaseline\\nADORE+STAR\\nOurs\\nAlibaba-NLP\\n(gte-multilingual-reranker-base)\\n0.39\\n0.42\\n0.45\\nBGE-Reranker\\n(bge-reranker-large)\\n0.44\\n0.47\\n0.52\\nCohere Embed English Light\\n(Cohere-embed-english-light-v3.0)\\n0.32\\n0.34\\n0.38\\nCohere Embed Multilingual\\n(Cohere-embed-multilingual-v3.0)\\n0.34\\n0.37\\n0.40\\nCohere Reranker\\n(rerank-multilingual-v2.0)\\n0.42\\n0.45\\n0.49\\nIBM Reranker\\n(re2g-reranker-nq)\\n0.40\\n0.43\\n0.46\\nInfloat Reranker\\n(e5-mistral-7b-instruct)\\n0.35\\n0.38\\n0.42\\nJina Reranker v2\\n(jina-reranker-v2-base-multilingual)\\n0.45\\n0.48\\n0.53\\nMS-MARCO\\n(ms-marco-MiniLM-L-6-v2)\\n0.41\\n0.43\\n0.46\\nNomic AI Embed Text\\n(nomic-embed-text-v1.5)\\n0.33\\n0.36\\n0.39\\nNVIDIA\\nNV-Embed-v2\\n0.38\\n0.41\\n0.44\\nSalesforce\\nSFR-Embedding-2_R\\n0.37\\n0.40\\n0.43\\nSalesforce\\nSFR-Embedding-Mistral\\n0.36\\n0.39\\n0.42\\nT5-Large\\n0.41\\n0.44\\n0.47\\nTable 4: Performance benchmarking (MRR@3) of\\nreranker and embedding models using the proposed\\nhard negative selection framework, compared with\\nADORE+STAR and baseline methods.\\nEffectiveness\\non\\nDomain-specific\\nPublic\\nDatasets\\nWe\\nfurther\\ntested\\nour\\nmethod’s\\nadaptability across diverse public domain-specific\\ndatasets (FiQA, Climate-FEVER, TechQA), as\\nshown in Table 2. Each dataset presents distinct\\nretrieval challenges, ranging from technical jargon\\nin TechQA to complex domain-specific reasoning\\nin Climate-FEVER. Fine-tuning with our generated\\nhard negatives consistently improved retrieval\\nacross these varied datasets.\\nFiQA exhibited\\nsignificant gains, likely due to the semantic\\ndifferentiation required in finance-specific queries.\\nThese results demonstrate that our negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Accepted in ACL 2025\\nsampling method is not only effective within our\\ninternal enterprise corpus but also valuable across\\ndiverse, domain-specific public datasets, indicating\\nbroad applicability and domain independence.\\nModel\\nMRR@3\\nMRR@10\\nShort Documents\\nBaseline\\n0.481\\n0.526\\nFT w/ proposed HN\\n0.61\\n0.662\\nLong Documents\\nBaseline\\n0.423\\n0.477\\nFT w/ proposed HN\\n0.475\\n0.521\\nTable 5:\\nPerformance comparison of the in-house\\nreranker without fine-tuning (Baseline) versus fine-\\ntuned (FT) with our proposed hard negatives (HN), eval-\\nuated separately on short and long documents.\\nPerformance Analysis on Short vs. Long Docu-\\nments\\nAn explicit analysis of short versus long\\ndocuments (Table 5) revealed differential perfor-\\nmance gains. Short documents (under 1024 to-\\nkens) experienced substantial performance im-\\nprovements (MRR@3 improving from 0.481 to\\n0.61), attributed to minimal semantic redundancy\\nand tokenization constraints.\\nConversely, long\\ndocuments showed more moderate improvements\\n(MRR@3 from 0.423 to 0.475), primarily due to\\nembedding truncation that causes loss of context\\nand increased semantic complexity.\\nFuture re-\\nsearch should focus explicitly on developing hi-\\nerarchical or segment-based embedding methods\\nto address these limitations.\\nAblation Studies\\nTo clearly understand the im-\\npact of the individual components of the frame-\\nwork, we conducted systematic ablation studies\\n(Table 6). Training with positive documents alone\\nproduced only slight gains (+0.03 MRR@3), reaf-\\nfirming the critical role of high-quality hard nega-\\ntives. Evaluating individual embedding models sep-\\narately indicated varying performance due to their\\ndiffering semantic representations and underlying\\ntraining. However, the concatenation of diverse\\nembeddings provided significant performance im-\\nprovements (+0.15 MRR@3), clearly highlighting\\nthe advantages of capturing semantic diversity.\\nAdditionally, PCA-based dimensionality reduc-\\ntion analysis identified the optimal variance thresh-\\nold at 95%. Lower thresholds resulted in marked\\nsemantic degradation, reducing retrieval perfor-\\nmance. This trade-off highlights PCA as an essen-\\ntial efficiency-enhancing step for the framework.\\nCollectively, these detailed analyses underscore\\nour method’s strengths, limitations, and method-\\nological rationale, providing clear empirical justifi-\\ncation for each design decision.\\n#\\nProposed Strategies\\nMRR@3\\nMRR@10\\n1\\nBaseline\\n0.42\\n0.45\\nPositive Document (PD) Only\\n2\\nFine-tuning with PD Only\\n0.45\\n0.51\\nHard Negative(HN) with Embedding Ek\\n3a\\nHN with E1 + PD\\n0.45\\n0.51\\n3b\\nHN with E2 + PD\\n0.47\\n0.53\\n3c\\nHN with E3 + PD\\n0.51\\n0.55\\n3d\\nHN with E4 + PD\\n0.45\\n0.52\\n3e\\nHN with E5 + PD\\n0.48\\n0.51\\n3f\\nHN with E6 + PD\\n0.49\\n0.52\\n3g\\nHN with Xconcat + PD\\n0.57\\n0.64\\nXPCA Variance Impact + PD\\n4a\\nHN with XPCA (99% Variance)\\n0.57\\n0.64\\n4b\\nHN with XPCA (95% Variance)\\n0.57\\n0.64\\n4c\\nHN with XPCA (90% Variance)\\n0.55\\n0.63\\n4d\\nHN with XPCA (80% Variance)\\n0.51\\n0.58\\n4e\\nHN with XPCA (70% Variance)\\n0.49\\n0.56\\nTable 6: Results of ablation study showing the impact\\nof embeddings, PCA variance thresholds, and positive\\ndocuments on MRR, on the in-house re-ranker model.\\n4.2\\nCase Studies: Examples of Hard Negative\\nImpact\\nFigure 3 shows how similar topics in the domain\\nof cloud computing. To demonstrate the qualitative\\nbenefits of the proposed framework, we present\\ntwo case studies where the baseline and fine-tuned\\nmodels produce different ranking results. These\\nexamples highlight the significance of hard neg-\\natives in distinguishing semantically similar but\\ncontextually irrelevant documents.\\nFigure 3: Illustrations of similar topics in the domain of\\nCloud Computing\\nCase Study 1:\\nDisambiguating Technical\\nAcronyms.\\n• Query (Q): \"What is VCN in Cloud Infras-\\ntructure?\"'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Accepted in ACL 2025\\n• Positive Document (PD): A document ex-\\nplaining \"Virtual Cloud Network (VCN)\" in\\nCloud Infrastructure, detailing its setup and\\nusage.\\n• Hard Negative (HN): A document discussing\\n\"Virtual Network Interface Card (VNIC)\" in\\nthe context of networking hardware.\\nBaseline Result: The baseline model incorrectly\\nranks the hard negative above the positive docu-\\nment due to overlapping terms such as \"virtual\"\\nand \"network.\"\\nProposed Method Result: The fine-tuned model\\nranks the positive document higher, correctly iden-\\ntifying the contextual match between the query\\nand the description of VCN. This improvement\\nis attributed to the triplet loss training with hard\\nnegatives.\\nCase Study 2: Domain-Specific Terminology.\\n• Query (Q): \"How does the CI WAF handle\\nincoming traffic?\"\\n• Positive Document (PD): A document ex-\\nplaining the Web Application Firewall (WAF)\\nin CI, its configuration, and traffic filtering\\nmechanisms.\\n• Hard Negative (HN): A document discussing\\ngeneral firewall configurations in networking.\\nBaseline Result: The baseline model ranks the\\nhard negative higher due to lexical overlap between\\nthe terms \"firewall\" and \"traffic.\"\\nProposed Method Result: The proposed frame-\\nwork ranks the positive document higher, leverag-\\ning domain-specific semantic representations.\\nThese case studies illustrate the practical ad-\\nvantages of training with hard negatives, espe-\\ncially in domains with overlapping terminology\\nor acronyms.\\nAdditional detailed analyses, illustrative prac-\\ntical implications for enterprise applications, and\\nexplicit future directions are discussed in detail in\\nA.4, and A.5.\\n5\\nConclusion\\nWe introduced a scalable, modular framework lever-\\naging dynamic ensemble-based hard-negative min-\\ning to significantly enhance re-ranking models in\\nenterprise and domain-specific retrieval scenarios.\\nOur method dynamically curates semantically chal-\\nlenging yet contextually irrelevant negatives, allow-\\ning re-ranking models to effectively discriminate\\nsubtle semantic differences. Empirical evaluations\\non proprietary enterprise data and diverse public\\ndomain-specific benchmarks demonstrated substan-\\ntial improvements of up to 15% in MRR@3 and\\n19% in MRR@10 over state-of-the-art negative\\nsampling techniques, including BM25, In-Batch\\nNegatives, STAR, and ADORE+STAR.\\nOur approach offers clear practical benefits in\\nreal-world deployments, benefiting downstream ap-\\nplications such as knowledge management, cus-\\ntomer support systems, and Retrieval-Augmented\\nGeneration (RAG), where retrieval precision di-\\nrectly influences user satisfaction and Generative\\nAI effectiveness. The strong performance and gen-\\neralizability across various domains further under-\\nscore the framework’s readiness for industry-scale\\ndeployment.\\nFuture work will focus on extending our frame-\\nwork to handle incremental updates of enterprise\\nknowledge bases and exploring real-time negative\\nsampling strategies for continuously evolving cor-\\npora, further enhancing the adaptability and robust-\\nness required in practical industry settings.\\n6\\nLimitations\\nWhile our approach advances the state of hard\\nnegative mining and encoder-based retrieval, sev-\\neral limitations remain that open avenues for fu-\\nture research. One key challenge is the perfor-\\nmance disparity between short and long documents.\\nAddressing this requires more effective document\\nchunking strategies and the development of hier-\\narchical representations to preserve context across\\nsegments. Additionally, the retrieval of long doc-\\numents is complicated by semantic redundancy\\nand truncation, warranting deeper analysis of their\\nstructural complexity. Our current use of embed-\\nding concatenation for ensembling could also be\\nrefined—future work should evaluate alternative\\nfusion techniques such as weighted averaging or\\nattention-based mechanisms. Moreover, extending\\nthe retrieval framework to support cross-lingual and\\nmultilingual scenarios would enhance its utility in\\nglobally distributed applications.\\nReferences\\nAMIT AGARWAL. 2021. Evaluate generalisation &\\nrobustness of visual features from images to video.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Accepted in ACL 2025\\nResearchGate. Available at https://doi.org/10.\\n13140/RG.2.2.33887.53928.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2024a. Synthetic document generation pipeline for\\ntraining artificial intelligence models. US Patent App.\\n17/994,712.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2025. FS-DAG: Few shot domain adapting graph net-\\nworks for visually rich document understanding. In\\nProceedings of the 31st International Conference on\\nComputational Linguistics: Industry Track, pages\\n100–114, Abu Dhabi, UAE. Association for Com-\\nputational Linguistics.\\nAmit Agarwal, Hitesh Patel, Priyaranjan Pattnayak,\\nSrikant Panda, Bhargava Kumar, and Tejaswini Ku-\\nmar. 2024b. Enhancing document ai data genera-\\ntion through graph-based synthetic layouts. arXiv\\npreprint arXiv:2412.03590.\\nJina AI. 2023. jina-reranker-v2-base-multilingual.\\nArian Askari, Mohammad Aliannejadi, Evangelos\\nKanoulas, and Suzan Verberne. 2023. Generating\\nsynthetic documents for cross-encoder re-rankers: A\\ncomparative study of chatgpt and human experts.\\nJiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang,\\nXinnian Liang, Zhao Yan, and Zhoujun Li. 2023.\\nGriprank: Bridging the gap between retrieval and\\ngeneration via the generative knowledge improved\\npassage ranking. Preprint, arXiv:2305.18144.\\nVittorio Castelli, Rishav Chakravarti, Saswati Dana, An-\\nthony Ferritto, Radu Florian, Martin Franz, Dinesh\\nGarg, Dinesh Khandelwal, Scott McCarley, Mike\\nMcCawley, Mohamed Nasr, Lin Pan, Cezar Pen-\\ndus, John Pitrelli, Saurabh Pujar, Salim Roukos, An-\\ndrzej Sakrajda, Avirup Sil, Rosario Uceda-Sosa, Todd\\nWard, and Rong Zhang. 2019. The techqa dataset.\\nPreprint, arXiv:1911.02984.\\nCohere. 2023a.\\nCohere-embed-multilingual-v3.0.\\nAvailable\\nat:\\nhttps://cohere.com/blog/\\nintroducing-embed-v3.\\nCohere. 2023b.\\nReranker model.\\nAvailable\\nat:\\nhttps://docs.cohere.com/v2/docs/\\nreranking-with-cohere.\\nGabriel de Souza P. Moreira, Radek Osmulski, Mengyao\\nXu, Ronay Ak, Benedikt Schifferer, and Even\\nOldridge. 2024. Nv-retriever: Improving text em-\\nbedding models with effective hard-negative mining.\\nPreprint, arXiv:2407.15831.\\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leip-\\npold. 2021.\\nClimate-fever:\\nA dataset for veri-\\nfication of real-world climate claims.\\nPreprint,\\narXiv:2012.00614.\\nKaran Dua, Praneet Pabolu, and Mengqing Guo. 2024.\\nGenerating templates for use in synthetic document\\ngeneration processes. US Patent App. 18/295,765.\\nKaran Dua, Praneet Pabolu, and Ranjeet Kumar Gupta.\\n2025. Generation of synthetic doctor-patient conver-\\nsations. US Patent App. 18/495,966.\\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\\nArivazhagan, and Wei Wang. 2022.\\nLanguage-\\nagnostic bert sentence embedding.\\nPreprint,\\narXiv:2007.01852.\\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\\ntraining architecture for dense retrieval. EMNLP\\n2021 - 2021 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings, pages\\n981–993.\\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\\nChowdhury, Ankita Naik, Pengshan Cai, and Al-\\nfio Gliozzo. 2022.\\nRe2G: Retrieve, rerank, gen-\\nerate.\\nIn Proceedings of the 2022 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, pages 2701–2715, Seattle, United\\nStates. Association for Computational Linguistics.\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\\nWu. 2023. How close is chatgpt to human experts?\\ncomparison corpus, evaluation, and detection.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training.\\nEK Jasila, N Saleena, and KA Abdul Nazeer. 2023. An\\nefficient document clustering approach for devising\\nsemantic clusters. Cybernetics and Systems, pages\\n1–18.\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen Tau Yih. 2020. Dense passage retrieval\\nfor open-domain question answering.\\nEMNLP\\n2020 - 2020 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings of the\\nConference, pages 6769–6781.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan\\nRaiman, Mohammad Shoeybi, Bryan Catanzaro, and\\nWei Ping. 2024. Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models. arXiv\\npreprint arXiv:2405.17428.\\nFulu Li, Zhiwen Xie, and Guangyou Zhou. 2024.\\nTheme-enhanced hard negative sample mining for\\nopen-domain question answering. In ICASSP 2024 -\\n2024 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages\\n12436–12440.\\nXianming Li and Jing Li. 2023. Angle-optimized text\\nembeddings. arXiv preprint arXiv:2309.12871.\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih\\nYavuz, Caiming Xiong, and Philip S. Yu. 2021.\\nDense hierarchical retrieval for open-domain ques-\\ntion answering.\\nIn Conference on Empirical\\nMethods in Natural Language Processing.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Accepted in ACL 2025\\nSean MacAvaney, Andrew Yates, Arman Cohan,\\nand Nazli Goharian. 2019.\\nCedr:\\nContextual-\\nized embeddings for document ranking.\\nSIGIR\\n2019 - Proceedings of the 42nd International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1101–1104.\\nAndrzej Ma´ckiewicz and Waldemar Ratajczak. 1993.\\nPrincipal components analysis (pca). Computers &\\nGeosciences, 19(3):303–342.\\nLeland McInnes, John Healy, and James Melville.\\n2020.\\nUmap: Uniform manifold approximation\\nand projection for dimension reduction. Preprint,\\narXiv:1802.03426.\\nHansa Meghwani. 2024. Enhancing retrieval perfor-\\nmance: An ensemble approach for hard negative min-\\ning. Preprint, arXiv:2411.02404.\\nVivek Mehta, Mohit Agarwal, and Rohit Kumar Kaliyar.\\n2024. A comprehensive and analytical review of text\\nclustering techniques. International Journal of Data\\nScience and Analytics, pages 1–20.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024a. Sfr-\\nembedding-2: Advanced text embedding with multi-\\nstage training.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024b. Sfr-\\nembedding-mistral: Enhance text retrieval with trans-\\nfer learning. Salesforce AI Research Blog.\\nThanh-Do Nguyen, Chi Minh Bui, Thi-Hai-Yen Vuong,\\nand Xuan-Hieu Phan. 2022. Passage-based bm25\\nhard negatives: A simple and effective negative sam-\\npling strategy for dense retrieval.\\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\\nre-ranking with bert.\\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\\nJimmy Lin. 2019. Multi-stage document ranking\\nwith bert.\\nZach Nussbaum, John X. Morris, Brandon Duderstadt,\\nand Andriy Mulyar. 2024. Nomic embed: Training\\na reproducible long context text embedder. Preprint,\\narXiv:2402.01613.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024a. Multi-lingual natural language generation.\\nUS Patent App. 18/318,315.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024b. Multi-lingual natural language generation.\\nUS Patent App. 18/318,327.\\nSrikant Panda, Amit Agarwal, Gouttham Nambirajan,\\nand Kulbhushan Pachauri. 2025a. Out of distribution\\nelement detection for information extraction. US\\nPatent App. 18/347,983.\\nSrikant Panda, Amit Agarwal, and Kulbhushan Pachauri.\\n2025b. Techniques of information extraction for se-\\nlection marks. US Patent App. 18/240,344.\\nHitesh Laxmichand Patel, Amit Agarwal, Arion Das,\\nBhargava Kumar, Srikant Panda, Priyaranjan Pat-\\ntnayak, Taki Hasan Rafi, Tejaswini Kumar, and Dong-\\nKyu Chae. 2025. Sweeval: Do llms really swear?\\na safety benchmark for testing limits for enterprise\\nuse. In Proceedings of the 2025 Conference of the\\nNations of the Americas Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies (Volume 3: Industry Track), pages\\n558–582.\\nHitesh Laxmichand Patel, Amit Agarwal, Bhargava\\nKumar, Karan Gupta, and Priyaranjan Pattnayak.\\n2024. Llm for barcodes: Generating diverse syn-\\nthetic data for identity documents. arXiv preprint\\narXiv:2411.14962.\\nPriyaranjan Pattnayak, Amit Agarwal, Hansa Megh-\\nwani, Hitesh Laxmichand Patel, and Srikant Panda.\\n2025a.\\nHybrid ai for responsive multi-turn on-\\nline conversations with novel dynamic routing and\\nfeedback adaptation.\\nIn Proceedings of the 4th\\nInternational Workshop on Knowledge-Augmented\\nMethods for Natural Language Processing, pages\\n215–229.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, and\\nAmit Agarwal. 2025b. Tokenization matters: Im-\\nproving zero-shot ner for indic languages. Preprint,\\narXiv:2504.16977.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit\\nAgarwal, Bhargava Kumar, Srikant Panda, and Te-\\njaswini Kumar. 2025c. Clinical qa 2.0: Multi-task\\nlearning for answer extraction and categorization.\\nPreprint, arXiv:2502.13108.\\nRonak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, An-\\ndrew Yates, and Jimmy Lin. 2022. Squeezing wa-\\nter from a stone: A bag of tricks for further im-\\nproving cross-encoder effectiveness for reranking.\\nIn Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in Artificial Intelligence and\\nLecture Notes in Bioinformatics), volume 13185\\nLNCS, pages 655–670. Springer Science and Busi-\\nness Media Deutschland GmbH.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\\nHaifeng Wang. 2020. Rocketqa: An optimized train-\\ning approach to dense passage retrieval for open-\\ndomain question answering.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\\nSentence embeddings using siamese bert-networks.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Accepted in ACL 2025\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing.\\nS. E. Robertson and S. Walker. 1994. Some Simple\\nEffective Approximations to the 2-Poisson Model\\nfor Probabilistic Weighted Retrieval, pages 232–241.\\nSpringer London.\\nSaba Sturua, Isabelle Mohr, Mohammad Kalim Akram,\\nMichael Günther, Bo Wang, Markus Krimmel, Feng\\nWang, Georgios Mastrapas, Andreas Koukounas, An-\\ndreas Koukounas, Nan Wang, and Han Xiao. 2024.\\njina-embeddings-v3: Multilingual embeddings with\\ntask lora. Preprint, arXiv:2409.10173.\\nTheFinAI. 2018. Fiqa: A financial question answering\\ndataset. Available at Hugging Face.\\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\\nVisualizing data using t-sne.\\nJournal of machine\\nlearning research, 9(11).\\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\\nRangan Majumder, and Furu Wei. 2023. Improving\\ntext embeddings with large language models. arXiv\\npreprint arXiv:2401.00368.\\nSvante Wold, Kim H. Esbensen, Kim H. Esbensen, Paul\\nGeladi, and Paul Geladi. 1987. Principal component\\nanalysis. Chemometrics and Intelligent Laboratory\\nSystems, 2:37–52.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding.\\nPreprint,\\narXiv:2309.07597.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2020. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval.\\nZhen Yang, Zhou Shao, Yuxiao Dong, and Jie Tang.\\n2024. Trisampler: A better negative sampling princi-\\nple for dense retrieval. Preprint, arXiv:2402.11855.\\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\\nZhang, and Shaoping Ma. 2021. Optimizing dense\\nretrieval model training with hard negatives. SIGIR\\n2021 - Proceedings of the 44th International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1503–1512.\\nDun Zhang. 2024. stella-embedding-model-2024.\\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie,\\nZiqi Dai, Jialong Tang, Huan Lin, Baosong Yang,\\nPengjun Xie, Fei Huang, et al. 2024. mgte: General-\\nized long-context text representation and reranking\\nmodels for multilingual text retrieval. arXiv preprint\\narXiv:2407.19669.\\nA\\nAppendix\\nA.1\\nExtended Related Work\\nHard Negatives in Retrieval Models\\nStatic and\\ndynamic hard negatives have been extensively stud-\\nied.\\nStatic negatives, such as those generated\\nby BM25 (Robertson and Walker, 1994) or Pas-\\nsageBM25 (Nguyen et al., 2022), provide challeng-\\ning lexical contrasts but risk overfitting due to their\\nfixed nature (Qu et al., 2020). Dynamic negatives,\\nas introduced in ANCE (Xiong et al., 2020) and\\nADORE (Zhan et al., 2021) adapt during training,\\nother effective methods like positive-aware mining\\n(de Souza P. Moreira et al., 2024), theme-enhanced\\nnegatives (Li et al., 2024) offers relevant chal-\\nlenges but incurring high computational costs due\\nto periodic re-indexing and bigger embedding di-\\nmension. Our framework mitigates these issues by\\nleveraging clustering and dimensionality reduction\\nto dynamically identify negatives without requiring\\nre-indexing.\\nLocalized Contrastive Estimation (LCE) (Guo\\net al., 2023; AGARWAL, 2021) further demon-\\nstrated the effectiveness of incorporating hard nega-\\ntives into cross-encoder training, improving rerank-\\ning accuracy when negatives align with retriever\\noutputs. Additionally, (Pradeep et al., 2022) high-\\nlighted the importance of hard negatives even in\\nadvanced pretraining setups like Condenser (Gao\\nand Callan, 2021), which emphasizes their neces-\\nsity for robust optimization.\\nAdvances\\nin\\nDense\\nRetrieval\\nand\\nCross-\\nEncoders\\nDense\\nretrieval\\nmodels\\nlike\\nDPR (Karpukhin et al., 2020) and REALM (Guu\\net al., 2020) encode queries and documents into\\ndense embeddings, enabling semantic matching.\\nRecent advances in dense retrieval and ranking\\ninclude GripRank’s generative knowledge-driven\\npassage ranking\\n(Bai et al., 2023), Dense\\nHierarchical Retrieval’s multi-stage framework\\nfor efficient question answering (Liu et al., 2021;\\nPattnayak et al., 2025a,c,b; Patel et al., 2025), and\\nTriSampler’s optimized negative sampling for\\ndense retrieval (Yang et al., 2024), collectively\\nenhancing retrieval performance.Cross-encoders,\\nsuch as monoBERT (Nogueira et al., 2019;\\nNogueira and Cho, 2019), further improve retrieval\\nprecision by jointly encoding query-document\\npairs but require high-quality training data,\\nparticularly challenging negatives (MacAvaney\\net al., 2019; Panda et al., 2025b). Techniques such'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Accepted in ACL 2025\\nas synthetic data generation (Askari et al., 2023;\\nAgarwal et al., 2024a, 2025) augment training\\ndatasets but lack the realism and semantic depth\\nprovided by our hard negative mining approach.\\nDimensionality Reduction in IR\\nClustering\\nmethods have been used to group semantically\\nsimilar documents, improving retrieval efficiency\\nand training data organization (Mehta et al., 2024;\\nJasila et al., 2023; Dua et al., 2025; Panda et al.,\\n2025a). Dimensionality reduction techniques like\\nPCA (Wold et al., 1987) enhance scalability by re-\\nducing computational complexity. Our framework\\nuniquely combines these techniques to dynamically\\nidentify negatives that challenge retrieval models\\nin a scalable manner.\\nSynthetic\\nData\\nin\\nRetrieval\\nRecent\\nwork (Askari et al., 2023; Agarwal et al.,\\n2024a,b; Patel et al., 2024; Dua et al., 2024; Pabolu\\net al., 2024a,b) has explored using large language\\nmodels to generate synthetic training data for\\nretrieval tasks. While effective in low-resource\\nsettings, synthetic data often struggles with factual\\ninaccuracies and domain-specific relevance. In\\ncontrast, our framework relies on real-world data\\nto curate semantically challenging negatives,\\nensuring high-quality training samples without\\nintroducing synthetic biases.\\nSummary of Contributions\\nWhile previous\\nworks address various aspects of negative sampling,\\nhard negatives, and synthetic data, our approach\\nbridges the gap between static and dynamic strate-\\ngies. By dynamically curating negatives using clus-\\ntering and dimensionality reduction, we achieve\\na scalable and semantically precise methodology\\ntailored to domain-specific retrieval tasks.\\nA.2\\nExtended Methodology\\nA.2.1\\nDataset Statistics\\nQueries Length Distribution\\nIn this section we\\nanalyze the distribution of queries length in our\\nenterprise dataset. Figure 4 shows that the length\\nof queries ranges from 1 to 25 words, with some\\nqueries having very few words. This highlights that\\nuser queries can sometime be just 2-3 words about\\na topic, increasing the probability of retrieving doc-\\numents mentioning those topics or concepts which\\ncan be contextually different. Therefore, when\\nwe select hard negatives, it is crucial to consider\\nnot only the relationship between the query and\\ndocuments but also the relationship between the\\nFigure 4: Length Distribution of queries in the dataset.\\npositive document and other documents, ensuring a\\ncomparison with texts on similar topics and similar\\nlengths.\\nModel (Ek)\\nParams (M)\\nDimension\\nMax Tokens\\nstella_en_400M_v5\\n435\\n8192\\n8192\\njina-embeddings-v3\\n572\\n1024\\n8194\\n(multilingual)\\nmxbai-embed-large-v1\\n335\\n1024\\n512\\nbge-large-en-v1.5\\n335\\n1024\\n512\\nLaBSE\\n471\\n768\\n256\\n(multilingual)\\nall-mpnet-base-v2\\n110\\n768\\n514\\n(multilingual)\\nTable 7: Embedding models used to construct Xconcat,\\ncombining diverse semantic representations for queries\\n(Q), positive documents (PD), and corpus documents\\n(D).\\nFigure 5: Shows document length distribution in Enter-\\nprise corpus.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Accepted in ACL 2025\\nDocument Length Distribution\\nAs shown in\\nFigure 5 , document lengths are significantly longer\\nthan query lengths. This disparity in context length\\naffects the similarity scores, potentially reducing\\nthe accuracy of retrieval systems. In our in-house\\ndataset, each query is paired with a single correct\\ndocument (though its not limited by number of\\npositive-negative document per query). This posi-\\ntive document is crucial for identifying challenging\\nhard negatives and hence helpful for encoder-based\\nmodel training.\\nA.2.2\\nEmbedding Models\\nTable 7 lists the embedding models (Zhang, 2024;\\nSturua et al., 2024; Li and Li, 2023; Xiao et al.,\\n2023; Feng et al., 2022; Reimers and Gurevych,\\n2019; Zhang et al., 2024) used to construct Xconcat,\\ncombining diverse semantic representations for\\nqueries (Q), positive documents (PD), and cor-\\npus documents (D). These models were selected\\nfor their performance, model size, ability to han-\\ndle multilingual context, providing complemen-\\ntary strengths in dimensionality and token cover-\\nage. By integrating embeddings from these models,\\nthe framework captures nuanced semantic relation-\\nships crucial for reranker training.\\nA.2.3\\nUnified Contrastive Loss\\nThe unified contrastive loss is designed to improve\\nranking precision for both bi-encoders and cross-\\nencoders, by ensuring that positive documents\\n(PD) are ranked closer to the query (Q) than hard\\nnegatives (DHN) by a margin m. The loss is de-\\nfined as:\\nL =\\nN\\nX\\ni=1\\nmax (0, m + d(Qi, PDi) −d(Qi, DHNi))\\n(7)\\nwhere:\\n• PDi:\\nPositive document associated with\\nquery Qi.\\n• DHNi: Hard negative document, semantically\\nsimilar to PDi but contextually irrelevant.\\n• d(Qi, Di): Distance metric measuring rele-\\nvance between Qi and Di.\\n• m: Margin ensuring PDi is closer to Qi than\\nDHNi by at least m, encouraging the model\\nto distinguish between relevant and irrelevant\\ndocuments effectively.\\nFor bi-encoders, the distance metric is defined as:\\nd(Qi, Di) = 1 −cosine(eQi, eDi),\\n(8)\\nwhere eQi and eDi are the embeddings of the query\\nand document, respectively. For cross-encoders,\\nthe distance metric is:\\nd(Qi, Di) = −s(Qi, Di),\\n(9)\\nwhere s(Qi, Di) is the cross-encoder’s relevance\\nscore for the query-document pair.\\nThis formulation leverages the triplet of (Q, PD,\\nDHN) to minimize d(Qi, PDi), pulling positive\\ndocuments closer to the query, while maximizing\\nd(Qi, DHNi), pushing hard negatives further away.\\nBy emphasizing semantically challenging exam-\\nples, the model learns sharper decision boundaries\\nfor improved ranking precision.\\nA.3\\nExperimental Setup\\nDatasets\\nWe evaluate our framework extensively\\nusing both proprietary and public datasets:\\n• Internal Proprietary Dataset: Consisting\\nof approximately 5250 query-document pairs,\\non cloud services like computing, networking,\\nfirewall, ai services. It includes both short (<\\n[1024 tokens]) and long documents (>= [1024\\ntokens]).\\n• FiQA Dataset: A financial domain-specific\\ndataset widely used for retrieval benchmark-\\ning.\\n• Climate-FEVER Dataset: An environment-\\nspecific fact-checking dataset focused on\\nclimate-related information retrieval.\\n• TechQA Dataset:\\nA technical question-\\nanswering dataset emphasizing software engi-\\nneering and technology-related queries.\\nTraining and Fine-tuning\\nAll re-ranking mod-\\nels are fine-tuned using a triplet loss with margin\\nwith same hyper-parameters. Early stopping is em-\\nployed based on validation MRR@10 scores to\\nprevent overfitting.\\nEvaluation Metrics\\nModel performance is eval-\\nuated using standard retrieval metrics: Mean Recip-\\nrocal Rank (MRR) at positions 3 and 10 (MRR@3\\nand MRR@10), which measure retrieval quality\\nand ranking precision. Each reported metric is\\naveraged across three experimental runs for robust-\\nness.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Accepted in ACL 2025\\nStrategy\\nTraining Data\\nMRR@3\\nMRR@10\\nBaseline\\n0\\n0.42\\n0.45\\nFinetuned with\\nHard Negatives\\n(Ours)\\n100\\n0.46\\n0.49\\n200\\n0.48\\n0.51\\n300\\n0.50\\n0.53\\n400\\n0.52\\n0.56\\n500\\n0.52\\n0.58\\n600\\n0.54\\n0.60\\n700\\n0.54\\n0.62\\n800\\n0.56\\n0.63\\n900\\n0.57\\n0.64\\n1000\\n0.57\\n0.64\\nTable 8: Comparison of Strategies with Varying Train-\\ning Data Sizes\\nA.4\\nExtended Results & Ablation\\nImpact of Training Data Size\\nAs shown in Ta-\\nble 8, both MRR@3 and MRR@10 improve as the\\ntraining data size increases, with more pronounced\\ngains in MRR@10. MRR@3 shows gradual im-\\nprovement, from 0.42 at the baseline to 0.57 with\\n100 examples, highlighting the model’s enhanced\\nability to rank relevant documents within the top 3.\\nMRR@10, on the other hand, shows more signif-\\nicant improvement, from 0.45 to 0.64, indicating\\nthat the model benefits more from additional data\\nwhen considering the top 10 ranked documents.\\nOur method shows promising results even with\\nsmaller training sets, demonstrating the effective-\\nness of incorporating hard negatives early in the\\ntraining process. This suggests that hard negatives\\nsignificantly enhance the model’s ability to distin-\\nguish relevant from irrelevant documents against\\na given query, even when data is limited. This ap-\\nproach is particularly beneficial in enterprise con-\\ntexts, where annotated data may be scarce, enabling\\nquicker improvements in domain-specific retrieval\\nperformance.\\nModels in the Study\\nIn our study we com-\\npared the performance of other finetuned re-ranker\\n(Glass et al., 2022; Wang et al., 2023; Raffel et al.,\\n2020) and embedding models (Zhang et al., 2024;\\nNussbaum et al., 2024) using hard negatives gen-\\nerated by our proposed framework in Table 4.\\nWe benchmarked the BGE-Reranker (Xiao et al.,\\n2023), NV-Embed (Lee et al., 2024) Salesforce-\\nSFR (Meng et al., 2024a,b) , jina-reranker (AI,\\n2023) and Cohere-Reranker (Cohere, 2023a,b),\\nA.4.1\\nAnalysis of Long vs. Short Documents\\nTable\\n5 reveals a consistent disparity in MRR\\nscores between short and long documents, with\\nlong documents showing lower performance. Here,\\nwe analyze potential reasons and propose mitiga-\\ntion strategies.\\nChallenges with Long Documents.\\n• Semantic Redundancy: Long documents of-\\nten contain repetitive or tangential content,\\ndiluting their relevance to a specific query.\\n• Context Truncation: Fixed-length tokeniza-\\ntion (e.g., 512 or 1024 tokens) truncates long\\ndocuments, potentially discarding critical in-\\nformation.\\n• Query-to-Document\\nMismatch:\\nShort\\nqueries may not provide sufficient context to\\nmatch the nuanced information spread across\\na lengthy document.\\nPotential Solutions.\\n• Chunk-Based Retrieval:\\nSplit long doc-\\numents into smaller, semantically coherent\\nchunks and rank them individually.\\n• Hierarchical Embeddings: Use hierarchical\\nmodels to aggregate sentence- or paragraph-\\nlevel embeddings for better context represen-\\ntation.\\n• Query Expansion: Enhance short queries\\nwith additional context using techniques like\\nquery rewriting or pseudo-relevance feedback.\\nThis analysis highlights the need for future work\\nto address the inherent challenges of ranking long\\ndocuments effectively.\\nA.5\\nPractical Implications for Enterprise\\nApplications\\nThe proposed framework has significant practical\\nimplications for enterprise information retrieval\\nsystems, particularly in retrieval-augmented gener-\\nation (RAG) pipelines.\\nImproved Ranking Precision.\\nBy training with\\nhard negatives, the model ensures that the most\\nrelevant documents are retrieved for each query.\\nThis is particularly critical for enterprise use cases\\nsuch as:\\n• Technical Support: Retrieving precise docu-\\nmentation for customer queries, reducing res-\\nolution times.\\n• Knowledge Management: Ensuring that em-\\nployees access the most relevant internal re-\\nsources quickly.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Accepted in ACL 2025\\nEnhanced Generative Quality.\\nHigh-quality re-\\ntrieval directly improves the factual accuracy and\\ncoherence of outputs generated by large language\\nmodels in RAG pipelines. For example:\\n• Documentation Summarization:\\nSum-\\nmaries generated by models like GPT are\\nmore reliable when based on top-ranked, ac-\\ncurate sources.\\n• Customer Interaction: Chatbots generate\\nmore contextually relevant responses when\\nfed precise retrieved documents.\\nScalability and Adaptability.\\nThe framework’s\\nmodular design, including the use of diverse embed-\\ndings and clustering-based hard negative selection,\\nallows it to adapt to:\\n• Different industries (e.g., healthcare, finance,\\nmanufacturing).\\n• Multi-lingual or cross-lingual retrieval tasks.\\nThese practical implications underscore the ver-\\nsatility and enterprise readiness of the proposed\\nframework.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd661c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97a086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG 1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
