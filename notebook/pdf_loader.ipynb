{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75a52d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dd66f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: 2505.18366v1.pdf\n",
      "  ✓ Loaded 14 pages\n",
      "\n",
      "Processing: attention.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Total documents loaded: 29\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1916450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='arXiv:2505.18366v1  [cs.IR]  23 May 2025\\nAccepted in ACL 2025\\nHard Negative Mining for Domain-Specific Retrieval in Enterprise Systems\\nHansa Meghwani*, Amit Agarwal*, Priyaranjan Pattnayak,\\nHitesh Laxmichand Patel, Srikant Panda\\nOracle AI\\nCorrespondence: hansa.meghwani@oracle.com; amit.h.agarwal@oracle.com *\\nAbstract\\nEnterprise search systems often struggle to re-\\ntrieve accurate, domain-specific information\\ndue to semantic mismatches and overlapping\\nterminologies. These issues can degrade the\\nperformance of downstream applications such\\nas knowledge management, customer support,\\nand retrieval-augmented generation agents. To\\naddress this challenge, we propose a scal-\\nable hard-negative mining framework tailored\\nspecifically for domain-specific enterprise data.\\nOur approach dynamically selects semantically\\nchallenging but contextually irrelevant docu-\\nments to enhance deployed re-ranking models.\\nOur method integrates diverse embedding mod-\\nels, performs dimensionality reduction, and\\nuniquely selects hard negatives, ensuring com-\\nputational efficiency and semantic precision.\\nEvaluation on our proprietary enterprise corpus\\n(cloud services domain) demonstrates substan-\\ntial improvements of 15% in MRR@3 and 19%\\nin MRR@10 compared to state-of-the-art base-\\nlines and other negative sampling techniques.\\nFurther validation on public domain-specific\\ndatasets (FiQA, Climate Fever, TechQA) con-\\nfirms our method’s generalizability and readi-\\nness for real-world applications.\\n1 Introduction\\nAccurate retrieval of domain-specific information\\nsignificantly impacts critical enterprise processes,\\nsuch as knowledge management, customer sup-\\nport, and Retrieval Augmented Generation (RAG)\\nAgents. However, achieving precise retrieval re-\\nmains challenging due to semantic mismatches,\\noverlapping terminologies, and ambiguous abbre-\\nviations common in specialized fields like finance,\\nand cloud computing. Traditional lexical retrieval\\ntechniques, such as BM25 (Robertson and Walker,\\n1994), struggle due to vocabulary mismatches, lead-\\ning to irrelevant results and poor user experience.\\n*The authors contributed equally to this work.\\nRecent dense retrieval approaches leveraging\\npre-trained language models, like BERT-based en-\\ncoders (Karpukhin et al., 2020; Xiong et al., 2020;\\nGuu et al., 2020), mitigate lexical limitations by\\ncapturing semantic relevance. Nevertheless, their\\nperformance heavily relies on the negative sam-\\nples—documents incorrectly retrieved due to se-\\nmantic similarity but lacking contextual relevance.\\nModels trained with negative sampling methods\\n(e.g., random sampling, BM25-based static sam-\\npling, or dynamic methods like ANCE (Xiong\\net al., 2020), STAR (Zhan et al., 2021)) either\\nlack sufficient semantic discrimination or incur\\nhigh computational costs, thus limiting scalability\\nand practical enterprise deployment. For instance,\\ngiven a query such as \"Steps to deploy a MySQL\\ndatabase on Cloud Infrastructure,\" most negative\\nsampling techniques select documents discussing\\nnon-MySQL database deployments. Conversely,\\nour method strategically selects a hard negative dis-\\ncussing MySQL deployment on-premises, which\\ndespite semantic overlap, is contextually distinct\\nand thus poses a stronger training challenge for the\\nretrieval and re-ranking models.\\nOur proposed framework addresses these by in-\\ntroducing a novel semantic selection criterion ex-\\nplicitly designed to curate high-quality hard nega-\\ntives. By uniquely formulating two semantic con-\\nditions that effectively select negatives that closely\\nresemble query semantics but remain contextually\\nirrelevant, significantly minimizing false negatives\\nencountered by existing techniques. The main con-\\ntributions of this paper are:\\n1. A negative mining framework for dynamically\\nselecting semantically challenging hard neg-\\natives, leveraging diverse embedding models\\nand semantic filtering criteria to significantly\\nimprove re-ranking models in domain-specific\\nretrieval scenarios.\\n2. Comprehensive evaluations demonstrating'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nconsistent and significant improvements\\nacross both proprietary and publicly available\\ndatasets, verifying our method’s impact and\\nbroad applicability across domain-specific\\nusecases.\\n3. In-depth analysis, of critical challenges in han-\\ndling both short and long-form enterprise doc-\\numents, laying a clear foundation for targeted\\nfuture improvements.\\nOur work directly enhances the semantic dis-\\ncrimination capabilities of re-ranking models, re-\\nsulting in 15% improvement in MRR@3 and\\n19% improvement in MRR@10 on our in-house\\ncloud-services domain dataset. Further evaluations\\non public domain-specific benchmarks (FiQA, Cli-\\nmate Fever, TechQA) confirm generalizability and\\ntangible improvements of our proposed negative\\nmining framework.\\n2 Related Work\\n2.1 Hard Negatives in Retrieval Models\\nThe role of hard negatives in training dense re-\\ntrieval models has been widely studied. Static\\nnegatives, such as BM25 (Robertson and Walker,\\n1994), provide lexical similarity but fail to capture\\nsemantic relevance, often leading to overfitting (Qu\\net al., 2020). Dynamic negatives, introduced in\\nANCE (Xiong et al., 2020) and STAR (Zhan et al.,\\n2021), adapt during training to provide more chal-\\nlenging contrasts but require significant computa-\\ntional resources due to periodic re-indexing. Our\\nframework addresses these limitations by dynam-\\nically identifying semantically challenging nega-\\ntives using clustering and dimensionality reduction,\\nensuring scalability and adaptability.\\nFurther studies have explored advanced meth-\\nods for negative sampling in cross-encoder mod-\\nels (Meghwani, 2024). Localized Contrastive Es-\\ntimation (LCE) (Guo et al., 2023) integrates hard\\nnegatives into cross-encoder training, improving\\nthe reranking performance when negatives align\\nwith the output of the retriever. Similarly, (Pradeep\\net al., 2022) demonstrated the importance of hard\\nnegatives even when models undergo advanced pre-\\ntraining techniques, such as condenser (Gao and\\nCallan, 2021). Our work builds on these efforts by\\noffering a scalable approach, which can be applied\\nto any domain-heavy enterprise data.\\n2.2 Negative Sampling Strategies\\nEffective negative sampling significantly affects the\\nperformance of the retrieval model by challenging\\nthe model to differentiate between relevant and\\nirrelevant examples. Common strategies include:\\n• Random Negatives: Efficient but lacking se-\\nmantic contrast, leading to suboptimal perfor-\\nmance (Karpukhin et al., 2020).\\n• BM25 Negatives: Leverage lexical similar-\\nity, but often introduce biases, particularly\\nin semantically rich domains (Robertson and\\nWalker, 1994).\\n• In-Batch Negatives: Computationally ef-\\nficient but limited to local semantic con-\\ntrasts, often underperforming in dense re-\\ntrieval tasks (Xiong et al., 2020).\\nOur framework complements these approaches\\nby dynamically generating negatives that balance\\nsemantic similarity and contextual irrelevance,\\navoiding the pitfalls of static or random methods.\\n2.3 Domain-Specific Retrieval Challenges\\nEnterprise retrieval systems face unique challenges,\\nsuch as ambiguous terminology, overlapping con-\\ncepts, and private datasets (Meghwani, 2024).\\nGeneral-purpose methods such as BM25 or dense\\nretrieval models (Qu et al., 2020) fail to capture\\ndomain-specific complexities effectively. Our ap-\\nproach addresses these gaps by curating hard nega-\\ntives that align with enterprise-specific semantics,\\nimproving retrieval precision and robustness for\\nproprietary datasets.\\nWe further discuss negative sampling techniques in\\nAppendix A.1.\\n3 Methodology\\nTo effectively train and finetune reranker models\\nfor domain-specific retrieval, it is essential to sys-\\ntematically handle technical ambiguities stemming\\nfrom specialized terminologies, overlapping con-\\ncepts, and abbreviations prevalent within enterprise\\ndomains.\\nWe propose a structured, modular framework\\nthat integrates diverse embedding models, dimen-\\nsionality reduction, and a novel semantic criterion\\nfor hard-negative selection. Figure 1 illustrates the\\nhigh-level pipeline, components and their interac-\\ntions. The re-ranking models fine-tuned using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nFigure 1: Overview of the methodology pipeline for training reranker models, including embedding generation,\\nPCA-based dimensionality reduction and hard negative selection for fine-tuning.\\nhard negatives generated by our framework are di-\\nrectly deployed in downstream applications, such\\nas RAG, significantly improving the resolution of\\ncustomer queries through enhanced retrieval.\\nOur approach begins by encoding queries and\\ndocuments into semantically rich vector represen-\\ntations using an ensemble of state-of-the-art bi-\\nencoder embedding models. These embeddings are\\nstrategically selected based on multilingual sup-\\nport, embedding quality, training data diversity,\\ncontext length handling, and performance (details\\nprovided in Appendix A.2. To manage embed-\\nding dimensionality and improve computational\\nefficiency, Principal Component Analysis (PCA)\\n(Ma´ckiewicz and Ratajczak, 1993) is utilized to\\nproject the concatenated embeddings onto a lower-\\ndimensional space, maintaining 95% of the original\\nvariance.\\nWe then define two semantic conditions (Eq. 5\\nand Eq. 6) to dynamically select high-quality hard\\nnegatives, addressing semantic similarity chal-\\nlenges and minimizing false negatives. Together,\\nthese two equations ensure that the selected hard\\nnegative is not only close to the query (Eq. 5) but\\nalso contextually distinct from the true positive,\\nminimizing the risk of selecting topic duplicates\\nor noisy positives (Eq. 6). For example, a query\\nabout deploying MySQL on Oracle Cloud, PD is a\\nguide on that topic, and D is a doc about MySQL\\non-premise — semantically close to Q, but distant\\nfrom PD.\\nBelow we detail each methodological compo-\\nnent, emphasizing their contributions to enhancing\\nretrieval precision in domain-specific or enterprise\\nretrieval tasks.\\nTotal Train Test\\n< Q, P D > 5250 1000 4250\\nTable 1: Dataset distribution of queries (Q) and positive\\ndocuments (PD).\\n3.1 Dataset Statistics\\nOur experiments leverage a proprietary corpus con-\\ntaining 36,871 unannotated documents sourced\\nfrom over 30 enterprise cloud services. Addition-\\nally, we prepared 5250 annotated query-positive\\ndocument pairs ( < Q, P D > ) for training and\\ntesting. Notably, we adopted a non-standard train-\\ntest split (as summarized in Table 1), allocating\\nfour times more data to testing than training to\\nrigorously evaluate model robustness against vary-\\ning training data volumes (additional analyses in\\nAppendix A.4). To further validate generaliz-\\nability, we conduct evaluations on publicly avail-\\nable domain-specific benchmarks: FiQA (finance)\\n(TheFinAI, 2018), Climate Fever (climate science)\\n(Diggelmann et al., 2021), and TechQA (technol-\\nogy) (Castelli et al., 2019). Detailed dataset statis-\\ntics are provided in Appendix A.2.1.\\n3.2 Embedding Generation\\nEmbeddings for queries, positive documents, and\\nthe corpus are computed via six diverse, high-\\nperformance bi-encoder models E1, E2, . . . , E6,\\neach selected strategically for capturing comple-\\nmentary semantic perspectives:\\nEk(x) ∈ Rdk (1)\\nwhere dk is the embedding dimension of the kth\\nmodel for textual input x. Concatenation of these'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nembeddings yields a comprehensive representation:\\nXconcat = [e1(x); e2(x); . . . ; e6(x)] (2)\\nwhere Xconcat ∈ R\\nP6\\nk=1 dk represents the con-\\ncatenated embedding for the input x.\\n3.3 Dimensionality Reduction\\nTo alleviate the computational overhead arising\\nfrom high-dimensional concatenated embeddings,\\nwe apply PCA to reduce dimensionality while pre-\\nserving semantic richness:\\nXPCA = XconcatP, (3)\\nwhere P represents the PCA projection matrix.\\nWe specifically select PCA due to its computational\\nefficiency, and scalability, essential given our large\\nenterprise corpus and high-dimensional embedding\\nspace. While we empirically evaluated nonlinear\\ndimensionality reduction methods such as UMAP\\n(McInnes et al., 2020) and t-SNE (Van der Maaten\\nand Hinton, 2008), they offered negligible perfor-\\nmance improvements over PCA but incurred sub-\\nstantially higher computational costs, making them\\nimpractical for deployment at scale in enterprise\\nsystems.\\n3.4 Hard Negative Selection Criteria\\nWe propose two semantic criteria to identify high-\\nquality hard negatives. PCA-reduced embeddings\\nXPCA are organized around each queryQ. For each\\nquery-positive document pair (Q, P D), candidate\\ndocuments D from the corpus are evaluated via\\ncosine distances:\\nd(Q, P D), d (Q, D), d (P D, D) (4)\\nA document D is selected as a hard negative\\nonly if it satisfies both criteria:\\nd(Q, D) < d(Q, P D) (5)\\nd(Q, D) < d(P D, D) (6)\\nEquation (5) ensures that the candidate negative\\ndocument is semantically closer to the query than\\nthe actual positive document, making it a challeng-\\ning negative example that potentially confuses the\\nreranking model. Equation (6), ensures that the se-\\nlected hard negative is not just query-confusing but\\nalso sufficiently dissimilar from the actual positive\\n(avoiding near-duplicates or false negatives).\\nThe candidate document DHN with minimal\\nd(Q, D) satisfying these conditions is chosen as\\nthe primary hard negative. Additional hard nega-\\ntives can similarly be selected based on semantic\\nproximity rankings.\\nFigure 2: Hard negative selection on the first two PCA\\ncomponents (78% variance). Q act as centroids, P D\\nguide selection of hard negatives; which are chosen\\nbased on semantic proximity.\\nFigure 2 illustrates an example embedding space,\\nclearly depicting the query Q, positive document\\nP D, and selected hard negative DHN , visualizing\\nthe semantic selection criteria. In cases where no\\ndocuments satisfy these conditions, no hard nega-\\ntives are selected for that particular query. Further\\ndetails on our embedding model & fine-tuning us-\\ning these hard negatives are provided in Appendix\\nA.2.\\n4 Experiments & Results\\nTo evaluate the effectiveness of our proposed hard-\\nnegative selection framework, we conduct exten-\\nsive experiments on our internal cloud-specific en-\\nterprise dataset, as well as domain-specific open-\\nsource benchmarks. We systematically compare\\nour approach against multiple competitive negative\\nsampling methods and perform detailed ablation\\nstudies to understand the contribution of individual\\nframework components. Complete details on exper-\\nimental setups and hyperparameters are provided\\nin Appendix A.3.\\n4.1 Results & Discussion\\nComparative Analysis of Negative Sampling\\nStrategies Table 3 presents a detailed compar-\\nison of of our negative sampling technique against\\nseveral established methods, including Random,\\nBM25, In-batch, STAR, and ADORE+STAR. The'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nRe-ranker (Fine-tuned w/) Internal FiQA Climate-FEVER TechQA\\nMRR@3 MRR@10MRR@3 MRR@10MRR@3 MRR@10MRR@3 MRR@10\\nBaseline (No Fine-tuning) 0.42 0.45 0.45 0.48 0.44 0.46 0.57 0.61\\nIn-batch Negatives 0.47 0.52 0.46 0.52 0.44 0.47 0.57 0.62\\nSTAR 0.53 0.56 0.51 0.54 0.47 0.49 0.61 0.63\\nADORE+STAR 0.54 0.57 0.52 0.54 0.48 0.52 0.63 0.66\\nOur Proposed HN 0.57 0.64 0.54 0.56 0.52 0.55 0.65 0.69\\nTable 2: Comparative performance benchmarking of our in-house reranker across multiple domain-specific datasets.\\nThe reranker is fine-tuned (FT) with different negative sampling techniques, highlighting the effectiveness of our\\nproposed hard-negative mining method (HN).\\nNegative Sampling Method MRR@3 MRR@10\\nBaseline 0.42 0.45\\nFT with Random Neg 0.47 0.51\\nFT with BM25 Neg 0.49 0.54\\nFT with In-batch Neg 0.47 0.52\\nFT with BM25+In-batch Neg 0.52 0.54\\nFT with STAR 0.53 0.56\\nFT with ADORE+STAR 0.54 0.57\\nFT with our HN 0.57 0.64\\nTable 3: Comparison of negative sampling methods for\\nfine-tuning(FT) in-house cross-encoder reranker model.\\nThe proposed framework achieves 15% and 19% im-\\nprovements in MRR@3 and MRR@10, respectively,\\nover baseline methods.\\nbaseline is defined as the performance of our inter-\\nnal reranker model without any fine-tuning. Our\\nmethod achieves notable relative improvements of\\n15% in MRR@3 and 19% in MRR@10 over this\\nbaseline. The semantic nature of our hard nega-\\ntives allows the reranker to distinguish contextually\\nirrelevant but semantically similar documents effec-\\ntively. In contrast, simpler baselines like Random\\nor BM25 negatives suffer due to no semantic con-\\nsideration, while advanced methods like STAR and\\nADORE+STAR occasionally miss subtle seman-\\ntic nuances that our formulated selection criteria\\naddress effectively.\\nGeneralization Across Open-source Models To\\nvalidate the robustness and versatility of our frame-\\nwork, we evaluated various open-source embed-\\nding and reranker models (Table 4), clearly demon-\\nstrating improvements across all models when fine-\\ntuned using our proposed negative sampling com-\\npared to ADORE+STAR and baseline (no fine-\\ntuning). Notably, rerankers with multilingual ca-\\npabilities, such as the BGE-Reranker and Jina\\nReranker, demonstrated pronounced improvements,\\nlikely benefiting from our embedding ensemble’s\\nmultilingual semantic richness. Similarly, larger\\nmodels like e5-mistral exhibit significant gains, re-\\nflecting their capacity to exploit nuanced semantic\\ndifferences provided by our negative samples. This\\nanalysis underscores the general applicability and\\nmodel-agnostic benefits of our approach.\\nModel Baseline ADORE+STAR Ours\\nAlibaba-NLP\\n(gte-multilingual-reranker-base) 0.39 0.42 0.45\\nBGE-Reranker\\n(bge-reranker-large) 0.44 0.47 0.52\\nCohere Embed English Light\\n(Cohere-embed-english-light-v3.0) 0.32 0.34 0.38\\nCohere Embed Multilingual\\n(Cohere-embed-multilingual-v3.0) 0.34 0.37 0.40\\nCohere Reranker\\n(rerank-multilingual-v2.0) 0.42 0.45 0.49\\nIBM Reranker\\n(re2g-reranker-nq) 0.40 0.43 0.46\\nInfloat Reranker\\n(e5-mistral-7b-instruct) 0.35 0.38 0.42\\nJina Reranker v2\\n(jina-reranker-v2-base-multilingual) 0.45 0.480.53\\nMS-MARCO\\n(ms-marco-MiniLM-L-6-v2) 0.41 0.43 0.46\\nNomic AI Embed Text\\n(nomic-embed-text-v1.5) 0.33 0.36 0.39\\nNVIDIA\\nNV-Embed-v2 0.38 0.41 0.44\\nSalesforce\\nSFR-Embedding-2_R 0.37 0.40 0.43\\nSalesforce\\nSFR-Embedding-Mistral 0.36 0.39 0.42\\nT5-Large 0.41 0.44 0.47\\nTable 4: Performance benchmarking (MRR@3) of\\nreranker and embedding models using the proposed\\nhard negative selection framework, compared with\\nADORE+STAR and baseline methods.\\nEffectiveness on Domain-specific Public\\nDatasets We further tested our method’s\\nadaptability across diverse public domain-specific\\ndatasets (FiQA, Climate-FEVER, TechQA), as\\nshown in Table 2. Each dataset presents distinct\\nretrieval challenges, ranging from technical jargon\\nin TechQA to complex domain-specific reasoning\\nin Climate-FEVER. Fine-tuning with our generated\\nhard negatives consistently improved retrieval\\nacross these varied datasets. FiQA exhibited\\nsignificant gains, likely due to the semantic\\ndifferentiation required in finance-specific queries.\\nThese results demonstrate that our negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nsampling method is not only effective within our\\ninternal enterprise corpus but also valuable across\\ndiverse, domain-specific public datasets, indicating\\nbroad applicability and domain independence.\\nModel MRR@3 MRR@10\\nShort DocumentsBaseline 0.481 0.526\\nFT w/ proposed HN0.61 0.662\\nLong DocumentsBaseline 0.423 0.477\\nFT w/ proposed HN0.475 0.521\\nTable 5: Performance comparison of the in-house\\nreranker without fine-tuning (Baseline) versus fine-\\ntuned (FT) with our proposed hard negatives (HN), eval-\\nuated separately on short and long documents.\\nPerformance Analysis on Short vs. Long Docu-\\nments An explicit analysis of short versus long\\ndocuments (Table 5) revealed differential perfor-\\nmance gains. Short documents (under 1024 to-\\nkens) experienced substantial performance im-\\nprovements (MRR@3 improving from 0.481 to\\n0.61), attributed to minimal semantic redundancy\\nand tokenization constraints. Conversely, long\\ndocuments showed more moderate improvements\\n(MRR@3 from 0.423 to 0.475), primarily due to\\nembedding truncation that causes loss of context\\nand increased semantic complexity. Future re-\\nsearch should focus explicitly on developing hi-\\nerarchical or segment-based embedding methods\\nto address these limitations.\\nAblation Studies To clearly understand the im-\\npact of the individual components of the frame-\\nwork, we conducted systematic ablation studies\\n(Table 6). Training with positive documents alone\\nproduced only slight gains (+0.03 MRR@3), reaf-\\nfirming the critical role of high-quality hard nega-\\ntives. Evaluating individual embedding models sep-\\narately indicated varying performance due to their\\ndiffering semantic representations and underlying\\ntraining. However, the concatenation of diverse\\nembeddings provided significant performance im-\\nprovements (+0.15 MRR@3), clearly highlighting\\nthe advantages of capturing semantic diversity.\\nAdditionally, PCA-based dimensionality reduc-\\ntion analysis identified the optimal variance thresh-\\nold at 95%. Lower thresholds resulted in marked\\nsemantic degradation, reducing retrieval perfor-\\nmance. This trade-off highlights PCA as an essen-\\ntial efficiency-enhancing step for the framework.\\nCollectively, these detailed analyses underscore\\nour method’s strengths, limitations, and method-\\nological rationale, providing clear empirical justifi-\\ncation for each design decision.\\n# Proposed Strategies MRR@3MRR@10\\n1 Baseline 0.42 0.45\\nPositive Document (PD) Only\\n2 Fine-tuning with PD Only 0.45 0.51\\nHard Negative(HN) with EmbeddingEk\\n3a HN withE1+ PD 0.45 0.51\\n3b HN withE2+ PD 0.47 0.53\\n3c HN withE3+ PD 0.51 0.55\\n3d HN withE4+ PD 0.45 0.52\\n3e HN withE5+ PD 0.48 0.51\\n3f HN withE6+ PD 0.49 0.52\\n3g HN with Xconcat+ PD 0.57 0.64\\nXPCAVariance Impact+ PD\\n4a HN with XPCA(99% Variance)0.57 0.64\\n4b HN with XPCA(95% Variance)0.57 0.64\\n4c HN with XPCA(90% Variance)0.55 0.63\\n4d HN with XPCA(80% Variance)0.51 0.58\\n4e HN with XPCA(70% Variance)0.49 0.56\\nTable 6: Results of ablation study showing the impact\\nof embeddings, PCA variance thresholds, and positive\\ndocuments on MRR, on the in-house re-ranker model.\\n4.2 Case Studies: Examples of Hard Negative\\nImpact\\nFigure 3 shows how similar topics in the domain\\nof cloud computing. To demonstrate the qualitative\\nbenefits of the proposed framework, we present\\ntwo case studies where the baseline and fine-tuned\\nmodels produce different ranking results. These\\nexamples highlight the significance of hard neg-\\natives in distinguishing semantically similar but\\ncontextually irrelevant documents.\\nFigure 3: Illustrations of similar topics in the domain of\\nCloud Computing\\nCase Study 1: Disambiguating Technical\\nAcronyms.\\n• Query (Q): \"What is VCN in Cloud Infras-\\ntructure?\"'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\n• Positive Document (PD): A document ex-\\nplaining \"Virtual Cloud Network (VCN)\" in\\nCloud Infrastructure, detailing its setup and\\nusage.\\n• Hard Negative (HN):A document discussing\\n\"Virtual Network Interface Card (VNIC)\" in\\nthe context of networking hardware.\\nBaseline Result: The baseline model incorrectly\\nranks the hard negative above the positive docu-\\nment due to overlapping terms such as \"virtual\"\\nand \"network.\"\\nProposed Method Result: The fine-tuned model\\nranks the positive document higher, correctly iden-\\ntifying the contextual match between the query\\nand the description of VCN. This improvement\\nis attributed to the triplet loss training with hard\\nnegatives.\\nCase Study 2: Domain-Specific Terminology.\\n• Query (Q): \"How does the CI WAF handle\\nincoming traffic?\"\\n• Positive Document (PD): A document ex-\\nplaining the Web Application Firewall (W AF)\\nin CI, its configuration, and traffic filtering\\nmechanisms.\\n• Hard Negative (HN):A document discussing\\ngeneral firewall configurations in networking.\\nBaseline Result: The baseline model ranks the\\nhard negative higher due to lexical overlap between\\nthe terms \"firewall\" and \"traffic.\"\\nProposed Method Result: The proposed frame-\\nwork ranks the positive document higher, leverag-\\ning domain-specific semantic representations.\\nThese case studies illustrate the practical ad-\\nvantages of training with hard negatives, espe-\\ncially in domains with overlapping terminology\\nor acronyms.\\nAdditional detailed analyses, illustrative prac-\\ntical implications for enterprise applications, and\\nexplicit future directions are discussed in detail in\\nA.4, and A.5.\\n5 Conclusion\\nWe introduced a scalable, modular framework lever-\\naging dynamic ensemble-based hard-negative min-\\ning to significantly enhance re-ranking models in\\nenterprise and domain-specific retrieval scenarios.\\nOur method dynamically curates semantically chal-\\nlenging yet contextually irrelevant negatives, allow-\\ning re-ranking models to effectively discriminate\\nsubtle semantic differences. Empirical evaluations\\non proprietary enterprise data and diverse public\\ndomain-specific benchmarks demonstrated substan-\\ntial improvements of up to 15% in MRR@3 and\\n19% in MRR@10 over state-of-the-art negative\\nsampling techniques, including BM25, In-Batch\\nNegatives, STAR, and ADORE+STAR.\\nOur approach offers clear practical benefits in\\nreal-world deployments, benefiting downstream ap-\\nplications such as knowledge management, cus-\\ntomer support systems, and Retrieval-Augmented\\nGeneration (RAG), where retrieval precision di-\\nrectly influences user satisfaction and Generative\\nAI effectiveness. The strong performance and gen-\\neralizability across various domains further under-\\nscore the framework’s readiness for industry-scale\\ndeployment.\\nFuture work will focus on extending our frame-\\nwork to handle incremental updates of enterprise\\nknowledge bases and exploring real-time negative\\nsampling strategies for continuously evolving cor-\\npora, further enhancing the adaptability and robust-\\nness required in practical industry settings.\\n6 Limitations\\nWhile our approach advances the state of hard\\nnegative mining and encoder-based retrieval, sev-\\neral limitations remain that open avenues for fu-\\nture research. One key challenge is the perfor-\\nmance disparity between short and long documents.\\nAddressing this requires more effective document\\nchunking strategies and the development of hier-\\narchical representations to preserve context across\\nsegments. Additionally, the retrieval of long doc-\\numents is complicated by semantic redundancy\\nand truncation, warranting deeper analysis of their\\nstructural complexity. Our current use of embed-\\nding concatenation for ensembling could also be\\nrefined—future work should evaluate alternative\\nfusion techniques such as weighted averaging or\\nattention-based mechanisms. Moreover, extending\\nthe retrieval framework to support cross-lingual and\\nmultilingual scenarios would enhance its utility in\\nglobally distributed applications.\\nReferences\\nAMIT AGARWAL. 2021. Evaluate generalisation &\\nrobustness of visual features from images to video.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nResearchGate. Available at https://doi.org/10.\\n13140/RG.2.2.33887.53928.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2024a. Synthetic document generation pipeline for\\ntraining artificial intelligence models. US Patent App.\\n17/994,712.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2025. FS-DAG: Few shot domain adapting graph net-\\nworks for visually rich document understanding. In\\nProceedings of the 31st International Conference on\\nComputational Linguistics: Industry Track, pages\\n100–114, Abu Dhabi, UAE. Association for Com-\\nputational Linguistics.\\nAmit Agarwal, Hitesh Patel, Priyaranjan Pattnayak,\\nSrikant Panda, Bhargava Kumar, and Tejaswini Ku-\\nmar. 2024b. Enhancing document ai data genera-\\ntion through graph-based synthetic layouts. arXiv\\npreprint arXiv:2412.03590.\\nJina AI. 2023. jina-reranker-v2-base-multilingual.\\nArian Askari, Mohammad Aliannejadi, Evangelos\\nKanoulas, and Suzan Verberne. 2023. Generating\\nsynthetic documents for cross-encoder re-rankers: A\\ncomparative study of chatgpt and human experts.\\nJiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang,\\nXinnian Liang, Zhao Yan, and Zhoujun Li. 2023.\\nGriprank: Bridging the gap between retrieval and\\ngeneration via the generative knowledge improved\\npassage ranking. Preprint, arXiv:2305.18144.\\nVittorio Castelli, Rishav Chakravarti, Saswati Dana, An-\\nthony Ferritto, Radu Florian, Martin Franz, Dinesh\\nGarg, Dinesh Khandelwal, Scott McCarley, Mike\\nMcCawley, Mohamed Nasr, Lin Pan, Cezar Pen-\\ndus, John Pitrelli, Saurabh Pujar, Salim Roukos, An-\\ndrzej Sakrajda, Avirup Sil, Rosario Uceda-Sosa, Todd\\nWard, and Rong Zhang. 2019. The techqa dataset.\\nPreprint, arXiv:1911.02984.\\nCohere. 2023a. Cohere-embed-multilingual-v3.0.\\nAvailable at: https://cohere.com/blog/\\nintroducing-embed-v3.\\nCohere. 2023b. Reranker model. Available\\nat: https://docs.cohere.com/v2/docs/\\nreranking-with-cohere.\\nGabriel de Souza P. Moreira, Radek Osmulski, Mengyao\\nXu, Ronay Ak, Benedikt Schifferer, and Even\\nOldridge. 2024. Nv-retriever: Improving text em-\\nbedding models with effective hard-negative mining.\\nPreprint, arXiv:2407.15831.\\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leip-\\npold. 2021. Climate-fever: A dataset for veri-\\nfication of real-world climate claims. Preprint,\\narXiv:2012.00614.\\nKaran Dua, Praneet Pabolu, and Mengqing Guo. 2024.\\nGenerating templates for use in synthetic document\\ngeneration processes. US Patent App. 18/295,765.\\nKaran Dua, Praneet Pabolu, and Ranjeet Kumar Gupta.\\n2025. Generation of synthetic doctor-patient conver-\\nsations. US Patent App. 18/495,966.\\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\\nArivazhagan, and Wei Wang. 2022. Language-\\nagnostic bert sentence embedding. Preprint,\\narXiv:2007.01852.\\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\\ntraining architecture for dense retrieval. EMNLP\\n2021 - 2021 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings, pages\\n981–993.\\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\\nChowdhury, Ankita Naik, Pengshan Cai, and Al-\\nfio Gliozzo. 2022. Re2G: Retrieve, rerank, gen-\\nerate. In Proceedings of the 2022 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, pages 2701–2715, Seattle, United\\nStates. Association for Computational Linguistics.\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\\nWu. 2023. How close is chatgpt to human experts?\\ncomparison corpus, evaluation, and detection.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training.\\nEK Jasila, N Saleena, and KA Abdul Nazeer. 2023. An\\nefficient document clustering approach for devising\\nsemantic clusters. Cybernetics and Systems, pages\\n1–18.\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen Tau Yih. 2020. Dense passage retrieval\\nfor open-domain question answering. EMNLP\\n2020 - 2020 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings of the\\nConference, pages 6769–6781.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan\\nRaiman, Mohammad Shoeybi, Bryan Catanzaro, and\\nWei Ping. 2024. Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models. arXiv\\npreprint arXiv:2405.17428.\\nFulu Li, Zhiwen Xie, and Guangyou Zhou. 2024.\\nTheme-enhanced hard negative sample mining for\\nopen-domain question answering. In ICASSP 2024 -\\n2024 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages\\n12436–12440.\\nXianming Li and Jing Li. 2023. Angle-optimized text\\nembeddings. arXiv preprint arXiv:2309.12871.\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih\\nYavuz, Caiming Xiong, and Philip S. Yu. 2021.\\nDense hierarchical retrieval for open-domain ques-\\ntion answering. In Conference on Empirical\\nMethods in Natural Language Processing.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nSean MacAvaney, Andrew Yates, Arman Cohan,\\nand Nazli Goharian. 2019. Cedr: Contextual-\\nized embeddings for document ranking. SIGIR\\n2019 - Proceedings of the 42nd International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1101–1104.\\nAndrzej Ma´ckiewicz and Waldemar Ratajczak. 1993.\\nPrincipal components analysis (pca). Computers &\\nGeosciences, 19(3):303–342.\\nLeland McInnes, John Healy, and James Melville.\\n2020. Umap: Uniform manifold approximation\\nand projection for dimension reduction. Preprint,\\narXiv:1802.03426.\\nHansa Meghwani. 2024. Enhancing retrieval perfor-\\nmance: An ensemble approach for hard negative min-\\ning. Preprint, arXiv:2411.02404.\\nVivek Mehta, Mohit Agarwal, and Rohit Kumar Kaliyar.\\n2024. A comprehensive and analytical review of text\\nclustering techniques. International Journal of Data\\nScience and Analytics, pages 1–20.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024a. Sfr-\\nembedding-2: Advanced text embedding with multi-\\nstage training.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024b. Sfr-\\nembedding-mistral: Enhance text retrieval with trans-\\nfer learning. Salesforce AI Research Blog.\\nThanh-Do Nguyen, Chi Minh Bui, Thi-Hai-Yen Vuong,\\nand Xuan-Hieu Phan. 2022. Passage-based bm25\\nhard negatives: A simple and effective negative sam-\\npling strategy for dense retrieval.\\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\\nre-ranking with bert.\\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\\nJimmy Lin. 2019. Multi-stage document ranking\\nwith bert.\\nZach Nussbaum, John X. Morris, Brandon Duderstadt,\\nand Andriy Mulyar. 2024. Nomic embed: Training\\na reproducible long context text embedder. Preprint,\\narXiv:2402.01613.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024a. Multi-lingual natural language generation.\\nUS Patent App. 18/318,315.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024b. Multi-lingual natural language generation.\\nUS Patent App. 18/318,327.\\nSrikant Panda, Amit Agarwal, Gouttham Nambirajan,\\nand Kulbhushan Pachauri. 2025a. Out of distribution\\nelement detection for information extraction. US\\nPatent App. 18/347,983.\\nSrikant Panda, Amit Agarwal, and Kulbhushan Pachauri.\\n2025b. Techniques of information extraction for se-\\nlection marks. US Patent App. 18/240,344.\\nHitesh Laxmichand Patel, Amit Agarwal, Arion Das,\\nBhargava Kumar, Srikant Panda, Priyaranjan Pat-\\ntnayak, Taki Hasan Rafi, Tejaswini Kumar, and Dong-\\nKyu Chae. 2025. Sweeval: Do llms really swear?\\na safety benchmark for testing limits for enterprise\\nuse. In Proceedings of the 2025 Conference of the\\nNations of the Americas Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies (V olume3: Industry Track), pages\\n558–582.\\nHitesh Laxmichand Patel, Amit Agarwal, Bhargava\\nKumar, Karan Gupta, and Priyaranjan Pattnayak.\\n2024. Llm for barcodes: Generating diverse syn-\\nthetic data for identity documents. arXiv preprint\\narXiv:2411.14962.\\nPriyaranjan Pattnayak, Amit Agarwal, Hansa Megh-\\nwani, Hitesh Laxmichand Patel, and Srikant Panda.\\n2025a. Hybrid ai for responsive multi-turn on-\\nline conversations with novel dynamic routing and\\nfeedback adaptation. In Proceedings of the 4th\\nInternational Workshop on Knowledge-Augmented\\nMethods for Natural Language Processing, pages\\n215–229.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, and\\nAmit Agarwal. 2025b. Tokenization matters: Im-\\nproving zero-shot ner for indic languages. Preprint,\\narXiv:2504.16977.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit\\nAgarwal, Bhargava Kumar, Srikant Panda, and Te-\\njaswini Kumar. 2025c. Clinical qa 2.0: Multi-task\\nlearning for answer extraction and categorization.\\nPreprint, arXiv:2502.13108.\\nRonak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, An-\\ndrew Yates, and Jimmy Lin. 2022. Squeezing wa-\\nter from a stone: A bag of tricks for further im-\\nproving cross-encoder effectiveness for reranking.\\nIn Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in Artificial Intelligence and\\nLecture Notes in Bioinformatics), volume 13185\\nLNCS, pages 655–670. Springer Science and Busi-\\nness Media Deutschland GmbH.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\\nHaifeng Wang. 2020. Rocketqa: An optimized train-\\ning approach to dense passage retrieval for open-\\ndomain question answering.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\\nSentence embeddings using siamese bert-networks.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing.\\nS. E. Robertson and S. Walker. 1994. Some Simple\\nEffective Approximations to the 2-Poisson Model\\nfor Probabilistic Weighted Retrieval, pages 232–241.\\nSpringer London.\\nSaba Sturua, Isabelle Mohr, Mohammad Kalim Akram,\\nMichael Günther, Bo Wang, Markus Krimmel, Feng\\nWang, Georgios Mastrapas, Andreas Koukounas, An-\\ndreas Koukounas, Nan Wang, and Han Xiao. 2024.\\njina-embeddings-v3: Multilingual embeddings with\\ntask lora. Preprint, arXiv:2409.10173.\\nTheFinAI. 2018. Fiqa: A financial question answering\\ndataset. Available at Hugging Face.\\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\\nVisualizing data using t-sne. Journal of machine\\nlearning research, 9(11).\\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\\nRangan Majumder, and Furu Wei. 2023. Improving\\ntext embeddings with large language models. arXiv\\npreprint arXiv:2401.00368.\\nSvante Wold, Kim H. Esbensen, Kim H. Esbensen, Paul\\nGeladi, and Paul Geladi. 1987. Principal component\\nanalysis. Chemometrics and Intelligent Laboratory\\nSystems, 2:37–52.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding. Preprint,\\narXiv:2309.07597.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2020. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval.\\nZhen Yang, Zhou Shao, Yuxiao Dong, and Jie Tang.\\n2024. Trisampler: A better negative sampling princi-\\nple for dense retrieval. Preprint, arXiv:2402.11855.\\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\\nZhang, and Shaoping Ma. 2021. Optimizing dense\\nretrieval model training with hard negatives. SIGIR\\n2021 - Proceedings of the 44th International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1503–1512.\\nDun Zhang. 2024. stella-embedding-model-2024.\\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie,\\nZiqi Dai, Jialong Tang, Huan Lin, Baosong Yang,\\nPengjun Xie, Fei Huang, et al. 2024. mgte: General-\\nized long-context text representation and reranking\\nmodels for multilingual text retrieval. arXiv preprint\\narXiv:2407.19669.\\nA Appendix\\nA.1 Extended Related Work\\nHard Negatives in Retrieval Models Static and\\ndynamic hard negatives have been extensively stud-\\nied. Static negatives, such as those generated\\nby BM25 (Robertson and Walker, 1994) or Pas-\\nsageBM25 (Nguyen et al., 2022), provide challeng-\\ning lexical contrasts but risk overfitting due to their\\nfixed nature (Qu et al., 2020). Dynamic negatives,\\nas introduced in ANCE (Xiong et al., 2020) and\\nADORE (Zhan et al., 2021) adapt during training,\\nother effective methods like positive-aware mining\\n(de Souza P. Moreira et al., 2024), theme-enhanced\\nnegatives (Li et al., 2024) offers relevant chal-\\nlenges but incurring high computational costs due\\nto periodic re-indexing and bigger embedding di-\\nmension. Our framework mitigates these issues by\\nleveraging clustering and dimensionality reduction\\nto dynamically identify negatives without requiring\\nre-indexing.\\nLocalized Contrastive Estimation (LCE) (Guo\\net al., 2023; AGARWAL, 2021) further demon-\\nstrated the effectiveness of incorporating hard nega-\\ntives into cross-encoder training, improving rerank-\\ning accuracy when negatives align with retriever\\noutputs. Additionally, (Pradeep et al., 2022) high-\\nlighted the importance of hard negatives even in\\nadvanced pretraining setups like Condenser (Gao\\nand Callan, 2021), which emphasizes their neces-\\nsity for robust optimization.\\nAdvances in Dense Retrieval and Cross-\\nEncoders Dense retrieval models like\\nDPR (Karpukhin et al., 2020) and REALM (Guu\\net al., 2020) encode queries and documents into\\ndense embeddings, enabling semantic matching.\\nRecent advances in dense retrieval and ranking\\ninclude GripRank’s generative knowledge-driven\\npassage ranking (Bai et al., 2023), Dense\\nHierarchical Retrieval’s multi-stage framework\\nfor efficient question answering (Liu et al., 2021;\\nPattnayak et al., 2025a,c,b; Patel et al., 2025), and\\nTriSampler’s optimized negative sampling for\\ndense retrieval (Yang et al., 2024), collectively\\nenhancing retrieval performance.Cross-encoders,\\nsuch as monoBERT (Nogueira et al., 2019;\\nNogueira and Cho, 2019), further improve retrieval\\nprecision by jointly encoding query-document\\npairs but require high-quality training data,\\nparticularly challenging negatives (MacAvaney\\net al., 2019; Panda et al., 2025b). Techniques such'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nas synthetic data generation (Askari et al., 2023;\\nAgarwal et al., 2024a, 2025) augment training\\ndatasets but lack the realism and semantic depth\\nprovided by our hard negative mining approach.\\nDimensionality Reduction in IR Clustering\\nmethods have been used to group semantically\\nsimilar documents, improving retrieval efficiency\\nand training data organization (Mehta et al., 2024;\\nJasila et al., 2023; Dua et al., 2025; Panda et al.,\\n2025a). Dimensionality reduction techniques like\\nPCA (Wold et al., 1987) enhance scalability by re-\\nducing computational complexity. Our framework\\nuniquely combines these techniques to dynamically\\nidentify negatives that challenge retrieval models\\nin a scalable manner.\\nSynthetic Data in Retrieval Recent\\nwork (Askari et al., 2023; Agarwal et al.,\\n2024a,b; Patel et al., 2024; Dua et al., 2024; Pabolu\\net al., 2024a,b) has explored using large language\\nmodels to generate synthetic training data for\\nretrieval tasks. While effective in low-resource\\nsettings, synthetic data often struggles with factual\\ninaccuracies and domain-specific relevance. In\\ncontrast, our framework relies on real-world data\\nto curate semantically challenging negatives,\\nensuring high-quality training samples without\\nintroducing synthetic biases.\\nSummary of Contributions While previous\\nworks address various aspects of negative sampling,\\nhard negatives, and synthetic data, our approach\\nbridges the gap between static and dynamic strate-\\ngies. By dynamically curating negatives using clus-\\ntering and dimensionality reduction, we achieve\\na scalable and semantically precise methodology\\ntailored to domain-specific retrieval tasks.\\nA.2 Extended Methodology\\nA.2.1 Dataset Statistics\\nQueries Length Distribution In this section we\\nanalyze the distribution of queries length in our\\nenterprise dataset. Figure 4 shows that the length\\nof queries ranges from 1 to 25 words, with some\\nqueries having very few words. This highlights that\\nuser queries can sometime be just 2-3 words about\\na topic, increasing the probability of retrieving doc-\\numents mentioning those topics or concepts which\\ncan be contextually different. Therefore, when\\nwe select hard negatives, it is crucial to consider\\nnot only the relationship between the query and\\ndocuments but also the relationship between the\\nFigure 4: Length Distribution of queries in the dataset.\\npositive document and other documents, ensuring a\\ncomparison with texts on similar topics and similar\\nlengths.\\nModel (Ek) Params (M) Dimension Max Tokens\\nstella_en_400M_v5 435 8192 8192\\njina-embeddings-v3 572 1024 8194\\n(multilingual)\\nmxbai-embed-large-v1 335 1024 512\\nbge-large-en-v1.5 335 1024 512\\nLaBSE 471 768 256\\n(multilingual)\\nall-mpnet-base-v2 110 768 514\\n(multilingual)\\nTable 7: Embedding models used to construct Xconcat,\\ncombining diverse semantic representations for queries\\n(Q), positive documents (P D), and corpus documents\\n(D).\\nFigure 5: Shows document length distribution in Enter-\\nprise corpus.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nDocument Length Distribution As shown in\\nFigure 5 , document lengths are significantly longer\\nthan query lengths. This disparity in context length\\naffects the similarity scores, potentially reducing\\nthe accuracy of retrieval systems. In our in-house\\ndataset, each query is paired with a single correct\\ndocument (though its not limited by number of\\npositive-negative document per query). This posi-\\ntive document is crucial for identifying challenging\\nhard negatives and hence helpful for encoder-based\\nmodel training.\\nA.2.2 Embedding Models\\nTable 7 lists the embedding models (Zhang, 2024;\\nSturua et al., 2024; Li and Li, 2023; Xiao et al.,\\n2023; Feng et al., 2022; Reimers and Gurevych,\\n2019; Zhang et al., 2024) used to construct Xconcat,\\ncombining diverse semantic representations for\\nqueries ( Q), positive documents ( P D), and cor-\\npus documents (D). These models were selected\\nfor their performance, model size, ability to han-\\ndle multilingual context, providing complemen-\\ntary strengths in dimensionality and token cover-\\nage. By integrating embeddings from these models,\\nthe framework captures nuanced semantic relation-\\nships crucial for reranker training.\\nA.2.3 Unified Contrastive Loss\\nThe unified contrastive loss is designed to improve\\nranking precision for both bi-encoders and cross-\\nencoders, by ensuring that positive documents\\n(P D) are ranked closer to the query (Q) than hard\\nnegatives (DHN ) by a margin m. The loss is de-\\nfined as:\\nL =\\nNX\\ni=1\\nmax (0, m + d(Qi, P Di) − d(Qi, DHN i))\\n(7)\\nwhere:\\n• P Di: Positive document associated with\\nquery Qi.\\n• DHN i: Hard negative document, semantically\\nsimilar to P Di but contextually irrelevant.\\n• d(Qi, Di): Distance metric measuring rele-\\nvance between Qi and Di.\\n• m: Margin ensuring P Di is closer to Qi than\\nDHN i by at least m, encouraging the model\\nto distinguish between relevant and irrelevant\\ndocuments effectively.\\nFor bi-encoders, the distance metric is defined as:\\nd(Qi, Di) = 1 − cosine(eQi, eDi), (8)\\nwhere eQi and eDi are the embeddings of the query\\nand document, respectively. For cross-encoders,\\nthe distance metric is:\\nd(Qi, Di) = −s(Qi, Di), (9)\\nwhere s(Qi, Di) is the cross-encoder’s relevance\\nscore for the query-document pair.\\nThis formulation leverages the triplet of (Q, P D,\\nDHN ) to minimize d(Qi, P Di), pulling positive\\ndocuments closer to the query, while maximizing\\nd(Qi, DHN i), pushing hard negatives further away.\\nBy emphasizing semantically challenging exam-\\nples, the model learns sharper decision boundaries\\nfor improved ranking precision.\\nA.3 Experimental Setup\\nDatasets We evaluate our framework extensively\\nusing both proprietary and public datasets:\\n• Internal Proprietary Dataset: Consisting\\nof approximately 5250 query-document pairs,\\non cloud services like computing, networking,\\nfirewall, ai services. It includes both short (<\\n[1024 tokens]) and long documents (>=[1024\\ntokens]).\\n• FiQA Dataset: A financial domain-specific\\ndataset widely used for retrieval benchmark-\\ning.\\n• Climate-FEVER Dataset: An environment-\\nspecific fact-checking dataset focused on\\nclimate-related information retrieval.\\n• TechQA Dataset: A technical question-\\nanswering dataset emphasizing software engi-\\nneering and technology-related queries.\\nTraining and Fine-tuning All re-ranking mod-\\nels are fine-tuned using a triplet loss with margin\\nwith same hyper-parameters. Early stopping is em-\\nployed based on validation MRR@10 scores to\\nprevent overfitting.\\nEvaluation Metrics Model performance is eval-\\nuated using standard retrieval metrics: Mean Recip-\\nrocal Rank (MRR) at positions 3 and 10 (MRR@3\\nand MRR@10), which measure retrieval quality\\nand ranking precision. Each reported metric is\\naveraged across three experimental runs for robust-\\nness.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nStrategy Training Data MRR@3 MRR@10\\nBaseline 0 0.42 0.45\\nFinetuned withHard Negatives\\n(Ours)\\n100 0.46 0.49\\n200 0.48 0.51\\n300 0.50 0.53\\n400 0.52 0.56\\n500 0.52 0.58\\n600 0.54 0.60\\n700 0.54 0.62\\n800 0.56 0.63\\n900 0.57 0.64\\n1000 0.57 0.64\\nTable 8: Comparison of Strategies with Varying Train-\\ning Data Sizes\\nA.4 Extended Results & Ablation\\nImpact of Training Data Size As shown in Ta-\\nble 8, both MRR@3 and MRR@10 improve as the\\ntraining data size increases, with more pronounced\\ngains in MRR@10. MRR@3 shows gradual im-\\nprovement, from 0.42 at the baseline to 0.57 with\\n100 examples, highlighting the model’s enhanced\\nability to rank relevant documents within the top 3.\\nMRR@10, on the other hand, shows more signif-\\nicant improvement, from 0.45 to 0.64, indicating\\nthat the model benefits more from additional data\\nwhen considering the top 10 ranked documents.\\nOur method shows promising results even with\\nsmaller training sets, demonstrating the effective-\\nness of incorporating hard negatives early in the\\ntraining process. This suggests that hard negatives\\nsignificantly enhance the model’s ability to distin-\\nguish relevant from irrelevant documents against\\na given query, even when data is limited. This ap-\\nproach is particularly beneficial in enterprise con-\\ntexts, where annotated data may be scarce, enabling\\nquicker improvements in domain-specific retrieval\\nperformance.\\nModels in the Study In our study we com-\\npared the performance of other finetuned re-ranker\\n(Glass et al., 2022; Wang et al., 2023; Raffel et al.,\\n2020) and embedding models (Zhang et al., 2024;\\nNussbaum et al., 2024) using hard negatives gen-\\nerated by our proposed framework in Table 4.\\nWe benchmarked the BGE-Reranker (Xiao et al.,\\n2023), NV-Embed (Lee et al., 2024) Salesforce-\\nSFR (Meng et al., 2024a,b) , jina-reranker (AI,\\n2023) and Cohere-Reranker (Cohere, 2023a,b),\\nA.4.1 Analysis of Long vs. Short Documents\\nTable 5 reveals a consistent disparity in MRR\\nscores between short and long documents, with\\nlong documents showing lower performance. Here,\\nwe analyze potential reasons and propose mitiga-\\ntion strategies.\\nChallenges with Long Documents.\\n• Semantic Redundancy: Long documents of-\\nten contain repetitive or tangential content,\\ndiluting their relevance to a specific query.\\n• Context Truncation: Fixed-length tokeniza-\\ntion (e.g., 512 or 1024 tokens) truncates long\\ndocuments, potentially discarding critical in-\\nformation.\\n• Query-to-Document Mismatch: Short\\nqueries may not provide sufficient context to\\nmatch the nuanced information spread across\\na lengthy document.\\nPotential Solutions.\\n• Chunk-Based Retrieval: Split long doc-\\numents into smaller, semantically coherent\\nchunks and rank them individually.\\n• Hierarchical Embeddings: Use hierarchical\\nmodels to aggregate sentence- or paragraph-\\nlevel embeddings for better context represen-\\ntation.\\n• Query Expansion: Enhance short queries\\nwith additional context using techniques like\\nquery rewriting or pseudo-relevance feedback.\\nThis analysis highlights the need for future work\\nto address the inherent challenges of ranking long\\ndocuments effectively.\\nA.5 Practical Implications for Enterprise\\nApplications\\nThe proposed framework has significant practical\\nimplications for enterprise information retrieval\\nsystems, particularly in retrieval-augmented gener-\\nation (RAG) pipelines.\\nImproved Ranking Precision. By training with\\nhard negatives, the model ensures that the most\\nrelevant documents are retrieved for each query.\\nThis is particularly critical for enterprise use cases\\nsuch as:\\n• Technical Support: Retrieving precise docu-\\nmentation for customer queries, reducing res-\\nolution times.\\n• Knowledge Management: Ensuring that em-\\nployees access the most relevant internal re-\\nsources quickly.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nEnhanced Generative Quality. High-quality re-\\ntrieval directly improves the factual accuracy and\\ncoherence of outputs generated by large language\\nmodels in RAG pipelines. For example:\\n• Documentation Summarization: Sum-\\nmaries generated by models like GPT are\\nmore reliable when based on top-ranked, ac-\\ncurate sources.\\n• Customer Interaction: Chatbots generate\\nmore contextually relevant responses when\\nfed precise retrieved documents.\\nScalability and Adaptability. The framework’s\\nmodular design, including the use of diverse embed-\\ndings and clustering-based hard negative selection,\\nallows it to adapt to:\\n• Different industries (e.g., healthcare, finance,\\nmanufacturing).\\n• Multi-lingual or cross-lingual retrieval tasks.\\nThese practical implications underscore the ver-\\nsatility and enterprise readiness of the proposed\\nframework.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = ( z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax( QK T\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head 1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matricesW Q\\ni ∈ Rdmodel×dk, W K\\ni ∈ Rdmodel×dk, W V\\ni ∈ Rdmodel×dv\\nand W O ∈ Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0 , xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndf f = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel)\\nP E(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10 −9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0 .1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0 .6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (V olume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64e93aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d7e98ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 29 documents into 119 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: arXiv:2505.18366v1  [cs.IR]  23 May 2025\n",
      "Accepted in ACL 2025\n",
      "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems\n",
      "Hansa Meghwani*, Amit Agarwal*, Priyaranjan Pattnayak,\n",
      "Hitesh Lax...\n",
      "Metadata: {'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='arXiv:2505.18366v1  [cs.IR]  23 May 2025\\nAccepted in ACL 2025\\nHard Negative Mining for Domain-Specific Retrieval in Enterprise Systems\\nHansa Meghwani*, Amit Agarwal*, Priyaranjan Pattnayak,\\nHitesh Laxmichand Patel, Srikant Panda\\nOracle AI\\nCorrespondence: hansa.meghwani@oracle.com; amit.h.agarwal@oracle.com *\\nAbstract\\nEnterprise search systems often struggle to re-\\ntrieve accurate, domain-specific information\\ndue to semantic mismatches and overlapping\\nterminologies. These issues can degrade the\\nperformance of downstream applications such\\nas knowledge management, customer support,\\nand retrieval-augmented generation agents. To\\naddress this challenge, we propose a scal-\\nable hard-negative mining framework tailored\\nspecifically for domain-specific enterprise data.\\nOur approach dynamically selects semantically\\nchallenging but contextually irrelevant docu-\\nments to enhance deployed re-ranking models.\\nOur method integrates diverse embedding mod-\\nels, performs dimensionality reduction, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='challenging but contextually irrelevant docu-\\nments to enhance deployed re-ranking models.\\nOur method integrates diverse embedding mod-\\nels, performs dimensionality reduction, and\\nuniquely selects hard negatives, ensuring com-\\nputational efficiency and semantic precision.\\nEvaluation on our proprietary enterprise corpus\\n(cloud services domain) demonstrates substan-\\ntial improvements of 15% in MRR@3 and 19%\\nin MRR@10 compared to state-of-the-art base-\\nlines and other negative sampling techniques.\\nFurther validation on public domain-specific\\ndatasets (FiQA, Climate Fever, TechQA) con-\\nfirms our method’s generalizability and readi-\\nness for real-world applications.\\n1 Introduction\\nAccurate retrieval of domain-specific information\\nsignificantly impacts critical enterprise processes,\\nsuch as knowledge management, customer sup-\\nport, and Retrieval Augmented Generation (RAG)\\nAgents. However, achieving precise retrieval re-\\nmains challenging due to semantic mismatches,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='such as knowledge management, customer sup-\\nport, and Retrieval Augmented Generation (RAG)\\nAgents. However, achieving precise retrieval re-\\nmains challenging due to semantic mismatches,\\noverlapping terminologies, and ambiguous abbre-\\nviations common in specialized fields like finance,\\nand cloud computing. Traditional lexical retrieval\\ntechniques, such as BM25 (Robertson and Walker,\\n1994), struggle due to vocabulary mismatches, lead-\\ning to irrelevant results and poor user experience.\\n*The authors contributed equally to this work.\\nRecent dense retrieval approaches leveraging\\npre-trained language models, like BERT-based en-\\ncoders (Karpukhin et al., 2020; Xiong et al., 2020;\\nGuu et al., 2020), mitigate lexical limitations by\\ncapturing semantic relevance. Nevertheless, their\\nperformance heavily relies on the negative sam-\\nples—documents incorrectly retrieved due to se-\\nmantic similarity but lacking contextual relevance.\\nModels trained with negative sampling methods'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='performance heavily relies on the negative sam-\\nples—documents incorrectly retrieved due to se-\\nmantic similarity but lacking contextual relevance.\\nModels trained with negative sampling methods\\n(e.g., random sampling, BM25-based static sam-\\npling, or dynamic methods like ANCE (Xiong\\net al., 2020), STAR (Zhan et al., 2021)) either\\nlack sufficient semantic discrimination or incur\\nhigh computational costs, thus limiting scalability\\nand practical enterprise deployment. For instance,\\ngiven a query such as \"Steps to deploy a MySQL\\ndatabase on Cloud Infrastructure,\" most negative\\nsampling techniques select documents discussing\\nnon-MySQL database deployments. Conversely,\\nour method strategically selects a hard negative dis-\\ncussing MySQL deployment on-premises, which\\ndespite semantic overlap, is contextually distinct\\nand thus poses a stronger training challenge for the\\nretrieval and re-ranking models.\\nOur proposed framework addresses these by in-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='despite semantic overlap, is contextually distinct\\nand thus poses a stronger training challenge for the\\nretrieval and re-ranking models.\\nOur proposed framework addresses these by in-\\ntroducing a novel semantic selection criterion ex-\\nplicitly designed to curate high-quality hard nega-\\ntives. By uniquely formulating two semantic con-\\nditions that effectively select negatives that closely\\nresemble query semantics but remain contextually\\nirrelevant, significantly minimizing false negatives\\nencountered by existing techniques. The main con-\\ntributions of this paper are:\\n1. A negative mining framework for dynamically\\nselecting semantically challenging hard neg-\\natives, leveraging diverse embedding models\\nand semantic filtering criteria to significantly\\nimprove re-ranking models in domain-specific\\nretrieval scenarios.\\n2. Comprehensive evaluations demonstrating'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nconsistent and significant improvements\\nacross both proprietary and publicly available\\ndatasets, verifying our method’s impact and\\nbroad applicability across domain-specific\\nusecases.\\n3. In-depth analysis, of critical challenges in han-\\ndling both short and long-form enterprise doc-\\numents, laying a clear foundation for targeted\\nfuture improvements.\\nOur work directly enhances the semantic dis-\\ncrimination capabilities of re-ranking models, re-\\nsulting in 15% improvement in MRR@3 and\\n19% improvement in MRR@10 on our in-house\\ncloud-services domain dataset. Further evaluations\\non public domain-specific benchmarks (FiQA, Cli-\\nmate Fever, TechQA) confirm generalizability and\\ntangible improvements of our proposed negative\\nmining framework.\\n2 Related Work\\n2.1 Hard Negatives in Retrieval Models\\nThe role of hard negatives in training dense re-\\ntrieval models has been widely studied. Static\\nnegatives, such as BM25 (Robertson and Walker,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='2 Related Work\\n2.1 Hard Negatives in Retrieval Models\\nThe role of hard negatives in training dense re-\\ntrieval models has been widely studied. Static\\nnegatives, such as BM25 (Robertson and Walker,\\n1994), provide lexical similarity but fail to capture\\nsemantic relevance, often leading to overfitting (Qu\\net al., 2020). Dynamic negatives, introduced in\\nANCE (Xiong et al., 2020) and STAR (Zhan et al.,\\n2021), adapt during training to provide more chal-\\nlenging contrasts but require significant computa-\\ntional resources due to periodic re-indexing. Our\\nframework addresses these limitations by dynam-\\nically identifying semantically challenging nega-\\ntives using clustering and dimensionality reduction,\\nensuring scalability and adaptability.\\nFurther studies have explored advanced meth-\\nods for negative sampling in cross-encoder mod-\\nels (Meghwani, 2024). Localized Contrastive Es-\\ntimation (LCE) (Guo et al., 2023) integrates hard\\nnegatives into cross-encoder training, improving'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='ods for negative sampling in cross-encoder mod-\\nels (Meghwani, 2024). Localized Contrastive Es-\\ntimation (LCE) (Guo et al., 2023) integrates hard\\nnegatives into cross-encoder training, improving\\nthe reranking performance when negatives align\\nwith the output of the retriever. Similarly, (Pradeep\\net al., 2022) demonstrated the importance of hard\\nnegatives even when models undergo advanced pre-\\ntraining techniques, such as condenser (Gao and\\nCallan, 2021). Our work builds on these efforts by\\noffering a scalable approach, which can be applied\\nto any domain-heavy enterprise data.\\n2.2 Negative Sampling Strategies\\nEffective negative sampling significantly affects the\\nperformance of the retrieval model by challenging\\nthe model to differentiate between relevant and\\nirrelevant examples. Common strategies include:\\n• Random Negatives: Efficient but lacking se-\\nmantic contrast, leading to suboptimal perfor-\\nmance (Karpukhin et al., 2020).\\n• BM25 Negatives: Leverage lexical similar-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='• Random Negatives: Efficient but lacking se-\\nmantic contrast, leading to suboptimal perfor-\\nmance (Karpukhin et al., 2020).\\n• BM25 Negatives: Leverage lexical similar-\\nity, but often introduce biases, particularly\\nin semantically rich domains (Robertson and\\nWalker, 1994).\\n• In-Batch Negatives: Computationally ef-\\nficient but limited to local semantic con-\\ntrasts, often underperforming in dense re-\\ntrieval tasks (Xiong et al., 2020).\\nOur framework complements these approaches\\nby dynamically generating negatives that balance\\nsemantic similarity and contextual irrelevance,\\navoiding the pitfalls of static or random methods.\\n2.3 Domain-Specific Retrieval Challenges\\nEnterprise retrieval systems face unique challenges,\\nsuch as ambiguous terminology, overlapping con-\\ncepts, and private datasets (Meghwani, 2024).\\nGeneral-purpose methods such as BM25 or dense\\nretrieval models (Qu et al., 2020) fail to capture\\ndomain-specific complexities effectively. Our ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='cepts, and private datasets (Meghwani, 2024).\\nGeneral-purpose methods such as BM25 or dense\\nretrieval models (Qu et al., 2020) fail to capture\\ndomain-specific complexities effectively. Our ap-\\nproach addresses these gaps by curating hard nega-\\ntives that align with enterprise-specific semantics,\\nimproving retrieval precision and robustness for\\nproprietary datasets.\\nWe further discuss negative sampling techniques in\\nAppendix A.1.\\n3 Methodology\\nTo effectively train and finetune reranker models\\nfor domain-specific retrieval, it is essential to sys-\\ntematically handle technical ambiguities stemming\\nfrom specialized terminologies, overlapping con-\\ncepts, and abbreviations prevalent within enterprise\\ndomains.\\nWe propose a structured, modular framework\\nthat integrates diverse embedding models, dimen-\\nsionality reduction, and a novel semantic criterion\\nfor hard-negative selection. Figure 1 illustrates the\\nhigh-level pipeline, components and their interac-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='sionality reduction, and a novel semantic criterion\\nfor hard-negative selection. Figure 1 illustrates the\\nhigh-level pipeline, components and their interac-\\ntions. The re-ranking models fine-tuned using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nFigure 1: Overview of the methodology pipeline for training reranker models, including embedding generation,\\nPCA-based dimensionality reduction and hard negative selection for fine-tuning.\\nhard negatives generated by our framework are di-\\nrectly deployed in downstream applications, such\\nas RAG, significantly improving the resolution of\\ncustomer queries through enhanced retrieval.\\nOur approach begins by encoding queries and\\ndocuments into semantically rich vector represen-\\ntations using an ensemble of state-of-the-art bi-\\nencoder embedding models. These embeddings are\\nstrategically selected based on multilingual sup-\\nport, embedding quality, training data diversity,\\ncontext length handling, and performance (details\\nprovided in Appendix A.2. To manage embed-\\nding dimensionality and improve computational\\nefficiency, Principal Component Analysis (PCA)\\n(Ma´ckiewicz and Ratajczak, 1993) is utilized to\\nproject the concatenated embeddings onto a lower-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='ding dimensionality and improve computational\\nefficiency, Principal Component Analysis (PCA)\\n(Ma´ckiewicz and Ratajczak, 1993) is utilized to\\nproject the concatenated embeddings onto a lower-\\ndimensional space, maintaining 95% of the original\\nvariance.\\nWe then define two semantic conditions (Eq. 5\\nand Eq. 6) to dynamically select high-quality hard\\nnegatives, addressing semantic similarity chal-\\nlenges and minimizing false negatives. Together,\\nthese two equations ensure that the selected hard\\nnegative is not only close to the query (Eq. 5) but\\nalso contextually distinct from the true positive,\\nminimizing the risk of selecting topic duplicates\\nor noisy positives (Eq. 6). For example, a query\\nabout deploying MySQL on Oracle Cloud, PD is a\\nguide on that topic, and D is a doc about MySQL\\non-premise — semantically close to Q, but distant\\nfrom PD.\\nBelow we detail each methodological compo-\\nnent, emphasizing their contributions to enhancing\\nretrieval precision in domain-specific or enterprise'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='from PD.\\nBelow we detail each methodological compo-\\nnent, emphasizing their contributions to enhancing\\nretrieval precision in domain-specific or enterprise\\nretrieval tasks.\\nTotal Train Test\\n< Q, P D > 5250 1000 4250\\nTable 1: Dataset distribution of queries (Q) and positive\\ndocuments (PD).\\n3.1 Dataset Statistics\\nOur experiments leverage a proprietary corpus con-\\ntaining 36,871 unannotated documents sourced\\nfrom over 30 enterprise cloud services. Addition-\\nally, we prepared 5250 annotated query-positive\\ndocument pairs ( < Q, P D > ) for training and\\ntesting. Notably, we adopted a non-standard train-\\ntest split (as summarized in Table 1), allocating\\nfour times more data to testing than training to\\nrigorously evaluate model robustness against vary-\\ning training data volumes (additional analyses in\\nAppendix A.4). To further validate generaliz-\\nability, we conduct evaluations on publicly avail-\\nable domain-specific benchmarks: FiQA (finance)\\n(TheFinAI, 2018), Climate Fever (climate science)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Appendix A.4). To further validate generaliz-\\nability, we conduct evaluations on publicly avail-\\nable domain-specific benchmarks: FiQA (finance)\\n(TheFinAI, 2018), Climate Fever (climate science)\\n(Diggelmann et al., 2021), and TechQA (technol-\\nogy) (Castelli et al., 2019). Detailed dataset statis-\\ntics are provided in Appendix A.2.1.\\n3.2 Embedding Generation\\nEmbeddings for queries, positive documents, and\\nthe corpus are computed via six diverse, high-\\nperformance bi-encoder models E1, E2, . . . , E6,\\neach selected strategically for capturing comple-\\nmentary semantic perspectives:\\nEk(x) ∈ Rdk (1)\\nwhere dk is the embedding dimension of the kth\\nmodel for textual input x. Concatenation of these'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nembeddings yields a comprehensive representation:\\nXconcat = [e1(x); e2(x); . . . ; e6(x)] (2)\\nwhere Xconcat ∈ R\\nP6\\nk=1 dk represents the con-\\ncatenated embedding for the input x.\\n3.3 Dimensionality Reduction\\nTo alleviate the computational overhead arising\\nfrom high-dimensional concatenated embeddings,\\nwe apply PCA to reduce dimensionality while pre-\\nserving semantic richness:\\nXPCA = XconcatP, (3)\\nwhere P represents the PCA projection matrix.\\nWe specifically select PCA due to its computational\\nefficiency, and scalability, essential given our large\\nenterprise corpus and high-dimensional embedding\\nspace. While we empirically evaluated nonlinear\\ndimensionality reduction methods such as UMAP\\n(McInnes et al., 2020) and t-SNE (Van der Maaten\\nand Hinton, 2008), they offered negligible perfor-\\nmance improvements over PCA but incurred sub-\\nstantially higher computational costs, making them\\nimpractical for deployment at scale in enterprise\\nsystems.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='mance improvements over PCA but incurred sub-\\nstantially higher computational costs, making them\\nimpractical for deployment at scale in enterprise\\nsystems.\\n3.4 Hard Negative Selection Criteria\\nWe propose two semantic criteria to identify high-\\nquality hard negatives. PCA-reduced embeddings\\nXPCA are organized around each queryQ. For each\\nquery-positive document pair (Q, P D), candidate\\ndocuments D from the corpus are evaluated via\\ncosine distances:\\nd(Q, P D), d (Q, D), d (P D, D) (4)\\nA document D is selected as a hard negative\\nonly if it satisfies both criteria:\\nd(Q, D) < d(Q, P D) (5)\\nd(Q, D) < d(P D, D) (6)\\nEquation (5) ensures that the candidate negative\\ndocument is semantically closer to the query than\\nthe actual positive document, making it a challeng-\\ning negative example that potentially confuses the\\nreranking model. Equation (6), ensures that the se-\\nlected hard negative is not just query-confusing but\\nalso sufficiently dissimilar from the actual positive'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='reranking model. Equation (6), ensures that the se-\\nlected hard negative is not just query-confusing but\\nalso sufficiently dissimilar from the actual positive\\n(avoiding near-duplicates or false negatives).\\nThe candidate document DHN with minimal\\nd(Q, D) satisfying these conditions is chosen as\\nthe primary hard negative. Additional hard nega-\\ntives can similarly be selected based on semantic\\nproximity rankings.\\nFigure 2: Hard negative selection on the first two PCA\\ncomponents (78% variance). Q act as centroids, P D\\nguide selection of hard negatives; which are chosen\\nbased on semantic proximity.\\nFigure 2 illustrates an example embedding space,\\nclearly depicting the query Q, positive document\\nP D, and selected hard negative DHN , visualizing\\nthe semantic selection criteria. In cases where no\\ndocuments satisfy these conditions, no hard nega-\\ntives are selected for that particular query. Further\\ndetails on our embedding model & fine-tuning us-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='documents satisfy these conditions, no hard nega-\\ntives are selected for that particular query. Further\\ndetails on our embedding model & fine-tuning us-\\ning these hard negatives are provided in Appendix\\nA.2.\\n4 Experiments & Results\\nTo evaluate the effectiveness of our proposed hard-\\nnegative selection framework, we conduct exten-\\nsive experiments on our internal cloud-specific en-\\nterprise dataset, as well as domain-specific open-\\nsource benchmarks. We systematically compare\\nour approach against multiple competitive negative\\nsampling methods and perform detailed ablation\\nstudies to understand the contribution of individual\\nframework components. Complete details on exper-\\nimental setups and hyperparameters are provided\\nin Appendix A.3.\\n4.1 Results & Discussion\\nComparative Analysis of Negative Sampling\\nStrategies Table 3 presents a detailed compar-\\nison of of our negative sampling technique against\\nseveral established methods, including Random,\\nBM25, In-batch, STAR, and ADORE+STAR. The'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nRe-ranker (Fine-tuned w/) Internal FiQA Climate-FEVER TechQA\\nMRR@3 MRR@10MRR@3 MRR@10MRR@3 MRR@10MRR@3 MRR@10\\nBaseline (No Fine-tuning) 0.42 0.45 0.45 0.48 0.44 0.46 0.57 0.61\\nIn-batch Negatives 0.47 0.52 0.46 0.52 0.44 0.47 0.57 0.62\\nSTAR 0.53 0.56 0.51 0.54 0.47 0.49 0.61 0.63\\nADORE+STAR 0.54 0.57 0.52 0.54 0.48 0.52 0.63 0.66\\nOur Proposed HN 0.57 0.64 0.54 0.56 0.52 0.55 0.65 0.69\\nTable 2: Comparative performance benchmarking of our in-house reranker across multiple domain-specific datasets.\\nThe reranker is fine-tuned (FT) with different negative sampling techniques, highlighting the effectiveness of our\\nproposed hard-negative mining method (HN).\\nNegative Sampling Method MRR@3 MRR@10\\nBaseline 0.42 0.45\\nFT with Random Neg 0.47 0.51\\nFT with BM25 Neg 0.49 0.54\\nFT with In-batch Neg 0.47 0.52\\nFT with BM25+In-batch Neg 0.52 0.54\\nFT with STAR 0.53 0.56\\nFT with ADORE+STAR 0.54 0.57\\nFT with our HN 0.57 0.64\\nTable 3: Comparison of negative sampling methods for'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='FT with In-batch Neg 0.47 0.52\\nFT with BM25+In-batch Neg 0.52 0.54\\nFT with STAR 0.53 0.56\\nFT with ADORE+STAR 0.54 0.57\\nFT with our HN 0.57 0.64\\nTable 3: Comparison of negative sampling methods for\\nfine-tuning(FT) in-house cross-encoder reranker model.\\nThe proposed framework achieves 15% and 19% im-\\nprovements in MRR@3 and MRR@10, respectively,\\nover baseline methods.\\nbaseline is defined as the performance of our inter-\\nnal reranker model without any fine-tuning. Our\\nmethod achieves notable relative improvements of\\n15% in MRR@3 and 19% in MRR@10 over this\\nbaseline. The semantic nature of our hard nega-\\ntives allows the reranker to distinguish contextually\\nirrelevant but semantically similar documents effec-\\ntively. In contrast, simpler baselines like Random\\nor BM25 negatives suffer due to no semantic con-\\nsideration, while advanced methods like STAR and\\nADORE+STAR occasionally miss subtle seman-\\ntic nuances that our formulated selection criteria\\naddress effectively.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='sideration, while advanced methods like STAR and\\nADORE+STAR occasionally miss subtle seman-\\ntic nuances that our formulated selection criteria\\naddress effectively.\\nGeneralization Across Open-source Models To\\nvalidate the robustness and versatility of our frame-\\nwork, we evaluated various open-source embed-\\nding and reranker models (Table 4), clearly demon-\\nstrating improvements across all models when fine-\\ntuned using our proposed negative sampling com-\\npared to ADORE+STAR and baseline (no fine-\\ntuning). Notably, rerankers with multilingual ca-\\npabilities, such as the BGE-Reranker and Jina\\nReranker, demonstrated pronounced improvements,\\nlikely benefiting from our embedding ensemble’s\\nmultilingual semantic richness. Similarly, larger\\nmodels like e5-mistral exhibit significant gains, re-\\nflecting their capacity to exploit nuanced semantic\\ndifferences provided by our negative samples. This\\nanalysis underscores the general applicability and\\nmodel-agnostic benefits of our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='flecting their capacity to exploit nuanced semantic\\ndifferences provided by our negative samples. This\\nanalysis underscores the general applicability and\\nmodel-agnostic benefits of our approach.\\nModel Baseline ADORE+STAR Ours\\nAlibaba-NLP\\n(gte-multilingual-reranker-base) 0.39 0.42 0.45\\nBGE-Reranker\\n(bge-reranker-large) 0.44 0.47 0.52\\nCohere Embed English Light\\n(Cohere-embed-english-light-v3.0) 0.32 0.34 0.38\\nCohere Embed Multilingual\\n(Cohere-embed-multilingual-v3.0) 0.34 0.37 0.40\\nCohere Reranker\\n(rerank-multilingual-v2.0) 0.42 0.45 0.49\\nIBM Reranker\\n(re2g-reranker-nq) 0.40 0.43 0.46\\nInfloat Reranker\\n(e5-mistral-7b-instruct) 0.35 0.38 0.42\\nJina Reranker v2\\n(jina-reranker-v2-base-multilingual) 0.45 0.480.53\\nMS-MARCO\\n(ms-marco-MiniLM-L-6-v2) 0.41 0.43 0.46\\nNomic AI Embed Text\\n(nomic-embed-text-v1.5) 0.33 0.36 0.39\\nNVIDIA\\nNV-Embed-v2 0.38 0.41 0.44\\nSalesforce\\nSFR-Embedding-2_R 0.37 0.40 0.43\\nSalesforce\\nSFR-Embedding-Mistral 0.36 0.39 0.42\\nT5-Large 0.41 0.44 0.47'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='(nomic-embed-text-v1.5) 0.33 0.36 0.39\\nNVIDIA\\nNV-Embed-v2 0.38 0.41 0.44\\nSalesforce\\nSFR-Embedding-2_R 0.37 0.40 0.43\\nSalesforce\\nSFR-Embedding-Mistral 0.36 0.39 0.42\\nT5-Large 0.41 0.44 0.47\\nTable 4: Performance benchmarking (MRR@3) of\\nreranker and embedding models using the proposed\\nhard negative selection framework, compared with\\nADORE+STAR and baseline methods.\\nEffectiveness on Domain-specific Public\\nDatasets We further tested our method’s\\nadaptability across diverse public domain-specific\\ndatasets (FiQA, Climate-FEVER, TechQA), as\\nshown in Table 2. Each dataset presents distinct\\nretrieval challenges, ranging from technical jargon\\nin TechQA to complex domain-specific reasoning\\nin Climate-FEVER. Fine-tuning with our generated\\nhard negatives consistently improved retrieval\\nacross these varied datasets. FiQA exhibited\\nsignificant gains, likely due to the semantic\\ndifferentiation required in finance-specific queries.\\nThese results demonstrate that our negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nsampling method is not only effective within our\\ninternal enterprise corpus but also valuable across\\ndiverse, domain-specific public datasets, indicating\\nbroad applicability and domain independence.\\nModel MRR@3 MRR@10\\nShort DocumentsBaseline 0.481 0.526\\nFT w/ proposed HN0.61 0.662\\nLong DocumentsBaseline 0.423 0.477\\nFT w/ proposed HN0.475 0.521\\nTable 5: Performance comparison of the in-house\\nreranker without fine-tuning (Baseline) versus fine-\\ntuned (FT) with our proposed hard negatives (HN), eval-\\nuated separately on short and long documents.\\nPerformance Analysis on Short vs. Long Docu-\\nments An explicit analysis of short versus long\\ndocuments (Table 5) revealed differential perfor-\\nmance gains. Short documents (under 1024 to-\\nkens) experienced substantial performance im-\\nprovements (MRR@3 improving from 0.481 to\\n0.61), attributed to minimal semantic redundancy\\nand tokenization constraints. Conversely, long\\ndocuments showed more moderate improvements'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='provements (MRR@3 improving from 0.481 to\\n0.61), attributed to minimal semantic redundancy\\nand tokenization constraints. Conversely, long\\ndocuments showed more moderate improvements\\n(MRR@3 from 0.423 to 0.475), primarily due to\\nembedding truncation that causes loss of context\\nand increased semantic complexity. Future re-\\nsearch should focus explicitly on developing hi-\\nerarchical or segment-based embedding methods\\nto address these limitations.\\nAblation Studies To clearly understand the im-\\npact of the individual components of the frame-\\nwork, we conducted systematic ablation studies\\n(Table 6). Training with positive documents alone\\nproduced only slight gains (+0.03 MRR@3), reaf-\\nfirming the critical role of high-quality hard nega-\\ntives. Evaluating individual embedding models sep-\\narately indicated varying performance due to their\\ndiffering semantic representations and underlying\\ntraining. However, the concatenation of diverse\\nembeddings provided significant performance im-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='arately indicated varying performance due to their\\ndiffering semantic representations and underlying\\ntraining. However, the concatenation of diverse\\nembeddings provided significant performance im-\\nprovements (+0.15 MRR@3), clearly highlighting\\nthe advantages of capturing semantic diversity.\\nAdditionally, PCA-based dimensionality reduc-\\ntion analysis identified the optimal variance thresh-\\nold at 95%. Lower thresholds resulted in marked\\nsemantic degradation, reducing retrieval perfor-\\nmance. This trade-off highlights PCA as an essen-\\ntial efficiency-enhancing step for the framework.\\nCollectively, these detailed analyses underscore\\nour method’s strengths, limitations, and method-\\nological rationale, providing clear empirical justifi-\\ncation for each design decision.\\n# Proposed Strategies MRR@3MRR@10\\n1 Baseline 0.42 0.45\\nPositive Document (PD) Only\\n2 Fine-tuning with PD Only 0.45 0.51\\nHard Negative(HN) with EmbeddingEk\\n3a HN withE1+ PD 0.45 0.51\\n3b HN withE2+ PD 0.47 0.53'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='1 Baseline 0.42 0.45\\nPositive Document (PD) Only\\n2 Fine-tuning with PD Only 0.45 0.51\\nHard Negative(HN) with EmbeddingEk\\n3a HN withE1+ PD 0.45 0.51\\n3b HN withE2+ PD 0.47 0.53\\n3c HN withE3+ PD 0.51 0.55\\n3d HN withE4+ PD 0.45 0.52\\n3e HN withE5+ PD 0.48 0.51\\n3f HN withE6+ PD 0.49 0.52\\n3g HN with Xconcat+ PD 0.57 0.64\\nXPCAVariance Impact+ PD\\n4a HN with XPCA(99% Variance)0.57 0.64\\n4b HN with XPCA(95% Variance)0.57 0.64\\n4c HN with XPCA(90% Variance)0.55 0.63\\n4d HN with XPCA(80% Variance)0.51 0.58\\n4e HN with XPCA(70% Variance)0.49 0.56\\nTable 6: Results of ablation study showing the impact\\nof embeddings, PCA variance thresholds, and positive\\ndocuments on MRR, on the in-house re-ranker model.\\n4.2 Case Studies: Examples of Hard Negative\\nImpact\\nFigure 3 shows how similar topics in the domain\\nof cloud computing. To demonstrate the qualitative\\nbenefits of the proposed framework, we present\\ntwo case studies where the baseline and fine-tuned\\nmodels produce different ranking results. These'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='of cloud computing. To demonstrate the qualitative\\nbenefits of the proposed framework, we present\\ntwo case studies where the baseline and fine-tuned\\nmodels produce different ranking results. These\\nexamples highlight the significance of hard neg-\\natives in distinguishing semantically similar but\\ncontextually irrelevant documents.\\nFigure 3: Illustrations of similar topics in the domain of\\nCloud Computing\\nCase Study 1: Disambiguating Technical\\nAcronyms.\\n• Query (Q): \"What is VCN in Cloud Infras-\\ntructure?\"'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\n• Positive Document (PD): A document ex-\\nplaining \"Virtual Cloud Network (VCN)\" in\\nCloud Infrastructure, detailing its setup and\\nusage.\\n• Hard Negative (HN):A document discussing\\n\"Virtual Network Interface Card (VNIC)\" in\\nthe context of networking hardware.\\nBaseline Result: The baseline model incorrectly\\nranks the hard negative above the positive docu-\\nment due to overlapping terms such as \"virtual\"\\nand \"network.\"\\nProposed Method Result: The fine-tuned model\\nranks the positive document higher, correctly iden-\\ntifying the contextual match between the query\\nand the description of VCN. This improvement\\nis attributed to the triplet loss training with hard\\nnegatives.\\nCase Study 2: Domain-Specific Terminology.\\n• Query (Q): \"How does the CI WAF handle\\nincoming traffic?\"\\n• Positive Document (PD): A document ex-\\nplaining the Web Application Firewall (W AF)\\nin CI, its configuration, and traffic filtering\\nmechanisms.\\n• Hard Negative (HN):A document discussing'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='• Positive Document (PD): A document ex-\\nplaining the Web Application Firewall (W AF)\\nin CI, its configuration, and traffic filtering\\nmechanisms.\\n• Hard Negative (HN):A document discussing\\ngeneral firewall configurations in networking.\\nBaseline Result: The baseline model ranks the\\nhard negative higher due to lexical overlap between\\nthe terms \"firewall\" and \"traffic.\"\\nProposed Method Result: The proposed frame-\\nwork ranks the positive document higher, leverag-\\ning domain-specific semantic representations.\\nThese case studies illustrate the practical ad-\\nvantages of training with hard negatives, espe-\\ncially in domains with overlapping terminology\\nor acronyms.\\nAdditional detailed analyses, illustrative prac-\\ntical implications for enterprise applications, and\\nexplicit future directions are discussed in detail in\\nA.4, and A.5.\\n5 Conclusion\\nWe introduced a scalable, modular framework lever-\\naging dynamic ensemble-based hard-negative min-\\ning to significantly enhance re-ranking models in'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='A.4, and A.5.\\n5 Conclusion\\nWe introduced a scalable, modular framework lever-\\naging dynamic ensemble-based hard-negative min-\\ning to significantly enhance re-ranking models in\\nenterprise and domain-specific retrieval scenarios.\\nOur method dynamically curates semantically chal-\\nlenging yet contextually irrelevant negatives, allow-\\ning re-ranking models to effectively discriminate\\nsubtle semantic differences. Empirical evaluations\\non proprietary enterprise data and diverse public\\ndomain-specific benchmarks demonstrated substan-\\ntial improvements of up to 15% in MRR@3 and\\n19% in MRR@10 over state-of-the-art negative\\nsampling techniques, including BM25, In-Batch\\nNegatives, STAR, and ADORE+STAR.\\nOur approach offers clear practical benefits in\\nreal-world deployments, benefiting downstream ap-\\nplications such as knowledge management, cus-\\ntomer support systems, and Retrieval-Augmented\\nGeneration (RAG), where retrieval precision di-\\nrectly influences user satisfaction and Generative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='plications such as knowledge management, cus-\\ntomer support systems, and Retrieval-Augmented\\nGeneration (RAG), where retrieval precision di-\\nrectly influences user satisfaction and Generative\\nAI effectiveness. The strong performance and gen-\\neralizability across various domains further under-\\nscore the framework’s readiness for industry-scale\\ndeployment.\\nFuture work will focus on extending our frame-\\nwork to handle incremental updates of enterprise\\nknowledge bases and exploring real-time negative\\nsampling strategies for continuously evolving cor-\\npora, further enhancing the adaptability and robust-\\nness required in practical industry settings.\\n6 Limitations\\nWhile our approach advances the state of hard\\nnegative mining and encoder-based retrieval, sev-\\neral limitations remain that open avenues for fu-\\nture research. One key challenge is the perfor-\\nmance disparity between short and long documents.\\nAddressing this requires more effective document'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='eral limitations remain that open avenues for fu-\\nture research. One key challenge is the perfor-\\nmance disparity between short and long documents.\\nAddressing this requires more effective document\\nchunking strategies and the development of hier-\\narchical representations to preserve context across\\nsegments. Additionally, the retrieval of long doc-\\numents is complicated by semantic redundancy\\nand truncation, warranting deeper analysis of their\\nstructural complexity. Our current use of embed-\\nding concatenation for ensembling could also be\\nrefined—future work should evaluate alternative\\nfusion techniques such as weighted averaging or\\nattention-based mechanisms. Moreover, extending\\nthe retrieval framework to support cross-lingual and\\nmultilingual scenarios would enhance its utility in\\nglobally distributed applications.\\nReferences\\nAMIT AGARWAL. 2021. Evaluate generalisation &\\nrobustness of visual features from images to video.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nResearchGate. Available at https://doi.org/10.\\n13140/RG.2.2.33887.53928.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2024a. Synthetic document generation pipeline for\\ntraining artificial intelligence models. US Patent App.\\n17/994,712.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2025. FS-DAG: Few shot domain adapting graph net-\\nworks for visually rich document understanding. In\\nProceedings of the 31st International Conference on\\nComputational Linguistics: Industry Track, pages\\n100–114, Abu Dhabi, UAE. Association for Com-\\nputational Linguistics.\\nAmit Agarwal, Hitesh Patel, Priyaranjan Pattnayak,\\nSrikant Panda, Bhargava Kumar, and Tejaswini Ku-\\nmar. 2024b. Enhancing document ai data genera-\\ntion through graph-based synthetic layouts. arXiv\\npreprint arXiv:2412.03590.\\nJina AI. 2023. jina-reranker-v2-base-multilingual.\\nArian Askari, Mohammad Aliannejadi, Evangelos\\nKanoulas, and Suzan Verberne. 2023. Generating'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2412.03590.\\nJina AI. 2023. jina-reranker-v2-base-multilingual.\\nArian Askari, Mohammad Aliannejadi, Evangelos\\nKanoulas, and Suzan Verberne. 2023. Generating\\nsynthetic documents for cross-encoder re-rankers: A\\ncomparative study of chatgpt and human experts.\\nJiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang,\\nXinnian Liang, Zhao Yan, and Zhoujun Li. 2023.\\nGriprank: Bridging the gap between retrieval and\\ngeneration via the generative knowledge improved\\npassage ranking. Preprint, arXiv:2305.18144.\\nVittorio Castelli, Rishav Chakravarti, Saswati Dana, An-\\nthony Ferritto, Radu Florian, Martin Franz, Dinesh\\nGarg, Dinesh Khandelwal, Scott McCarley, Mike\\nMcCawley, Mohamed Nasr, Lin Pan, Cezar Pen-\\ndus, John Pitrelli, Saurabh Pujar, Salim Roukos, An-\\ndrzej Sakrajda, Avirup Sil, Rosario Uceda-Sosa, Todd\\nWard, and Rong Zhang. 2019. The techqa dataset.\\nPreprint, arXiv:1911.02984.\\nCohere. 2023a. Cohere-embed-multilingual-v3.0.\\nAvailable at: https://cohere.com/blog/\\nintroducing-embed-v3.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Ward, and Rong Zhang. 2019. The techqa dataset.\\nPreprint, arXiv:1911.02984.\\nCohere. 2023a. Cohere-embed-multilingual-v3.0.\\nAvailable at: https://cohere.com/blog/\\nintroducing-embed-v3.\\nCohere. 2023b. Reranker model. Available\\nat: https://docs.cohere.com/v2/docs/\\nreranking-with-cohere.\\nGabriel de Souza P. Moreira, Radek Osmulski, Mengyao\\nXu, Ronay Ak, Benedikt Schifferer, and Even\\nOldridge. 2024. Nv-retriever: Improving text em-\\nbedding models with effective hard-negative mining.\\nPreprint, arXiv:2407.15831.\\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leip-\\npold. 2021. Climate-fever: A dataset for veri-\\nfication of real-world climate claims. Preprint,\\narXiv:2012.00614.\\nKaran Dua, Praneet Pabolu, and Mengqing Guo. 2024.\\nGenerating templates for use in synthetic document\\ngeneration processes. US Patent App. 18/295,765.\\nKaran Dua, Praneet Pabolu, and Ranjeet Kumar Gupta.\\n2025. Generation of synthetic doctor-patient conver-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='generation processes. US Patent App. 18/295,765.\\nKaran Dua, Praneet Pabolu, and Ranjeet Kumar Gupta.\\n2025. Generation of synthetic doctor-patient conver-\\nsations. US Patent App. 18/495,966.\\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\\nArivazhagan, and Wei Wang. 2022. Language-\\nagnostic bert sentence embedding. Preprint,\\narXiv:2007.01852.\\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\\ntraining architecture for dense retrieval. EMNLP\\n2021 - 2021 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings, pages\\n981–993.\\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\\nChowdhury, Ankita Naik, Pengshan Cai, and Al-\\nfio Gliozzo. 2022. Re2G: Retrieve, rerank, gen-\\nerate. In Proceedings of the 2022 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, pages 2701–2715, Seattle, United\\nStates. Association for Computational Linguistics.\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='for Computational Linguistics: Human Language\\nTechnologies, pages 2701–2715, Seattle, United\\nStates. Association for Computational Linguistics.\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\\nWu. 2023. How close is chatgpt to human experts?\\ncomparison corpus, evaluation, and detection.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training.\\nEK Jasila, N Saleena, and KA Abdul Nazeer. 2023. An\\nefficient document clustering approach for devising\\nsemantic clusters. Cybernetics and Systems, pages\\n1–18.\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen Tau Yih. 2020. Dense passage retrieval\\nfor open-domain question answering. EMNLP\\n2020 - 2020 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings of the\\nConference, pages 6769–6781.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='2020 - 2020 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings of the\\nConference, pages 6769–6781.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan\\nRaiman, Mohammad Shoeybi, Bryan Catanzaro, and\\nWei Ping. 2024. Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models. arXiv\\npreprint arXiv:2405.17428.\\nFulu Li, Zhiwen Xie, and Guangyou Zhou. 2024.\\nTheme-enhanced hard negative sample mining for\\nopen-domain question answering. In ICASSP 2024 -\\n2024 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages\\n12436–12440.\\nXianming Li and Jing Li. 2023. Angle-optimized text\\nembeddings. arXiv preprint arXiv:2309.12871.\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih\\nYavuz, Caiming Xiong, and Philip S. Yu. 2021.\\nDense hierarchical retrieval for open-domain ques-\\ntion answering. In Conference on Empirical\\nMethods in Natural Language Processing.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nSean MacAvaney, Andrew Yates, Arman Cohan,\\nand Nazli Goharian. 2019. Cedr: Contextual-\\nized embeddings for document ranking. SIGIR\\n2019 - Proceedings of the 42nd International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1101–1104.\\nAndrzej Ma´ckiewicz and Waldemar Ratajczak. 1993.\\nPrincipal components analysis (pca). Computers &\\nGeosciences, 19(3):303–342.\\nLeland McInnes, John Healy, and James Melville.\\n2020. Umap: Uniform manifold approximation\\nand projection for dimension reduction. Preprint,\\narXiv:1802.03426.\\nHansa Meghwani. 2024. Enhancing retrieval perfor-\\nmance: An ensemble approach for hard negative min-\\ning. Preprint, arXiv:2411.02404.\\nVivek Mehta, Mohit Agarwal, and Rohit Kumar Kaliyar.\\n2024. A comprehensive and analytical review of text\\nclustering techniques. International Journal of Data\\nScience and Analytics, pages 1–20.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024a. Sfr-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='clustering techniques. International Journal of Data\\nScience and Analytics, pages 1–20.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024a. Sfr-\\nembedding-2: Advanced text embedding with multi-\\nstage training.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024b. Sfr-\\nembedding-mistral: Enhance text retrieval with trans-\\nfer learning. Salesforce AI Research Blog.\\nThanh-Do Nguyen, Chi Minh Bui, Thi-Hai-Yen Vuong,\\nand Xuan-Hieu Phan. 2022. Passage-based bm25\\nhard negatives: A simple and effective negative sam-\\npling strategy for dense retrieval.\\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\\nre-ranking with bert.\\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\\nJimmy Lin. 2019. Multi-stage document ranking\\nwith bert.\\nZach Nussbaum, John X. Morris, Brandon Duderstadt,\\nand Andriy Mulyar. 2024. Nomic embed: Training\\na reproducible long context text embedder. Preprint,\\narXiv:2402.01613.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='with bert.\\nZach Nussbaum, John X. Morris, Brandon Duderstadt,\\nand Andriy Mulyar. 2024. Nomic embed: Training\\na reproducible long context text embedder. Preprint,\\narXiv:2402.01613.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024a. Multi-lingual natural language generation.\\nUS Patent App. 18/318,315.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024b. Multi-lingual natural language generation.\\nUS Patent App. 18/318,327.\\nSrikant Panda, Amit Agarwal, Gouttham Nambirajan,\\nand Kulbhushan Pachauri. 2025a. Out of distribution\\nelement detection for information extraction. US\\nPatent App. 18/347,983.\\nSrikant Panda, Amit Agarwal, and Kulbhushan Pachauri.\\n2025b. Techniques of information extraction for se-\\nlection marks. US Patent App. 18/240,344.\\nHitesh Laxmichand Patel, Amit Agarwal, Arion Das,\\nBhargava Kumar, Srikant Panda, Priyaranjan Pat-\\ntnayak, Taki Hasan Rafi, Tejaswini Kumar, and Dong-\\nKyu Chae. 2025. Sweeval: Do llms really swear?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Hitesh Laxmichand Patel, Amit Agarwal, Arion Das,\\nBhargava Kumar, Srikant Panda, Priyaranjan Pat-\\ntnayak, Taki Hasan Rafi, Tejaswini Kumar, and Dong-\\nKyu Chae. 2025. Sweeval: Do llms really swear?\\na safety benchmark for testing limits for enterprise\\nuse. In Proceedings of the 2025 Conference of the\\nNations of the Americas Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies (V olume3: Industry Track), pages\\n558–582.\\nHitesh Laxmichand Patel, Amit Agarwal, Bhargava\\nKumar, Karan Gupta, and Priyaranjan Pattnayak.\\n2024. Llm for barcodes: Generating diverse syn-\\nthetic data for identity documents. arXiv preprint\\narXiv:2411.14962.\\nPriyaranjan Pattnayak, Amit Agarwal, Hansa Megh-\\nwani, Hitesh Laxmichand Patel, and Srikant Panda.\\n2025a. Hybrid ai for responsive multi-turn on-\\nline conversations with novel dynamic routing and\\nfeedback adaptation. In Proceedings of the 4th\\nInternational Workshop on Knowledge-Augmented'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='2025a. Hybrid ai for responsive multi-turn on-\\nline conversations with novel dynamic routing and\\nfeedback adaptation. In Proceedings of the 4th\\nInternational Workshop on Knowledge-Augmented\\nMethods for Natural Language Processing, pages\\n215–229.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, and\\nAmit Agarwal. 2025b. Tokenization matters: Im-\\nproving zero-shot ner for indic languages. Preprint,\\narXiv:2504.16977.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit\\nAgarwal, Bhargava Kumar, Srikant Panda, and Te-\\njaswini Kumar. 2025c. Clinical qa 2.0: Multi-task\\nlearning for answer extraction and categorization.\\nPreprint, arXiv:2502.13108.\\nRonak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, An-\\ndrew Yates, and Jimmy Lin. 2022. Squeezing wa-\\nter from a stone: A bag of tricks for further im-\\nproving cross-encoder effectiveness for reranking.\\nIn Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in Artificial Intelligence and\\nLecture Notes in Bioinformatics), volume 13185'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='In Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in Artificial Intelligence and\\nLecture Notes in Bioinformatics), volume 13185\\nLNCS, pages 655–670. Springer Science and Busi-\\nness Media Deutschland GmbH.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\\nHaifeng Wang. 2020. Rocketqa: An optimized train-\\ning approach to dense passage retrieval for open-\\ndomain question answering.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\\nSentence embeddings using siamese bert-networks.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing.\\nS. E. Robertson and S. Walker. 1994. Some Simple\\nEffective Approximations to the 2-Poisson Model\\nfor Probabilistic Weighted Retrieval, pages 232–241.\\nSpringer London.\\nSaba Sturua, Isabelle Mohr, Mohammad Kalim Akram,\\nMichael Günther, Bo Wang, Markus Krimmel, Feng\\nWang, Georgios Mastrapas, Andreas Koukounas, An-\\ndreas Koukounas, Nan Wang, and Han Xiao. 2024.\\njina-embeddings-v3: Multilingual embeddings with\\ntask lora. Preprint, arXiv:2409.10173.\\nTheFinAI. 2018. Fiqa: A financial question answering\\ndataset. Available at Hugging Face.\\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\\nVisualizing data using t-sne. Journal of machine\\nlearning research, 9(11).\\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\\nRangan Majumder, and Furu Wei. 2023. Improving\\ntext embeddings with large language models. arXiv\\npreprint arXiv:2401.00368.\\nSvante Wold, Kim H. Esbensen, Kim H. Esbensen, Paul'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. 2023. Improving\\ntext embeddings with large language models. arXiv\\npreprint arXiv:2401.00368.\\nSvante Wold, Kim H. Esbensen, Kim H. Esbensen, Paul\\nGeladi, and Paul Geladi. 1987. Principal component\\nanalysis. Chemometrics and Intelligent Laboratory\\nSystems, 2:37–52.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding. Preprint,\\narXiv:2309.07597.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2020. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval.\\nZhen Yang, Zhou Shao, Yuxiao Dong, and Jie Tang.\\n2024. Trisampler: A better negative sampling princi-\\nple for dense retrieval. Preprint, arXiv:2402.11855.\\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\\nZhang, and Shaoping Ma. 2021. Optimizing dense\\nretrieval model training with hard negatives. SIGIR'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\\nZhang, and Shaoping Ma. 2021. Optimizing dense\\nretrieval model training with hard negatives. SIGIR\\n2021 - Proceedings of the 44th International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1503–1512.\\nDun Zhang. 2024. stella-embedding-model-2024.\\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie,\\nZiqi Dai, Jialong Tang, Huan Lin, Baosong Yang,\\nPengjun Xie, Fei Huang, et al. 2024. mgte: General-\\nized long-context text representation and reranking\\nmodels for multilingual text retrieval. arXiv preprint\\narXiv:2407.19669.\\nA Appendix\\nA.1 Extended Related Work\\nHard Negatives in Retrieval Models Static and\\ndynamic hard negatives have been extensively stud-\\nied. Static negatives, such as those generated\\nby BM25 (Robertson and Walker, 1994) or Pas-\\nsageBM25 (Nguyen et al., 2022), provide challeng-\\ning lexical contrasts but risk overfitting due to their\\nfixed nature (Qu et al., 2020). Dynamic negatives,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='sageBM25 (Nguyen et al., 2022), provide challeng-\\ning lexical contrasts but risk overfitting due to their\\nfixed nature (Qu et al., 2020). Dynamic negatives,\\nas introduced in ANCE (Xiong et al., 2020) and\\nADORE (Zhan et al., 2021) adapt during training,\\nother effective methods like positive-aware mining\\n(de Souza P. Moreira et al., 2024), theme-enhanced\\nnegatives (Li et al., 2024) offers relevant chal-\\nlenges but incurring high computational costs due\\nto periodic re-indexing and bigger embedding di-\\nmension. Our framework mitigates these issues by\\nleveraging clustering and dimensionality reduction\\nto dynamically identify negatives without requiring\\nre-indexing.\\nLocalized Contrastive Estimation (LCE) (Guo\\net al., 2023; AGARWAL, 2021) further demon-\\nstrated the effectiveness of incorporating hard nega-\\ntives into cross-encoder training, improving rerank-\\ning accuracy when negatives align with retriever\\noutputs. Additionally, (Pradeep et al., 2022) high-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='tives into cross-encoder training, improving rerank-\\ning accuracy when negatives align with retriever\\noutputs. Additionally, (Pradeep et al., 2022) high-\\nlighted the importance of hard negatives even in\\nadvanced pretraining setups like Condenser (Gao\\nand Callan, 2021), which emphasizes their neces-\\nsity for robust optimization.\\nAdvances in Dense Retrieval and Cross-\\nEncoders Dense retrieval models like\\nDPR (Karpukhin et al., 2020) and REALM (Guu\\net al., 2020) encode queries and documents into\\ndense embeddings, enabling semantic matching.\\nRecent advances in dense retrieval and ranking\\ninclude GripRank’s generative knowledge-driven\\npassage ranking (Bai et al., 2023), Dense\\nHierarchical Retrieval’s multi-stage framework\\nfor efficient question answering (Liu et al., 2021;\\nPattnayak et al., 2025a,c,b; Patel et al., 2025), and\\nTriSampler’s optimized negative sampling for\\ndense retrieval (Yang et al., 2024), collectively\\nenhancing retrieval performance.Cross-encoders,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Pattnayak et al., 2025a,c,b; Patel et al., 2025), and\\nTriSampler’s optimized negative sampling for\\ndense retrieval (Yang et al., 2024), collectively\\nenhancing retrieval performance.Cross-encoders,\\nsuch as monoBERT (Nogueira et al., 2019;\\nNogueira and Cho, 2019), further improve retrieval\\nprecision by jointly encoding query-document\\npairs but require high-quality training data,\\nparticularly challenging negatives (MacAvaney\\net al., 2019; Panda et al., 2025b). Techniques such'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nas synthetic data generation (Askari et al., 2023;\\nAgarwal et al., 2024a, 2025) augment training\\ndatasets but lack the realism and semantic depth\\nprovided by our hard negative mining approach.\\nDimensionality Reduction in IR Clustering\\nmethods have been used to group semantically\\nsimilar documents, improving retrieval efficiency\\nand training data organization (Mehta et al., 2024;\\nJasila et al., 2023; Dua et al., 2025; Panda et al.,\\n2025a). Dimensionality reduction techniques like\\nPCA (Wold et al., 1987) enhance scalability by re-\\nducing computational complexity. Our framework\\nuniquely combines these techniques to dynamically\\nidentify negatives that challenge retrieval models\\nin a scalable manner.\\nSynthetic Data in Retrieval Recent\\nwork (Askari et al., 2023; Agarwal et al.,\\n2024a,b; Patel et al., 2024; Dua et al., 2024; Pabolu\\net al., 2024a,b) has explored using large language\\nmodels to generate synthetic training data for'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='work (Askari et al., 2023; Agarwal et al.,\\n2024a,b; Patel et al., 2024; Dua et al., 2024; Pabolu\\net al., 2024a,b) has explored using large language\\nmodels to generate synthetic training data for\\nretrieval tasks. While effective in low-resource\\nsettings, synthetic data often struggles with factual\\ninaccuracies and domain-specific relevance. In\\ncontrast, our framework relies on real-world data\\nto curate semantically challenging negatives,\\nensuring high-quality training samples without\\nintroducing synthetic biases.\\nSummary of Contributions While previous\\nworks address various aspects of negative sampling,\\nhard negatives, and synthetic data, our approach\\nbridges the gap between static and dynamic strate-\\ngies. By dynamically curating negatives using clus-\\ntering and dimensionality reduction, we achieve\\na scalable and semantically precise methodology\\ntailored to domain-specific retrieval tasks.\\nA.2 Extended Methodology\\nA.2.1 Dataset Statistics\\nQueries Length Distribution In this section we'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='a scalable and semantically precise methodology\\ntailored to domain-specific retrieval tasks.\\nA.2 Extended Methodology\\nA.2.1 Dataset Statistics\\nQueries Length Distribution In this section we\\nanalyze the distribution of queries length in our\\nenterprise dataset. Figure 4 shows that the length\\nof queries ranges from 1 to 25 words, with some\\nqueries having very few words. This highlights that\\nuser queries can sometime be just 2-3 words about\\na topic, increasing the probability of retrieving doc-\\numents mentioning those topics or concepts which\\ncan be contextually different. Therefore, when\\nwe select hard negatives, it is crucial to consider\\nnot only the relationship between the query and\\ndocuments but also the relationship between the\\nFigure 4: Length Distribution of queries in the dataset.\\npositive document and other documents, ensuring a\\ncomparison with texts on similar topics and similar\\nlengths.\\nModel (Ek) Params (M) Dimension Max Tokens\\nstella_en_400M_v5 435 8192 8192'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='positive document and other documents, ensuring a\\ncomparison with texts on similar topics and similar\\nlengths.\\nModel (Ek) Params (M) Dimension Max Tokens\\nstella_en_400M_v5 435 8192 8192\\njina-embeddings-v3 572 1024 8194\\n(multilingual)\\nmxbai-embed-large-v1 335 1024 512\\nbge-large-en-v1.5 335 1024 512\\nLaBSE 471 768 256\\n(multilingual)\\nall-mpnet-base-v2 110 768 514\\n(multilingual)\\nTable 7: Embedding models used to construct Xconcat,\\ncombining diverse semantic representations for queries\\n(Q), positive documents (P D), and corpus documents\\n(D).\\nFigure 5: Shows document length distribution in Enter-\\nprise corpus.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nDocument Length Distribution As shown in\\nFigure 5 , document lengths are significantly longer\\nthan query lengths. This disparity in context length\\naffects the similarity scores, potentially reducing\\nthe accuracy of retrieval systems. In our in-house\\ndataset, each query is paired with a single correct\\ndocument (though its not limited by number of\\npositive-negative document per query). This posi-\\ntive document is crucial for identifying challenging\\nhard negatives and hence helpful for encoder-based\\nmodel training.\\nA.2.2 Embedding Models\\nTable 7 lists the embedding models (Zhang, 2024;\\nSturua et al., 2024; Li and Li, 2023; Xiao et al.,\\n2023; Feng et al., 2022; Reimers and Gurevych,\\n2019; Zhang et al., 2024) used to construct Xconcat,\\ncombining diverse semantic representations for\\nqueries ( Q), positive documents ( P D), and cor-\\npus documents (D). These models were selected\\nfor their performance, model size, ability to han-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='combining diverse semantic representations for\\nqueries ( Q), positive documents ( P D), and cor-\\npus documents (D). These models were selected\\nfor their performance, model size, ability to han-\\ndle multilingual context, providing complemen-\\ntary strengths in dimensionality and token cover-\\nage. By integrating embeddings from these models,\\nthe framework captures nuanced semantic relation-\\nships crucial for reranker training.\\nA.2.3 Unified Contrastive Loss\\nThe unified contrastive loss is designed to improve\\nranking precision for both bi-encoders and cross-\\nencoders, by ensuring that positive documents\\n(P D) are ranked closer to the query (Q) than hard\\nnegatives (DHN ) by a margin m. The loss is de-\\nfined as:\\nL =\\nNX\\ni=1\\nmax (0, m + d(Qi, P Di) − d(Qi, DHN i))\\n(7)\\nwhere:\\n• P Di: Positive document associated with\\nquery Qi.\\n• DHN i: Hard negative document, semantically\\nsimilar to P Di but contextually irrelevant.\\n• d(Qi, Di): Distance metric measuring rele-\\nvance between Qi and Di.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='query Qi.\\n• DHN i: Hard negative document, semantically\\nsimilar to P Di but contextually irrelevant.\\n• d(Qi, Di): Distance metric measuring rele-\\nvance between Qi and Di.\\n• m: Margin ensuring P Di is closer to Qi than\\nDHN i by at least m, encouraging the model\\nto distinguish between relevant and irrelevant\\ndocuments effectively.\\nFor bi-encoders, the distance metric is defined as:\\nd(Qi, Di) = 1 − cosine(eQi, eDi), (8)\\nwhere eQi and eDi are the embeddings of the query\\nand document, respectively. For cross-encoders,\\nthe distance metric is:\\nd(Qi, Di) = −s(Qi, Di), (9)\\nwhere s(Qi, Di) is the cross-encoder’s relevance\\nscore for the query-document pair.\\nThis formulation leverages the triplet of (Q, P D,\\nDHN ) to minimize d(Qi, P Di), pulling positive\\ndocuments closer to the query, while maximizing\\nd(Qi, DHN i), pushing hard negatives further away.\\nBy emphasizing semantically challenging exam-\\nples, the model learns sharper decision boundaries\\nfor improved ranking precision.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='d(Qi, DHN i), pushing hard negatives further away.\\nBy emphasizing semantically challenging exam-\\nples, the model learns sharper decision boundaries\\nfor improved ranking precision.\\nA.3 Experimental Setup\\nDatasets We evaluate our framework extensively\\nusing both proprietary and public datasets:\\n• Internal Proprietary Dataset: Consisting\\nof approximately 5250 query-document pairs,\\non cloud services like computing, networking,\\nfirewall, ai services. It includes both short (<\\n[1024 tokens]) and long documents (>=[1024\\ntokens]).\\n• FiQA Dataset: A financial domain-specific\\ndataset widely used for retrieval benchmark-\\ning.\\n• Climate-FEVER Dataset: An environment-\\nspecific fact-checking dataset focused on\\nclimate-related information retrieval.\\n• TechQA Dataset: A technical question-\\nanswering dataset emphasizing software engi-\\nneering and technology-related queries.\\nTraining and Fine-tuning All re-ranking mod-\\nels are fine-tuned using a triplet loss with margin'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='answering dataset emphasizing software engi-\\nneering and technology-related queries.\\nTraining and Fine-tuning All re-ranking mod-\\nels are fine-tuned using a triplet loss with margin\\nwith same hyper-parameters. Early stopping is em-\\nployed based on validation MRR@10 scores to\\nprevent overfitting.\\nEvaluation Metrics Model performance is eval-\\nuated using standard retrieval metrics: Mean Recip-\\nrocal Rank (MRR) at positions 3 and 10 (MRR@3\\nand MRR@10), which measure retrieval quality\\nand ranking precision. Each reported metric is\\naveraged across three experimental runs for robust-\\nness.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nStrategy Training Data MRR@3 MRR@10\\nBaseline 0 0.42 0.45\\nFinetuned withHard Negatives\\n(Ours)\\n100 0.46 0.49\\n200 0.48 0.51\\n300 0.50 0.53\\n400 0.52 0.56\\n500 0.52 0.58\\n600 0.54 0.60\\n700 0.54 0.62\\n800 0.56 0.63\\n900 0.57 0.64\\n1000 0.57 0.64\\nTable 8: Comparison of Strategies with Varying Train-\\ning Data Sizes\\nA.4 Extended Results & Ablation\\nImpact of Training Data Size As shown in Ta-\\nble 8, both MRR@3 and MRR@10 improve as the\\ntraining data size increases, with more pronounced\\ngains in MRR@10. MRR@3 shows gradual im-\\nprovement, from 0.42 at the baseline to 0.57 with\\n100 examples, highlighting the model’s enhanced\\nability to rank relevant documents within the top 3.\\nMRR@10, on the other hand, shows more signif-\\nicant improvement, from 0.45 to 0.64, indicating\\nthat the model benefits more from additional data\\nwhen considering the top 10 ranked documents.\\nOur method shows promising results even with\\nsmaller training sets, demonstrating the effective-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='that the model benefits more from additional data\\nwhen considering the top 10 ranked documents.\\nOur method shows promising results even with\\nsmaller training sets, demonstrating the effective-\\nness of incorporating hard negatives early in the\\ntraining process. This suggests that hard negatives\\nsignificantly enhance the model’s ability to distin-\\nguish relevant from irrelevant documents against\\na given query, even when data is limited. This ap-\\nproach is particularly beneficial in enterprise con-\\ntexts, where annotated data may be scarce, enabling\\nquicker improvements in domain-specific retrieval\\nperformance.\\nModels in the Study In our study we com-\\npared the performance of other finetuned re-ranker\\n(Glass et al., 2022; Wang et al., 2023; Raffel et al.,\\n2020) and embedding models (Zhang et al., 2024;\\nNussbaum et al., 2024) using hard negatives gen-\\nerated by our proposed framework in Table 4.\\nWe benchmarked the BGE-Reranker (Xiao et al.,\\n2023), NV-Embed (Lee et al., 2024) Salesforce-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Nussbaum et al., 2024) using hard negatives gen-\\nerated by our proposed framework in Table 4.\\nWe benchmarked the BGE-Reranker (Xiao et al.,\\n2023), NV-Embed (Lee et al., 2024) Salesforce-\\nSFR (Meng et al., 2024a,b) , jina-reranker (AI,\\n2023) and Cohere-Reranker (Cohere, 2023a,b),\\nA.4.1 Analysis of Long vs. Short Documents\\nTable 5 reveals a consistent disparity in MRR\\nscores between short and long documents, with\\nlong documents showing lower performance. Here,\\nwe analyze potential reasons and propose mitiga-\\ntion strategies.\\nChallenges with Long Documents.\\n• Semantic Redundancy: Long documents of-\\nten contain repetitive or tangential content,\\ndiluting their relevance to a specific query.\\n• Context Truncation: Fixed-length tokeniza-\\ntion (e.g., 512 or 1024 tokens) truncates long\\ndocuments, potentially discarding critical in-\\nformation.\\n• Query-to-Document Mismatch: Short\\nqueries may not provide sufficient context to\\nmatch the nuanced information spread across\\na lengthy document.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='formation.\\n• Query-to-Document Mismatch: Short\\nqueries may not provide sufficient context to\\nmatch the nuanced information spread across\\na lengthy document.\\nPotential Solutions.\\n• Chunk-Based Retrieval: Split long doc-\\numents into smaller, semantically coherent\\nchunks and rank them individually.\\n• Hierarchical Embeddings: Use hierarchical\\nmodels to aggregate sentence- or paragraph-\\nlevel embeddings for better context represen-\\ntation.\\n• Query Expansion: Enhance short queries\\nwith additional context using techniques like\\nquery rewriting or pseudo-relevance feedback.\\nThis analysis highlights the need for future work\\nto address the inherent challenges of ranking long\\ndocuments effectively.\\nA.5 Practical Implications for Enterprise\\nApplications\\nThe proposed framework has significant practical\\nimplications for enterprise information retrieval\\nsystems, particularly in retrieval-augmented gener-\\nation (RAG) pipelines.\\nImproved Ranking Precision. By training with'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='implications for enterprise information retrieval\\nsystems, particularly in retrieval-augmented gener-\\nation (RAG) pipelines.\\nImproved Ranking Precision. By training with\\nhard negatives, the model ensures that the most\\nrelevant documents are retrieved for each query.\\nThis is particularly critical for enterprise use cases\\nsuch as:\\n• Technical Support: Retrieving precise docu-\\nmentation for customer queries, reducing res-\\nolution times.\\n• Knowledge Management: Ensuring that em-\\nployees access the most relevant internal re-\\nsources quickly.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nEnhanced Generative Quality. High-quality re-\\ntrieval directly improves the factual accuracy and\\ncoherence of outputs generated by large language\\nmodels in RAG pipelines. For example:\\n• Documentation Summarization: Sum-\\nmaries generated by models like GPT are\\nmore reliable when based on top-ranked, ac-\\ncurate sources.\\n• Customer Interaction: Chatbots generate\\nmore contextually relevant responses when\\nfed precise retrieved documents.\\nScalability and Adaptability. The framework’s\\nmodular design, including the use of diverse embed-\\ndings and clustering-based hard negative selection,\\nallows it to adapt to:\\n• Different industries (e.g., healthcare, finance,\\nmanufacturing).\\n• Multi-lingual or cross-lingual retrieval tasks.\\nThese practical implications underscore the ver-\\nsatility and enterprise readiness of the proposed\\nframework.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = ( z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = ( z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax( QK T\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax( QK T\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head 1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matricesW Q\\ni ∈ Rdmodel×dk, W K\\ni ∈ Rdmodel×dk, W V\\ni ∈ Rdmodel×dv\\nand W O ∈ Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0 , xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0 , xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndf f = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel)\\nP E(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10 −9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0 .1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0 .6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations , 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (V olume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f60b1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c96d33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1145.95it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1e4c4ae3d70>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc25d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1e4c4c0bf50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6df4d4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='arXiv:2505.18366v1  [cs.IR]  23 May 2025\\nAccepted in ACL 2025\\nHard Negative Mining for Domain-Specific Retrieval in Enterprise Systems\\nHansa Meghwani*, Amit Agarwal*, Priyaranjan Pattnayak,\\nHitesh Laxmichand Patel, Srikant Panda\\nOracle AI\\nCorrespondence: hansa.meghwani@oracle.com; amit.h.agarwal@oracle.com *\\nAbstract\\nEnterprise search systems often struggle to re-\\ntrieve accurate, domain-specific information\\ndue to semantic mismatches and overlapping\\nterminologies. These issues can degrade the\\nperformance of downstream applications such\\nas knowledge management, customer support,\\nand retrieval-augmented generation agents. To\\naddress this challenge, we propose a scal-\\nable hard-negative mining framework tailored\\nspecifically for domain-specific enterprise data.\\nOur approach dynamically selects semantically\\nchallenging but contextually irrelevant docu-\\nments to enhance deployed re-ranking models.\\nOur method integrates diverse embedding mod-\\nels, performs dimensionality reduction, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='challenging but contextually irrelevant docu-\\nments to enhance deployed re-ranking models.\\nOur method integrates diverse embedding mod-\\nels, performs dimensionality reduction, and\\nuniquely selects hard negatives, ensuring com-\\nputational efficiency and semantic precision.\\nEvaluation on our proprietary enterprise corpus\\n(cloud services domain) demonstrates substan-\\ntial improvements of 15% in MRR@3 and 19%\\nin MRR@10 compared to state-of-the-art base-\\nlines and other negative sampling techniques.\\nFurther validation on public domain-specific\\ndatasets (FiQA, Climate Fever, TechQA) con-\\nfirms our method’s generalizability and readi-\\nness for real-world applications.\\n1 Introduction\\nAccurate retrieval of domain-specific information\\nsignificantly impacts critical enterprise processes,\\nsuch as knowledge management, customer sup-\\nport, and Retrieval Augmented Generation (RAG)\\nAgents. However, achieving precise retrieval re-\\nmains challenging due to semantic mismatches,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='such as knowledge management, customer sup-\\nport, and Retrieval Augmented Generation (RAG)\\nAgents. However, achieving precise retrieval re-\\nmains challenging due to semantic mismatches,\\noverlapping terminologies, and ambiguous abbre-\\nviations common in specialized fields like finance,\\nand cloud computing. Traditional lexical retrieval\\ntechniques, such as BM25 (Robertson and Walker,\\n1994), struggle due to vocabulary mismatches, lead-\\ning to irrelevant results and poor user experience.\\n*The authors contributed equally to this work.\\nRecent dense retrieval approaches leveraging\\npre-trained language models, like BERT-based en-\\ncoders (Karpukhin et al., 2020; Xiong et al., 2020;\\nGuu et al., 2020), mitigate lexical limitations by\\ncapturing semantic relevance. Nevertheless, their\\nperformance heavily relies on the negative sam-\\nples—documents incorrectly retrieved due to se-\\nmantic similarity but lacking contextual relevance.\\nModels trained with negative sampling methods'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='performance heavily relies on the negative sam-\\nples—documents incorrectly retrieved due to se-\\nmantic similarity but lacking contextual relevance.\\nModels trained with negative sampling methods\\n(e.g., random sampling, BM25-based static sam-\\npling, or dynamic methods like ANCE (Xiong\\net al., 2020), STAR (Zhan et al., 2021)) either\\nlack sufficient semantic discrimination or incur\\nhigh computational costs, thus limiting scalability\\nand practical enterprise deployment. For instance,\\ngiven a query such as \"Steps to deploy a MySQL\\ndatabase on Cloud Infrastructure,\" most negative\\nsampling techniques select documents discussing\\nnon-MySQL database deployments. Conversely,\\nour method strategically selects a hard negative dis-\\ncussing MySQL deployment on-premises, which\\ndespite semantic overlap, is contextually distinct\\nand thus poses a stronger training challenge for the\\nretrieval and re-ranking models.\\nOur proposed framework addresses these by in-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='despite semantic overlap, is contextually distinct\\nand thus poses a stronger training challenge for the\\nretrieval and re-ranking models.\\nOur proposed framework addresses these by in-\\ntroducing a novel semantic selection criterion ex-\\nplicitly designed to curate high-quality hard nega-\\ntives. By uniquely formulating two semantic con-\\nditions that effectively select negatives that closely\\nresemble query semantics but remain contextually\\nirrelevant, significantly minimizing false negatives\\nencountered by existing techniques. The main con-\\ntributions of this paper are:\\n1. A negative mining framework for dynamically\\nselecting semantically challenging hard neg-\\natives, leveraging diverse embedding models\\nand semantic filtering criteria to significantly\\nimprove re-ranking models in domain-specific\\nretrieval scenarios.\\n2. Comprehensive evaluations demonstrating'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nconsistent and significant improvements\\nacross both proprietary and publicly available\\ndatasets, verifying our method’s impact and\\nbroad applicability across domain-specific\\nusecases.\\n3. In-depth analysis, of critical challenges in han-\\ndling both short and long-form enterprise doc-\\numents, laying a clear foundation for targeted\\nfuture improvements.\\nOur work directly enhances the semantic dis-\\ncrimination capabilities of re-ranking models, re-\\nsulting in 15% improvement in MRR@3 and\\n19% improvement in MRR@10 on our in-house\\ncloud-services domain dataset. Further evaluations\\non public domain-specific benchmarks (FiQA, Cli-\\nmate Fever, TechQA) confirm generalizability and\\ntangible improvements of our proposed negative\\nmining framework.\\n2 Related Work\\n2.1 Hard Negatives in Retrieval Models\\nThe role of hard negatives in training dense re-\\ntrieval models has been widely studied. Static\\nnegatives, such as BM25 (Robertson and Walker,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='2 Related Work\\n2.1 Hard Negatives in Retrieval Models\\nThe role of hard negatives in training dense re-\\ntrieval models has been widely studied. Static\\nnegatives, such as BM25 (Robertson and Walker,\\n1994), provide lexical similarity but fail to capture\\nsemantic relevance, often leading to overfitting (Qu\\net al., 2020). Dynamic negatives, introduced in\\nANCE (Xiong et al., 2020) and STAR (Zhan et al.,\\n2021), adapt during training to provide more chal-\\nlenging contrasts but require significant computa-\\ntional resources due to periodic re-indexing. Our\\nframework addresses these limitations by dynam-\\nically identifying semantically challenging nega-\\ntives using clustering and dimensionality reduction,\\nensuring scalability and adaptability.\\nFurther studies have explored advanced meth-\\nods for negative sampling in cross-encoder mod-\\nels (Meghwani, 2024). Localized Contrastive Es-\\ntimation (LCE) (Guo et al., 2023) integrates hard\\nnegatives into cross-encoder training, improving'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='ods for negative sampling in cross-encoder mod-\\nels (Meghwani, 2024). Localized Contrastive Es-\\ntimation (LCE) (Guo et al., 2023) integrates hard\\nnegatives into cross-encoder training, improving\\nthe reranking performance when negatives align\\nwith the output of the retriever. Similarly, (Pradeep\\net al., 2022) demonstrated the importance of hard\\nnegatives even when models undergo advanced pre-\\ntraining techniques, such as condenser (Gao and\\nCallan, 2021). Our work builds on these efforts by\\noffering a scalable approach, which can be applied\\nto any domain-heavy enterprise data.\\n2.2 Negative Sampling Strategies\\nEffective negative sampling significantly affects the\\nperformance of the retrieval model by challenging\\nthe model to differentiate between relevant and\\nirrelevant examples. Common strategies include:\\n• Random Negatives: Efficient but lacking se-\\nmantic contrast, leading to suboptimal perfor-\\nmance (Karpukhin et al., 2020).\\n• BM25 Negatives: Leverage lexical similar-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='• Random Negatives: Efficient but lacking se-\\nmantic contrast, leading to suboptimal perfor-\\nmance (Karpukhin et al., 2020).\\n• BM25 Negatives: Leverage lexical similar-\\nity, but often introduce biases, particularly\\nin semantically rich domains (Robertson and\\nWalker, 1994).\\n• In-Batch Negatives: Computationally ef-\\nficient but limited to local semantic con-\\ntrasts, often underperforming in dense re-\\ntrieval tasks (Xiong et al., 2020).\\nOur framework complements these approaches\\nby dynamically generating negatives that balance\\nsemantic similarity and contextual irrelevance,\\navoiding the pitfalls of static or random methods.\\n2.3 Domain-Specific Retrieval Challenges\\nEnterprise retrieval systems face unique challenges,\\nsuch as ambiguous terminology, overlapping con-\\ncepts, and private datasets (Meghwani, 2024).\\nGeneral-purpose methods such as BM25 or dense\\nretrieval models (Qu et al., 2020) fail to capture\\ndomain-specific complexities effectively. Our ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='cepts, and private datasets (Meghwani, 2024).\\nGeneral-purpose methods such as BM25 or dense\\nretrieval models (Qu et al., 2020) fail to capture\\ndomain-specific complexities effectively. Our ap-\\nproach addresses these gaps by curating hard nega-\\ntives that align with enterprise-specific semantics,\\nimproving retrieval precision and robustness for\\nproprietary datasets.\\nWe further discuss negative sampling techniques in\\nAppendix A.1.\\n3 Methodology\\nTo effectively train and finetune reranker models\\nfor domain-specific retrieval, it is essential to sys-\\ntematically handle technical ambiguities stemming\\nfrom specialized terminologies, overlapping con-\\ncepts, and abbreviations prevalent within enterprise\\ndomains.\\nWe propose a structured, modular framework\\nthat integrates diverse embedding models, dimen-\\nsionality reduction, and a novel semantic criterion\\nfor hard-negative selection. Figure 1 illustrates the\\nhigh-level pipeline, components and their interac-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='sionality reduction, and a novel semantic criterion\\nfor hard-negative selection. Figure 1 illustrates the\\nhigh-level pipeline, components and their interac-\\ntions. The re-ranking models fine-tuned using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nFigure 1: Overview of the methodology pipeline for training reranker models, including embedding generation,\\nPCA-based dimensionality reduction and hard negative selection for fine-tuning.\\nhard negatives generated by our framework are di-\\nrectly deployed in downstream applications, such\\nas RAG, significantly improving the resolution of\\ncustomer queries through enhanced retrieval.\\nOur approach begins by encoding queries and\\ndocuments into semantically rich vector represen-\\ntations using an ensemble of state-of-the-art bi-\\nencoder embedding models. These embeddings are\\nstrategically selected based on multilingual sup-\\nport, embedding quality, training data diversity,\\ncontext length handling, and performance (details\\nprovided in Appendix A.2. To manage embed-\\nding dimensionality and improve computational\\nefficiency, Principal Component Analysis (PCA)\\n(Ma´ckiewicz and Ratajczak, 1993) is utilized to\\nproject the concatenated embeddings onto a lower-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='ding dimensionality and improve computational\\nefficiency, Principal Component Analysis (PCA)\\n(Ma´ckiewicz and Ratajczak, 1993) is utilized to\\nproject the concatenated embeddings onto a lower-\\ndimensional space, maintaining 95% of the original\\nvariance.\\nWe then define two semantic conditions (Eq. 5\\nand Eq. 6) to dynamically select high-quality hard\\nnegatives, addressing semantic similarity chal-\\nlenges and minimizing false negatives. Together,\\nthese two equations ensure that the selected hard\\nnegative is not only close to the query (Eq. 5) but\\nalso contextually distinct from the true positive,\\nminimizing the risk of selecting topic duplicates\\nor noisy positives (Eq. 6). For example, a query\\nabout deploying MySQL on Oracle Cloud, PD is a\\nguide on that topic, and D is a doc about MySQL\\non-premise — semantically close to Q, but distant\\nfrom PD.\\nBelow we detail each methodological compo-\\nnent, emphasizing their contributions to enhancing\\nretrieval precision in domain-specific or enterprise'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='from PD.\\nBelow we detail each methodological compo-\\nnent, emphasizing their contributions to enhancing\\nretrieval precision in domain-specific or enterprise\\nretrieval tasks.\\nTotal Train Test\\n< Q, P D > 5250 1000 4250\\nTable 1: Dataset distribution of queries (Q) and positive\\ndocuments (PD).\\n3.1 Dataset Statistics\\nOur experiments leverage a proprietary corpus con-\\ntaining 36,871 unannotated documents sourced\\nfrom over 30 enterprise cloud services. Addition-\\nally, we prepared 5250 annotated query-positive\\ndocument pairs ( < Q, P D > ) for training and\\ntesting. Notably, we adopted a non-standard train-\\ntest split (as summarized in Table 1), allocating\\nfour times more data to testing than training to\\nrigorously evaluate model robustness against vary-\\ning training data volumes (additional analyses in\\nAppendix A.4). To further validate generaliz-\\nability, we conduct evaluations on publicly avail-\\nable domain-specific benchmarks: FiQA (finance)\\n(TheFinAI, 2018), Climate Fever (climate science)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Appendix A.4). To further validate generaliz-\\nability, we conduct evaluations on publicly avail-\\nable domain-specific benchmarks: FiQA (finance)\\n(TheFinAI, 2018), Climate Fever (climate science)\\n(Diggelmann et al., 2021), and TechQA (technol-\\nogy) (Castelli et al., 2019). Detailed dataset statis-\\ntics are provided in Appendix A.2.1.\\n3.2 Embedding Generation\\nEmbeddings for queries, positive documents, and\\nthe corpus are computed via six diverse, high-\\nperformance bi-encoder models E1, E2, . . . , E6,\\neach selected strategically for capturing comple-\\nmentary semantic perspectives:\\nEk(x) ∈ Rdk (1)\\nwhere dk is the embedding dimension of the kth\\nmodel for textual input x. Concatenation of these'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nembeddings yields a comprehensive representation:\\nXconcat = [e1(x); e2(x); . . . ; e6(x)] (2)\\nwhere Xconcat ∈ R\\nP6\\nk=1 dk represents the con-\\ncatenated embedding for the input x.\\n3.3 Dimensionality Reduction\\nTo alleviate the computational overhead arising\\nfrom high-dimensional concatenated embeddings,\\nwe apply PCA to reduce dimensionality while pre-\\nserving semantic richness:\\nXPCA = XconcatP, (3)\\nwhere P represents the PCA projection matrix.\\nWe specifically select PCA due to its computational\\nefficiency, and scalability, essential given our large\\nenterprise corpus and high-dimensional embedding\\nspace. While we empirically evaluated nonlinear\\ndimensionality reduction methods such as UMAP\\n(McInnes et al., 2020) and t-SNE (Van der Maaten\\nand Hinton, 2008), they offered negligible perfor-\\nmance improvements over PCA but incurred sub-\\nstantially higher computational costs, making them\\nimpractical for deployment at scale in enterprise\\nsystems.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='mance improvements over PCA but incurred sub-\\nstantially higher computational costs, making them\\nimpractical for deployment at scale in enterprise\\nsystems.\\n3.4 Hard Negative Selection Criteria\\nWe propose two semantic criteria to identify high-\\nquality hard negatives. PCA-reduced embeddings\\nXPCA are organized around each queryQ. For each\\nquery-positive document pair (Q, P D), candidate\\ndocuments D from the corpus are evaluated via\\ncosine distances:\\nd(Q, P D), d (Q, D), d (P D, D) (4)\\nA document D is selected as a hard negative\\nonly if it satisfies both criteria:\\nd(Q, D) < d(Q, P D) (5)\\nd(Q, D) < d(P D, D) (6)\\nEquation (5) ensures that the candidate negative\\ndocument is semantically closer to the query than\\nthe actual positive document, making it a challeng-\\ning negative example that potentially confuses the\\nreranking model. Equation (6), ensures that the se-\\nlected hard negative is not just query-confusing but\\nalso sufficiently dissimilar from the actual positive'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='reranking model. Equation (6), ensures that the se-\\nlected hard negative is not just query-confusing but\\nalso sufficiently dissimilar from the actual positive\\n(avoiding near-duplicates or false negatives).\\nThe candidate document DHN with minimal\\nd(Q, D) satisfying these conditions is chosen as\\nthe primary hard negative. Additional hard nega-\\ntives can similarly be selected based on semantic\\nproximity rankings.\\nFigure 2: Hard negative selection on the first two PCA\\ncomponents (78% variance). Q act as centroids, P D\\nguide selection of hard negatives; which are chosen\\nbased on semantic proximity.\\nFigure 2 illustrates an example embedding space,\\nclearly depicting the query Q, positive document\\nP D, and selected hard negative DHN , visualizing\\nthe semantic selection criteria. In cases where no\\ndocuments satisfy these conditions, no hard nega-\\ntives are selected for that particular query. Further\\ndetails on our embedding model & fine-tuning us-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='documents satisfy these conditions, no hard nega-\\ntives are selected for that particular query. Further\\ndetails on our embedding model & fine-tuning us-\\ning these hard negatives are provided in Appendix\\nA.2.\\n4 Experiments & Results\\nTo evaluate the effectiveness of our proposed hard-\\nnegative selection framework, we conduct exten-\\nsive experiments on our internal cloud-specific en-\\nterprise dataset, as well as domain-specific open-\\nsource benchmarks. We systematically compare\\nour approach against multiple competitive negative\\nsampling methods and perform detailed ablation\\nstudies to understand the contribution of individual\\nframework components. Complete details on exper-\\nimental setups and hyperparameters are provided\\nin Appendix A.3.\\n4.1 Results & Discussion\\nComparative Analysis of Negative Sampling\\nStrategies Table 3 presents a detailed compar-\\nison of of our negative sampling technique against\\nseveral established methods, including Random,\\nBM25, In-batch, STAR, and ADORE+STAR. The'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nRe-ranker (Fine-tuned w/) Internal FiQA Climate-FEVER TechQA\\nMRR@3 MRR@10MRR@3 MRR@10MRR@3 MRR@10MRR@3 MRR@10\\nBaseline (No Fine-tuning) 0.42 0.45 0.45 0.48 0.44 0.46 0.57 0.61\\nIn-batch Negatives 0.47 0.52 0.46 0.52 0.44 0.47 0.57 0.62\\nSTAR 0.53 0.56 0.51 0.54 0.47 0.49 0.61 0.63\\nADORE+STAR 0.54 0.57 0.52 0.54 0.48 0.52 0.63 0.66\\nOur Proposed HN 0.57 0.64 0.54 0.56 0.52 0.55 0.65 0.69\\nTable 2: Comparative performance benchmarking of our in-house reranker across multiple domain-specific datasets.\\nThe reranker is fine-tuned (FT) with different negative sampling techniques, highlighting the effectiveness of our\\nproposed hard-negative mining method (HN).\\nNegative Sampling Method MRR@3 MRR@10\\nBaseline 0.42 0.45\\nFT with Random Neg 0.47 0.51\\nFT with BM25 Neg 0.49 0.54\\nFT with In-batch Neg 0.47 0.52\\nFT with BM25+In-batch Neg 0.52 0.54\\nFT with STAR 0.53 0.56\\nFT with ADORE+STAR 0.54 0.57\\nFT with our HN 0.57 0.64\\nTable 3: Comparison of negative sampling methods for'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='FT with In-batch Neg 0.47 0.52\\nFT with BM25+In-batch Neg 0.52 0.54\\nFT with STAR 0.53 0.56\\nFT with ADORE+STAR 0.54 0.57\\nFT with our HN 0.57 0.64\\nTable 3: Comparison of negative sampling methods for\\nfine-tuning(FT) in-house cross-encoder reranker model.\\nThe proposed framework achieves 15% and 19% im-\\nprovements in MRR@3 and MRR@10, respectively,\\nover baseline methods.\\nbaseline is defined as the performance of our inter-\\nnal reranker model without any fine-tuning. Our\\nmethod achieves notable relative improvements of\\n15% in MRR@3 and 19% in MRR@10 over this\\nbaseline. The semantic nature of our hard nega-\\ntives allows the reranker to distinguish contextually\\nirrelevant but semantically similar documents effec-\\ntively. In contrast, simpler baselines like Random\\nor BM25 negatives suffer due to no semantic con-\\nsideration, while advanced methods like STAR and\\nADORE+STAR occasionally miss subtle seman-\\ntic nuances that our formulated selection criteria\\naddress effectively.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='sideration, while advanced methods like STAR and\\nADORE+STAR occasionally miss subtle seman-\\ntic nuances that our formulated selection criteria\\naddress effectively.\\nGeneralization Across Open-source Models To\\nvalidate the robustness and versatility of our frame-\\nwork, we evaluated various open-source embed-\\nding and reranker models (Table 4), clearly demon-\\nstrating improvements across all models when fine-\\ntuned using our proposed negative sampling com-\\npared to ADORE+STAR and baseline (no fine-\\ntuning). Notably, rerankers with multilingual ca-\\npabilities, such as the BGE-Reranker and Jina\\nReranker, demonstrated pronounced improvements,\\nlikely benefiting from our embedding ensemble’s\\nmultilingual semantic richness. Similarly, larger\\nmodels like e5-mistral exhibit significant gains, re-\\nflecting their capacity to exploit nuanced semantic\\ndifferences provided by our negative samples. This\\nanalysis underscores the general applicability and\\nmodel-agnostic benefits of our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='flecting their capacity to exploit nuanced semantic\\ndifferences provided by our negative samples. This\\nanalysis underscores the general applicability and\\nmodel-agnostic benefits of our approach.\\nModel Baseline ADORE+STAR Ours\\nAlibaba-NLP\\n(gte-multilingual-reranker-base) 0.39 0.42 0.45\\nBGE-Reranker\\n(bge-reranker-large) 0.44 0.47 0.52\\nCohere Embed English Light\\n(Cohere-embed-english-light-v3.0) 0.32 0.34 0.38\\nCohere Embed Multilingual\\n(Cohere-embed-multilingual-v3.0) 0.34 0.37 0.40\\nCohere Reranker\\n(rerank-multilingual-v2.0) 0.42 0.45 0.49\\nIBM Reranker\\n(re2g-reranker-nq) 0.40 0.43 0.46\\nInfloat Reranker\\n(e5-mistral-7b-instruct) 0.35 0.38 0.42\\nJina Reranker v2\\n(jina-reranker-v2-base-multilingual) 0.45 0.480.53\\nMS-MARCO\\n(ms-marco-MiniLM-L-6-v2) 0.41 0.43 0.46\\nNomic AI Embed Text\\n(nomic-embed-text-v1.5) 0.33 0.36 0.39\\nNVIDIA\\nNV-Embed-v2 0.38 0.41 0.44\\nSalesforce\\nSFR-Embedding-2_R 0.37 0.40 0.43\\nSalesforce\\nSFR-Embedding-Mistral 0.36 0.39 0.42\\nT5-Large 0.41 0.44 0.47'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='(nomic-embed-text-v1.5) 0.33 0.36 0.39\\nNVIDIA\\nNV-Embed-v2 0.38 0.41 0.44\\nSalesforce\\nSFR-Embedding-2_R 0.37 0.40 0.43\\nSalesforce\\nSFR-Embedding-Mistral 0.36 0.39 0.42\\nT5-Large 0.41 0.44 0.47\\nTable 4: Performance benchmarking (MRR@3) of\\nreranker and embedding models using the proposed\\nhard negative selection framework, compared with\\nADORE+STAR and baseline methods.\\nEffectiveness on Domain-specific Public\\nDatasets We further tested our method’s\\nadaptability across diverse public domain-specific\\ndatasets (FiQA, Climate-FEVER, TechQA), as\\nshown in Table 2. Each dataset presents distinct\\nretrieval challenges, ranging from technical jargon\\nin TechQA to complex domain-specific reasoning\\nin Climate-FEVER. Fine-tuning with our generated\\nhard negatives consistently improved retrieval\\nacross these varied datasets. FiQA exhibited\\nsignificant gains, likely due to the semantic\\ndifferentiation required in finance-specific queries.\\nThese results demonstrate that our negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nsampling method is not only effective within our\\ninternal enterprise corpus but also valuable across\\ndiverse, domain-specific public datasets, indicating\\nbroad applicability and domain independence.\\nModel MRR@3 MRR@10\\nShort DocumentsBaseline 0.481 0.526\\nFT w/ proposed HN0.61 0.662\\nLong DocumentsBaseline 0.423 0.477\\nFT w/ proposed HN0.475 0.521\\nTable 5: Performance comparison of the in-house\\nreranker without fine-tuning (Baseline) versus fine-\\ntuned (FT) with our proposed hard negatives (HN), eval-\\nuated separately on short and long documents.\\nPerformance Analysis on Short vs. Long Docu-\\nments An explicit analysis of short versus long\\ndocuments (Table 5) revealed differential perfor-\\nmance gains. Short documents (under 1024 to-\\nkens) experienced substantial performance im-\\nprovements (MRR@3 improving from 0.481 to\\n0.61), attributed to minimal semantic redundancy\\nand tokenization constraints. Conversely, long\\ndocuments showed more moderate improvements'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='provements (MRR@3 improving from 0.481 to\\n0.61), attributed to minimal semantic redundancy\\nand tokenization constraints. Conversely, long\\ndocuments showed more moderate improvements\\n(MRR@3 from 0.423 to 0.475), primarily due to\\nembedding truncation that causes loss of context\\nand increased semantic complexity. Future re-\\nsearch should focus explicitly on developing hi-\\nerarchical or segment-based embedding methods\\nto address these limitations.\\nAblation Studies To clearly understand the im-\\npact of the individual components of the frame-\\nwork, we conducted systematic ablation studies\\n(Table 6). Training with positive documents alone\\nproduced only slight gains (+0.03 MRR@3), reaf-\\nfirming the critical role of high-quality hard nega-\\ntives. Evaluating individual embedding models sep-\\narately indicated varying performance due to their\\ndiffering semantic representations and underlying\\ntraining. However, the concatenation of diverse\\nembeddings provided significant performance im-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='arately indicated varying performance due to their\\ndiffering semantic representations and underlying\\ntraining. However, the concatenation of diverse\\nembeddings provided significant performance im-\\nprovements (+0.15 MRR@3), clearly highlighting\\nthe advantages of capturing semantic diversity.\\nAdditionally, PCA-based dimensionality reduc-\\ntion analysis identified the optimal variance thresh-\\nold at 95%. Lower thresholds resulted in marked\\nsemantic degradation, reducing retrieval perfor-\\nmance. This trade-off highlights PCA as an essen-\\ntial efficiency-enhancing step for the framework.\\nCollectively, these detailed analyses underscore\\nour method’s strengths, limitations, and method-\\nological rationale, providing clear empirical justifi-\\ncation for each design decision.\\n# Proposed Strategies MRR@3MRR@10\\n1 Baseline 0.42 0.45\\nPositive Document (PD) Only\\n2 Fine-tuning with PD Only 0.45 0.51\\nHard Negative(HN) with EmbeddingEk\\n3a HN withE1+ PD 0.45 0.51\\n3b HN withE2+ PD 0.47 0.53'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='1 Baseline 0.42 0.45\\nPositive Document (PD) Only\\n2 Fine-tuning with PD Only 0.45 0.51\\nHard Negative(HN) with EmbeddingEk\\n3a HN withE1+ PD 0.45 0.51\\n3b HN withE2+ PD 0.47 0.53\\n3c HN withE3+ PD 0.51 0.55\\n3d HN withE4+ PD 0.45 0.52\\n3e HN withE5+ PD 0.48 0.51\\n3f HN withE6+ PD 0.49 0.52\\n3g HN with Xconcat+ PD 0.57 0.64\\nXPCAVariance Impact+ PD\\n4a HN with XPCA(99% Variance)0.57 0.64\\n4b HN with XPCA(95% Variance)0.57 0.64\\n4c HN with XPCA(90% Variance)0.55 0.63\\n4d HN with XPCA(80% Variance)0.51 0.58\\n4e HN with XPCA(70% Variance)0.49 0.56\\nTable 6: Results of ablation study showing the impact\\nof embeddings, PCA variance thresholds, and positive\\ndocuments on MRR, on the in-house re-ranker model.\\n4.2 Case Studies: Examples of Hard Negative\\nImpact\\nFigure 3 shows how similar topics in the domain\\nof cloud computing. To demonstrate the qualitative\\nbenefits of the proposed framework, we present\\ntwo case studies where the baseline and fine-tuned\\nmodels produce different ranking results. These'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='of cloud computing. To demonstrate the qualitative\\nbenefits of the proposed framework, we present\\ntwo case studies where the baseline and fine-tuned\\nmodels produce different ranking results. These\\nexamples highlight the significance of hard neg-\\natives in distinguishing semantically similar but\\ncontextually irrelevant documents.\\nFigure 3: Illustrations of similar topics in the domain of\\nCloud Computing\\nCase Study 1: Disambiguating Technical\\nAcronyms.\\n• Query (Q): \"What is VCN in Cloud Infras-\\ntructure?\"'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\n• Positive Document (PD): A document ex-\\nplaining \"Virtual Cloud Network (VCN)\" in\\nCloud Infrastructure, detailing its setup and\\nusage.\\n• Hard Negative (HN):A document discussing\\n\"Virtual Network Interface Card (VNIC)\" in\\nthe context of networking hardware.\\nBaseline Result: The baseline model incorrectly\\nranks the hard negative above the positive docu-\\nment due to overlapping terms such as \"virtual\"\\nand \"network.\"\\nProposed Method Result: The fine-tuned model\\nranks the positive document higher, correctly iden-\\ntifying the contextual match between the query\\nand the description of VCN. This improvement\\nis attributed to the triplet loss training with hard\\nnegatives.\\nCase Study 2: Domain-Specific Terminology.\\n• Query (Q): \"How does the CI WAF handle\\nincoming traffic?\"\\n• Positive Document (PD): A document ex-\\nplaining the Web Application Firewall (W AF)\\nin CI, its configuration, and traffic filtering\\nmechanisms.\\n• Hard Negative (HN):A document discussing'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='• Positive Document (PD): A document ex-\\nplaining the Web Application Firewall (W AF)\\nin CI, its configuration, and traffic filtering\\nmechanisms.\\n• Hard Negative (HN):A document discussing\\ngeneral firewall configurations in networking.\\nBaseline Result: The baseline model ranks the\\nhard negative higher due to lexical overlap between\\nthe terms \"firewall\" and \"traffic.\"\\nProposed Method Result: The proposed frame-\\nwork ranks the positive document higher, leverag-\\ning domain-specific semantic representations.\\nThese case studies illustrate the practical ad-\\nvantages of training with hard negatives, espe-\\ncially in domains with overlapping terminology\\nor acronyms.\\nAdditional detailed analyses, illustrative prac-\\ntical implications for enterprise applications, and\\nexplicit future directions are discussed in detail in\\nA.4, and A.5.\\n5 Conclusion\\nWe introduced a scalable, modular framework lever-\\naging dynamic ensemble-based hard-negative min-\\ning to significantly enhance re-ranking models in'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='A.4, and A.5.\\n5 Conclusion\\nWe introduced a scalable, modular framework lever-\\naging dynamic ensemble-based hard-negative min-\\ning to significantly enhance re-ranking models in\\nenterprise and domain-specific retrieval scenarios.\\nOur method dynamically curates semantically chal-\\nlenging yet contextually irrelevant negatives, allow-\\ning re-ranking models to effectively discriminate\\nsubtle semantic differences. Empirical evaluations\\non proprietary enterprise data and diverse public\\ndomain-specific benchmarks demonstrated substan-\\ntial improvements of up to 15% in MRR@3 and\\n19% in MRR@10 over state-of-the-art negative\\nsampling techniques, including BM25, In-Batch\\nNegatives, STAR, and ADORE+STAR.\\nOur approach offers clear practical benefits in\\nreal-world deployments, benefiting downstream ap-\\nplications such as knowledge management, cus-\\ntomer support systems, and Retrieval-Augmented\\nGeneration (RAG), where retrieval precision di-\\nrectly influences user satisfaction and Generative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='plications such as knowledge management, cus-\\ntomer support systems, and Retrieval-Augmented\\nGeneration (RAG), where retrieval precision di-\\nrectly influences user satisfaction and Generative\\nAI effectiveness. The strong performance and gen-\\neralizability across various domains further under-\\nscore the framework’s readiness for industry-scale\\ndeployment.\\nFuture work will focus on extending our frame-\\nwork to handle incremental updates of enterprise\\nknowledge bases and exploring real-time negative\\nsampling strategies for continuously evolving cor-\\npora, further enhancing the adaptability and robust-\\nness required in practical industry settings.\\n6 Limitations\\nWhile our approach advances the state of hard\\nnegative mining and encoder-based retrieval, sev-\\neral limitations remain that open avenues for fu-\\nture research. One key challenge is the perfor-\\nmance disparity between short and long documents.\\nAddressing this requires more effective document'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='eral limitations remain that open avenues for fu-\\nture research. One key challenge is the perfor-\\nmance disparity between short and long documents.\\nAddressing this requires more effective document\\nchunking strategies and the development of hier-\\narchical representations to preserve context across\\nsegments. Additionally, the retrieval of long doc-\\numents is complicated by semantic redundancy\\nand truncation, warranting deeper analysis of their\\nstructural complexity. Our current use of embed-\\nding concatenation for ensembling could also be\\nrefined—future work should evaluate alternative\\nfusion techniques such as weighted averaging or\\nattention-based mechanisms. Moreover, extending\\nthe retrieval framework to support cross-lingual and\\nmultilingual scenarios would enhance its utility in\\nglobally distributed applications.\\nReferences\\nAMIT AGARWAL. 2021. Evaluate generalisation &\\nrobustness of visual features from images to video.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nResearchGate. Available at https://doi.org/10.\\n13140/RG.2.2.33887.53928.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2024a. Synthetic document generation pipeline for\\ntraining artificial intelligence models. US Patent App.\\n17/994,712.\\nAmit Agarwal, Srikant Panda, and Kulbhushan Pachauri.\\n2025. FS-DAG: Few shot domain adapting graph net-\\nworks for visually rich document understanding. In\\nProceedings of the 31st International Conference on\\nComputational Linguistics: Industry Track, pages\\n100–114, Abu Dhabi, UAE. Association for Com-\\nputational Linguistics.\\nAmit Agarwal, Hitesh Patel, Priyaranjan Pattnayak,\\nSrikant Panda, Bhargava Kumar, and Tejaswini Ku-\\nmar. 2024b. Enhancing document ai data genera-\\ntion through graph-based synthetic layouts. arXiv\\npreprint arXiv:2412.03590.\\nJina AI. 2023. jina-reranker-v2-base-multilingual.\\nArian Askari, Mohammad Aliannejadi, Evangelos\\nKanoulas, and Suzan Verberne. 2023. Generating'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2412.03590.\\nJina AI. 2023. jina-reranker-v2-base-multilingual.\\nArian Askari, Mohammad Aliannejadi, Evangelos\\nKanoulas, and Suzan Verberne. 2023. Generating\\nsynthetic documents for cross-encoder re-rankers: A\\ncomparative study of chatgpt and human experts.\\nJiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang,\\nXinnian Liang, Zhao Yan, and Zhoujun Li. 2023.\\nGriprank: Bridging the gap between retrieval and\\ngeneration via the generative knowledge improved\\npassage ranking. Preprint, arXiv:2305.18144.\\nVittorio Castelli, Rishav Chakravarti, Saswati Dana, An-\\nthony Ferritto, Radu Florian, Martin Franz, Dinesh\\nGarg, Dinesh Khandelwal, Scott McCarley, Mike\\nMcCawley, Mohamed Nasr, Lin Pan, Cezar Pen-\\ndus, John Pitrelli, Saurabh Pujar, Salim Roukos, An-\\ndrzej Sakrajda, Avirup Sil, Rosario Uceda-Sosa, Todd\\nWard, and Rong Zhang. 2019. The techqa dataset.\\nPreprint, arXiv:1911.02984.\\nCohere. 2023a. Cohere-embed-multilingual-v3.0.\\nAvailable at: https://cohere.com/blog/\\nintroducing-embed-v3.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Ward, and Rong Zhang. 2019. The techqa dataset.\\nPreprint, arXiv:1911.02984.\\nCohere. 2023a. Cohere-embed-multilingual-v3.0.\\nAvailable at: https://cohere.com/blog/\\nintroducing-embed-v3.\\nCohere. 2023b. Reranker model. Available\\nat: https://docs.cohere.com/v2/docs/\\nreranking-with-cohere.\\nGabriel de Souza P. Moreira, Radek Osmulski, Mengyao\\nXu, Ronay Ak, Benedikt Schifferer, and Even\\nOldridge. 2024. Nv-retriever: Improving text em-\\nbedding models with effective hard-negative mining.\\nPreprint, arXiv:2407.15831.\\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leip-\\npold. 2021. Climate-fever: A dataset for veri-\\nfication of real-world climate claims. Preprint,\\narXiv:2012.00614.\\nKaran Dua, Praneet Pabolu, and Mengqing Guo. 2024.\\nGenerating templates for use in synthetic document\\ngeneration processes. US Patent App. 18/295,765.\\nKaran Dua, Praneet Pabolu, and Ranjeet Kumar Gupta.\\n2025. Generation of synthetic doctor-patient conver-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='generation processes. US Patent App. 18/295,765.\\nKaran Dua, Praneet Pabolu, and Ranjeet Kumar Gupta.\\n2025. Generation of synthetic doctor-patient conver-\\nsations. US Patent App. 18/495,966.\\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\\nArivazhagan, and Wei Wang. 2022. Language-\\nagnostic bert sentence embedding. Preprint,\\narXiv:2007.01852.\\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\\ntraining architecture for dense retrieval. EMNLP\\n2021 - 2021 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings, pages\\n981–993.\\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\\nChowdhury, Ankita Naik, Pengshan Cai, and Al-\\nfio Gliozzo. 2022. Re2G: Retrieve, rerank, gen-\\nerate. In Proceedings of the 2022 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, pages 2701–2715, Seattle, United\\nStates. Association for Computational Linguistics.\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='for Computational Linguistics: Human Language\\nTechnologies, pages 2701–2715, Seattle, United\\nStates. Association for Computational Linguistics.\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\\nWu. 2023. How close is chatgpt to human experts?\\ncomparison corpus, evaluation, and detection.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training.\\nEK Jasila, N Saleena, and KA Abdul Nazeer. 2023. An\\nefficient document clustering approach for devising\\nsemantic clusters. Cybernetics and Systems, pages\\n1–18.\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen Tau Yih. 2020. Dense passage retrieval\\nfor open-domain question answering. EMNLP\\n2020 - 2020 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings of the\\nConference, pages 6769–6781.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='2020 - 2020 Conference on Empirical Methods in\\nNatural Language Processing, Proceedings of the\\nConference, pages 6769–6781.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan\\nRaiman, Mohammad Shoeybi, Bryan Catanzaro, and\\nWei Ping. 2024. Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models. arXiv\\npreprint arXiv:2405.17428.\\nFulu Li, Zhiwen Xie, and Guangyou Zhou. 2024.\\nTheme-enhanced hard negative sample mining for\\nopen-domain question answering. In ICASSP 2024 -\\n2024 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages\\n12436–12440.\\nXianming Li and Jing Li. 2023. Angle-optimized text\\nembeddings. arXiv preprint arXiv:2309.12871.\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih\\nYavuz, Caiming Xiong, and Philip S. Yu. 2021.\\nDense hierarchical retrieval for open-domain ques-\\ntion answering. In Conference on Empirical\\nMethods in Natural Language Processing.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nSean MacAvaney, Andrew Yates, Arman Cohan,\\nand Nazli Goharian. 2019. Cedr: Contextual-\\nized embeddings for document ranking. SIGIR\\n2019 - Proceedings of the 42nd International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1101–1104.\\nAndrzej Ma´ckiewicz and Waldemar Ratajczak. 1993.\\nPrincipal components analysis (pca). Computers &\\nGeosciences, 19(3):303–342.\\nLeland McInnes, John Healy, and James Melville.\\n2020. Umap: Uniform manifold approximation\\nand projection for dimension reduction. Preprint,\\narXiv:1802.03426.\\nHansa Meghwani. 2024. Enhancing retrieval perfor-\\nmance: An ensemble approach for hard negative min-\\ning. Preprint, arXiv:2411.02404.\\nVivek Mehta, Mohit Agarwal, and Rohit Kumar Kaliyar.\\n2024. A comprehensive and analytical review of text\\nclustering techniques. International Journal of Data\\nScience and Analytics, pages 1–20.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024a. Sfr-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='clustering techniques. International Journal of Data\\nScience and Analytics, pages 1–20.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024a. Sfr-\\nembedding-2: Advanced text embedding with multi-\\nstage training.\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\\nXiong, Yingbo Zhou, and Semih Yavuz. 2024b. Sfr-\\nembedding-mistral: Enhance text retrieval with trans-\\nfer learning. Salesforce AI Research Blog.\\nThanh-Do Nguyen, Chi Minh Bui, Thi-Hai-Yen Vuong,\\nand Xuan-Hieu Phan. 2022. Passage-based bm25\\nhard negatives: A simple and effective negative sam-\\npling strategy for dense retrieval.\\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\\nre-ranking with bert.\\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\\nJimmy Lin. 2019. Multi-stage document ranking\\nwith bert.\\nZach Nussbaum, John X. Morris, Brandon Duderstadt,\\nand Andriy Mulyar. 2024. Nomic embed: Training\\na reproducible long context text embedder. Preprint,\\narXiv:2402.01613.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='with bert.\\nZach Nussbaum, John X. Morris, Brandon Duderstadt,\\nand Andriy Mulyar. 2024. Nomic embed: Training\\na reproducible long context text embedder. Preprint,\\narXiv:2402.01613.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024a. Multi-lingual natural language generation.\\nUS Patent App. 18/318,315.\\nPraneet Pabolu, Karan Dua, and Sriram Chaudhury.\\n2024b. Multi-lingual natural language generation.\\nUS Patent App. 18/318,327.\\nSrikant Panda, Amit Agarwal, Gouttham Nambirajan,\\nand Kulbhushan Pachauri. 2025a. Out of distribution\\nelement detection for information extraction. US\\nPatent App. 18/347,983.\\nSrikant Panda, Amit Agarwal, and Kulbhushan Pachauri.\\n2025b. Techniques of information extraction for se-\\nlection marks. US Patent App. 18/240,344.\\nHitesh Laxmichand Patel, Amit Agarwal, Arion Das,\\nBhargava Kumar, Srikant Panda, Priyaranjan Pat-\\ntnayak, Taki Hasan Rafi, Tejaswini Kumar, and Dong-\\nKyu Chae. 2025. Sweeval: Do llms really swear?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Hitesh Laxmichand Patel, Amit Agarwal, Arion Das,\\nBhargava Kumar, Srikant Panda, Priyaranjan Pat-\\ntnayak, Taki Hasan Rafi, Tejaswini Kumar, and Dong-\\nKyu Chae. 2025. Sweeval: Do llms really swear?\\na safety benchmark for testing limits for enterprise\\nuse. In Proceedings of the 2025 Conference of the\\nNations of the Americas Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies (V olume3: Industry Track), pages\\n558–582.\\nHitesh Laxmichand Patel, Amit Agarwal, Bhargava\\nKumar, Karan Gupta, and Priyaranjan Pattnayak.\\n2024. Llm for barcodes: Generating diverse syn-\\nthetic data for identity documents. arXiv preprint\\narXiv:2411.14962.\\nPriyaranjan Pattnayak, Amit Agarwal, Hansa Megh-\\nwani, Hitesh Laxmichand Patel, and Srikant Panda.\\n2025a. Hybrid ai for responsive multi-turn on-\\nline conversations with novel dynamic routing and\\nfeedback adaptation. In Proceedings of the 4th\\nInternational Workshop on Knowledge-Augmented'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='2025a. Hybrid ai for responsive multi-turn on-\\nline conversations with novel dynamic routing and\\nfeedback adaptation. In Proceedings of the 4th\\nInternational Workshop on Knowledge-Augmented\\nMethods for Natural Language Processing, pages\\n215–229.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, and\\nAmit Agarwal. 2025b. Tokenization matters: Im-\\nproving zero-shot ner for indic languages. Preprint,\\narXiv:2504.16977.\\nPriyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit\\nAgarwal, Bhargava Kumar, Srikant Panda, and Te-\\njaswini Kumar. 2025c. Clinical qa 2.0: Multi-task\\nlearning for answer extraction and categorization.\\nPreprint, arXiv:2502.13108.\\nRonak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, An-\\ndrew Yates, and Jimmy Lin. 2022. Squeezing wa-\\nter from a stone: A bag of tricks for further im-\\nproving cross-encoder effectiveness for reranking.\\nIn Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in Artificial Intelligence and\\nLecture Notes in Bioinformatics), volume 13185'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='In Lecture Notes in Computer Science (including\\nsubseries Lecture Notes in Artificial Intelligence and\\nLecture Notes in Bioinformatics), volume 13185\\nLNCS, pages 655–670. Springer Science and Busi-\\nness Media Deutschland GmbH.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\\nHaifeng Wang. 2020. Rocketqa: An optimized train-\\ning approach to dense passage retrieval for open-\\ndomain question answering.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\\nSentence embeddings using siamese bert-networks.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing.\\nS. E. Robertson and S. Walker. 1994. Some Simple\\nEffective Approximations to the 2-Poisson Model\\nfor Probabilistic Weighted Retrieval, pages 232–241.\\nSpringer London.\\nSaba Sturua, Isabelle Mohr, Mohammad Kalim Akram,\\nMichael Günther, Bo Wang, Markus Krimmel, Feng\\nWang, Georgios Mastrapas, Andreas Koukounas, An-\\ndreas Koukounas, Nan Wang, and Han Xiao. 2024.\\njina-embeddings-v3: Multilingual embeddings with\\ntask lora. Preprint, arXiv:2409.10173.\\nTheFinAI. 2018. Fiqa: A financial question answering\\ndataset. Available at Hugging Face.\\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\\nVisualizing data using t-sne. Journal of machine\\nlearning research, 9(11).\\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\\nRangan Majumder, and Furu Wei. 2023. Improving\\ntext embeddings with large language models. arXiv\\npreprint arXiv:2401.00368.\\nSvante Wold, Kim H. Esbensen, Kim H. Esbensen, Paul'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. 2023. Improving\\ntext embeddings with large language models. arXiv\\npreprint arXiv:2401.00368.\\nSvante Wold, Kim H. Esbensen, Kim H. Esbensen, Paul\\nGeladi, and Paul Geladi. 1987. Principal component\\nanalysis. Chemometrics and Intelligent Laboratory\\nSystems, 2:37–52.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding. Preprint,\\narXiv:2309.07597.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2020. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval.\\nZhen Yang, Zhou Shao, Yuxiao Dong, and Jie Tang.\\n2024. Trisampler: A better negative sampling princi-\\nple for dense retrieval. Preprint, arXiv:2402.11855.\\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\\nZhang, and Shaoping Ma. 2021. Optimizing dense\\nretrieval model training with hard negatives. SIGIR'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\\nZhang, and Shaoping Ma. 2021. Optimizing dense\\nretrieval model training with hard negatives. SIGIR\\n2021 - Proceedings of the 44th International ACM\\nSIGIR Conference on Research and Development in\\nInformation Retrieval, pages 1503–1512.\\nDun Zhang. 2024. stella-embedding-model-2024.\\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie,\\nZiqi Dai, Jialong Tang, Huan Lin, Baosong Yang,\\nPengjun Xie, Fei Huang, et al. 2024. mgte: General-\\nized long-context text representation and reranking\\nmodels for multilingual text retrieval. arXiv preprint\\narXiv:2407.19669.\\nA Appendix\\nA.1 Extended Related Work\\nHard Negatives in Retrieval Models Static and\\ndynamic hard negatives have been extensively stud-\\nied. Static negatives, such as those generated\\nby BM25 (Robertson and Walker, 1994) or Pas-\\nsageBM25 (Nguyen et al., 2022), provide challeng-\\ning lexical contrasts but risk overfitting due to their\\nfixed nature (Qu et al., 2020). Dynamic negatives,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='sageBM25 (Nguyen et al., 2022), provide challeng-\\ning lexical contrasts but risk overfitting due to their\\nfixed nature (Qu et al., 2020). Dynamic negatives,\\nas introduced in ANCE (Xiong et al., 2020) and\\nADORE (Zhan et al., 2021) adapt during training,\\nother effective methods like positive-aware mining\\n(de Souza P. Moreira et al., 2024), theme-enhanced\\nnegatives (Li et al., 2024) offers relevant chal-\\nlenges but incurring high computational costs due\\nto periodic re-indexing and bigger embedding di-\\nmension. Our framework mitigates these issues by\\nleveraging clustering and dimensionality reduction\\nto dynamically identify negatives without requiring\\nre-indexing.\\nLocalized Contrastive Estimation (LCE) (Guo\\net al., 2023; AGARWAL, 2021) further demon-\\nstrated the effectiveness of incorporating hard nega-\\ntives into cross-encoder training, improving rerank-\\ning accuracy when negatives align with retriever\\noutputs. Additionally, (Pradeep et al., 2022) high-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='tives into cross-encoder training, improving rerank-\\ning accuracy when negatives align with retriever\\noutputs. Additionally, (Pradeep et al., 2022) high-\\nlighted the importance of hard negatives even in\\nadvanced pretraining setups like Condenser (Gao\\nand Callan, 2021), which emphasizes their neces-\\nsity for robust optimization.\\nAdvances in Dense Retrieval and Cross-\\nEncoders Dense retrieval models like\\nDPR (Karpukhin et al., 2020) and REALM (Guu\\net al., 2020) encode queries and documents into\\ndense embeddings, enabling semantic matching.\\nRecent advances in dense retrieval and ranking\\ninclude GripRank’s generative knowledge-driven\\npassage ranking (Bai et al., 2023), Dense\\nHierarchical Retrieval’s multi-stage framework\\nfor efficient question answering (Liu et al., 2021;\\nPattnayak et al., 2025a,c,b; Patel et al., 2025), and\\nTriSampler’s optimized negative sampling for\\ndense retrieval (Yang et al., 2024), collectively\\nenhancing retrieval performance.Cross-encoders,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Pattnayak et al., 2025a,c,b; Patel et al., 2025), and\\nTriSampler’s optimized negative sampling for\\ndense retrieval (Yang et al., 2024), collectively\\nenhancing retrieval performance.Cross-encoders,\\nsuch as monoBERT (Nogueira et al., 2019;\\nNogueira and Cho, 2019), further improve retrieval\\nprecision by jointly encoding query-document\\npairs but require high-quality training data,\\nparticularly challenging negatives (MacAvaney\\net al., 2019; Panda et al., 2025b). Techniques such'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nas synthetic data generation (Askari et al., 2023;\\nAgarwal et al., 2024a, 2025) augment training\\ndatasets but lack the realism and semantic depth\\nprovided by our hard negative mining approach.\\nDimensionality Reduction in IR Clustering\\nmethods have been used to group semantically\\nsimilar documents, improving retrieval efficiency\\nand training data organization (Mehta et al., 2024;\\nJasila et al., 2023; Dua et al., 2025; Panda et al.,\\n2025a). Dimensionality reduction techniques like\\nPCA (Wold et al., 1987) enhance scalability by re-\\nducing computational complexity. Our framework\\nuniquely combines these techniques to dynamically\\nidentify negatives that challenge retrieval models\\nin a scalable manner.\\nSynthetic Data in Retrieval Recent\\nwork (Askari et al., 2023; Agarwal et al.,\\n2024a,b; Patel et al., 2024; Dua et al., 2024; Pabolu\\net al., 2024a,b) has explored using large language\\nmodels to generate synthetic training data for'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='work (Askari et al., 2023; Agarwal et al.,\\n2024a,b; Patel et al., 2024; Dua et al., 2024; Pabolu\\net al., 2024a,b) has explored using large language\\nmodels to generate synthetic training data for\\nretrieval tasks. While effective in low-resource\\nsettings, synthetic data often struggles with factual\\ninaccuracies and domain-specific relevance. In\\ncontrast, our framework relies on real-world data\\nto curate semantically challenging negatives,\\nensuring high-quality training samples without\\nintroducing synthetic biases.\\nSummary of Contributions While previous\\nworks address various aspects of negative sampling,\\nhard negatives, and synthetic data, our approach\\nbridges the gap between static and dynamic strate-\\ngies. By dynamically curating negatives using clus-\\ntering and dimensionality reduction, we achieve\\na scalable and semantically precise methodology\\ntailored to domain-specific retrieval tasks.\\nA.2 Extended Methodology\\nA.2.1 Dataset Statistics\\nQueries Length Distribution In this section we'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='a scalable and semantically precise methodology\\ntailored to domain-specific retrieval tasks.\\nA.2 Extended Methodology\\nA.2.1 Dataset Statistics\\nQueries Length Distribution In this section we\\nanalyze the distribution of queries length in our\\nenterprise dataset. Figure 4 shows that the length\\nof queries ranges from 1 to 25 words, with some\\nqueries having very few words. This highlights that\\nuser queries can sometime be just 2-3 words about\\na topic, increasing the probability of retrieving doc-\\numents mentioning those topics or concepts which\\ncan be contextually different. Therefore, when\\nwe select hard negatives, it is crucial to consider\\nnot only the relationship between the query and\\ndocuments but also the relationship between the\\nFigure 4: Length Distribution of queries in the dataset.\\npositive document and other documents, ensuring a\\ncomparison with texts on similar topics and similar\\nlengths.\\nModel (Ek) Params (M) Dimension Max Tokens\\nstella_en_400M_v5 435 8192 8192'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='positive document and other documents, ensuring a\\ncomparison with texts on similar topics and similar\\nlengths.\\nModel (Ek) Params (M) Dimension Max Tokens\\nstella_en_400M_v5 435 8192 8192\\njina-embeddings-v3 572 1024 8194\\n(multilingual)\\nmxbai-embed-large-v1 335 1024 512\\nbge-large-en-v1.5 335 1024 512\\nLaBSE 471 768 256\\n(multilingual)\\nall-mpnet-base-v2 110 768 514\\n(multilingual)\\nTable 7: Embedding models used to construct Xconcat,\\ncombining diverse semantic representations for queries\\n(Q), positive documents (P D), and corpus documents\\n(D).\\nFigure 5: Shows document length distribution in Enter-\\nprise corpus.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nDocument Length Distribution As shown in\\nFigure 5 , document lengths are significantly longer\\nthan query lengths. This disparity in context length\\naffects the similarity scores, potentially reducing\\nthe accuracy of retrieval systems. In our in-house\\ndataset, each query is paired with a single correct\\ndocument (though its not limited by number of\\npositive-negative document per query). This posi-\\ntive document is crucial for identifying challenging\\nhard negatives and hence helpful for encoder-based\\nmodel training.\\nA.2.2 Embedding Models\\nTable 7 lists the embedding models (Zhang, 2024;\\nSturua et al., 2024; Li and Li, 2023; Xiao et al.,\\n2023; Feng et al., 2022; Reimers and Gurevych,\\n2019; Zhang et al., 2024) used to construct Xconcat,\\ncombining diverse semantic representations for\\nqueries ( Q), positive documents ( P D), and cor-\\npus documents (D). These models were selected\\nfor their performance, model size, ability to han-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='combining diverse semantic representations for\\nqueries ( Q), positive documents ( P D), and cor-\\npus documents (D). These models were selected\\nfor their performance, model size, ability to han-\\ndle multilingual context, providing complemen-\\ntary strengths in dimensionality and token cover-\\nage. By integrating embeddings from these models,\\nthe framework captures nuanced semantic relation-\\nships crucial for reranker training.\\nA.2.3 Unified Contrastive Loss\\nThe unified contrastive loss is designed to improve\\nranking precision for both bi-encoders and cross-\\nencoders, by ensuring that positive documents\\n(P D) are ranked closer to the query (Q) than hard\\nnegatives (DHN ) by a margin m. The loss is de-\\nfined as:\\nL =\\nNX\\ni=1\\nmax (0, m + d(Qi, P Di) − d(Qi, DHN i))\\n(7)\\nwhere:\\n• P Di: Positive document associated with\\nquery Qi.\\n• DHN i: Hard negative document, semantically\\nsimilar to P Di but contextually irrelevant.\\n• d(Qi, Di): Distance metric measuring rele-\\nvance between Qi and Di.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='query Qi.\\n• DHN i: Hard negative document, semantically\\nsimilar to P Di but contextually irrelevant.\\n• d(Qi, Di): Distance metric measuring rele-\\nvance between Qi and Di.\\n• m: Margin ensuring P Di is closer to Qi than\\nDHN i by at least m, encouraging the model\\nto distinguish between relevant and irrelevant\\ndocuments effectively.\\nFor bi-encoders, the distance metric is defined as:\\nd(Qi, Di) = 1 − cosine(eQi, eDi), (8)\\nwhere eQi and eDi are the embeddings of the query\\nand document, respectively. For cross-encoders,\\nthe distance metric is:\\nd(Qi, Di) = −s(Qi, Di), (9)\\nwhere s(Qi, Di) is the cross-encoder’s relevance\\nscore for the query-document pair.\\nThis formulation leverages the triplet of (Q, P D,\\nDHN ) to minimize d(Qi, P Di), pulling positive\\ndocuments closer to the query, while maximizing\\nd(Qi, DHN i), pushing hard negatives further away.\\nBy emphasizing semantically challenging exam-\\nples, the model learns sharper decision boundaries\\nfor improved ranking precision.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='d(Qi, DHN i), pushing hard negatives further away.\\nBy emphasizing semantically challenging exam-\\nples, the model learns sharper decision boundaries\\nfor improved ranking precision.\\nA.3 Experimental Setup\\nDatasets We evaluate our framework extensively\\nusing both proprietary and public datasets:\\n• Internal Proprietary Dataset: Consisting\\nof approximately 5250 query-document pairs,\\non cloud services like computing, networking,\\nfirewall, ai services. It includes both short (<\\n[1024 tokens]) and long documents (>=[1024\\ntokens]).\\n• FiQA Dataset: A financial domain-specific\\ndataset widely used for retrieval benchmark-\\ning.\\n• Climate-FEVER Dataset: An environment-\\nspecific fact-checking dataset focused on\\nclimate-related information retrieval.\\n• TechQA Dataset: A technical question-\\nanswering dataset emphasizing software engi-\\nneering and technology-related queries.\\nTraining and Fine-tuning All re-ranking mod-\\nels are fine-tuned using a triplet loss with margin'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='answering dataset emphasizing software engi-\\nneering and technology-related queries.\\nTraining and Fine-tuning All re-ranking mod-\\nels are fine-tuned using a triplet loss with margin\\nwith same hyper-parameters. Early stopping is em-\\nployed based on validation MRR@10 scores to\\nprevent overfitting.\\nEvaluation Metrics Model performance is eval-\\nuated using standard retrieval metrics: Mean Recip-\\nrocal Rank (MRR) at positions 3 and 10 (MRR@3\\nand MRR@10), which measure retrieval quality\\nand ranking precision. Each reported metric is\\naveraged across three experimental runs for robust-\\nness.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nStrategy Training Data MRR@3 MRR@10\\nBaseline 0 0.42 0.45\\nFinetuned withHard Negatives\\n(Ours)\\n100 0.46 0.49\\n200 0.48 0.51\\n300 0.50 0.53\\n400 0.52 0.56\\n500 0.52 0.58\\n600 0.54 0.60\\n700 0.54 0.62\\n800 0.56 0.63\\n900 0.57 0.64\\n1000 0.57 0.64\\nTable 8: Comparison of Strategies with Varying Train-\\ning Data Sizes\\nA.4 Extended Results & Ablation\\nImpact of Training Data Size As shown in Ta-\\nble 8, both MRR@3 and MRR@10 improve as the\\ntraining data size increases, with more pronounced\\ngains in MRR@10. MRR@3 shows gradual im-\\nprovement, from 0.42 at the baseline to 0.57 with\\n100 examples, highlighting the model’s enhanced\\nability to rank relevant documents within the top 3.\\nMRR@10, on the other hand, shows more signif-\\nicant improvement, from 0.45 to 0.64, indicating\\nthat the model benefits more from additional data\\nwhen considering the top 10 ranked documents.\\nOur method shows promising results even with\\nsmaller training sets, demonstrating the effective-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='that the model benefits more from additional data\\nwhen considering the top 10 ranked documents.\\nOur method shows promising results even with\\nsmaller training sets, demonstrating the effective-\\nness of incorporating hard negatives early in the\\ntraining process. This suggests that hard negatives\\nsignificantly enhance the model’s ability to distin-\\nguish relevant from irrelevant documents against\\na given query, even when data is limited. This ap-\\nproach is particularly beneficial in enterprise con-\\ntexts, where annotated data may be scarce, enabling\\nquicker improvements in domain-specific retrieval\\nperformance.\\nModels in the Study In our study we com-\\npared the performance of other finetuned re-ranker\\n(Glass et al., 2022; Wang et al., 2023; Raffel et al.,\\n2020) and embedding models (Zhang et al., 2024;\\nNussbaum et al., 2024) using hard negatives gen-\\nerated by our proposed framework in Table 4.\\nWe benchmarked the BGE-Reranker (Xiao et al.,\\n2023), NV-Embed (Lee et al., 2024) Salesforce-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Nussbaum et al., 2024) using hard negatives gen-\\nerated by our proposed framework in Table 4.\\nWe benchmarked the BGE-Reranker (Xiao et al.,\\n2023), NV-Embed (Lee et al., 2024) Salesforce-\\nSFR (Meng et al., 2024a,b) , jina-reranker (AI,\\n2023) and Cohere-Reranker (Cohere, 2023a,b),\\nA.4.1 Analysis of Long vs. Short Documents\\nTable 5 reveals a consistent disparity in MRR\\nscores between short and long documents, with\\nlong documents showing lower performance. Here,\\nwe analyze potential reasons and propose mitiga-\\ntion strategies.\\nChallenges with Long Documents.\\n• Semantic Redundancy: Long documents of-\\nten contain repetitive or tangential content,\\ndiluting their relevance to a specific query.\\n• Context Truncation: Fixed-length tokeniza-\\ntion (e.g., 512 or 1024 tokens) truncates long\\ndocuments, potentially discarding critical in-\\nformation.\\n• Query-to-Document Mismatch: Short\\nqueries may not provide sufficient context to\\nmatch the nuanced information spread across\\na lengthy document.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='formation.\\n• Query-to-Document Mismatch: Short\\nqueries may not provide sufficient context to\\nmatch the nuanced information spread across\\na lengthy document.\\nPotential Solutions.\\n• Chunk-Based Retrieval: Split long doc-\\numents into smaller, semantically coherent\\nchunks and rank them individually.\\n• Hierarchical Embeddings: Use hierarchical\\nmodels to aggregate sentence- or paragraph-\\nlevel embeddings for better context represen-\\ntation.\\n• Query Expansion: Enhance short queries\\nwith additional context using techniques like\\nquery rewriting or pseudo-relevance feedback.\\nThis analysis highlights the need for future work\\nto address the inherent challenges of ranking long\\ndocuments effectively.\\nA.5 Practical Implications for Enterprise\\nApplications\\nThe proposed framework has significant practical\\nimplications for enterprise information retrieval\\nsystems, particularly in retrieval-augmented gener-\\nation (RAG) pipelines.\\nImproved Ranking Precision. By training with'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='implications for enterprise information retrieval\\nsystems, particularly in retrieval-augmented gener-\\nation (RAG) pipelines.\\nImproved Ranking Precision. By training with\\nhard negatives, the model ensures that the most\\nrelevant documents are retrieved for each query.\\nThis is particularly critical for enterprise use cases\\nsuch as:\\n• Technical Support: Retrieving precise docu-\\nmentation for customer queries, reducing res-\\nolution times.\\n• Knowledge Management: Ensuring that em-\\nployees access the most relevant internal re-\\nsources quickly.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Hansa Meghwani; Amit Agarwal; Priyaranjan Pattnayak; Hitesh Laxmichand Patel; Srikant Panda', 'doi': 'https://doi.org/10.48550/arXiv.2505.18366', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.18366v1', 'source': '..\\\\data\\\\pdf\\\\2505.18366v1.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': '2505.18366v1.pdf', 'file_type': 'pdf'}, page_content='Accepted in ACL 2025\\nEnhanced Generative Quality. High-quality re-\\ntrieval directly improves the factual accuracy and\\ncoherence of outputs generated by large language\\nmodels in RAG pipelines. For example:\\n• Documentation Summarization: Sum-\\nmaries generated by models like GPT are\\nmore reliable when based on top-ranked, ac-\\ncurate sources.\\n• Customer Interaction: Chatbots generate\\nmore contextually relevant responses when\\nfed precise retrieved documents.\\nScalability and Adaptability. The framework’s\\nmodular design, including the use of diverse embed-\\ndings and clustering-based hard negative selection,\\nallows it to adapt to:\\n• Different industries (e.g., healthcare, finance,\\nmanufacturing).\\n• Multi-lingual or cross-lingual retrieval tasks.\\nThese practical implications underscore the ver-\\nsatility and enterprise readiness of the proposed\\nframework.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = ( z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = ( z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax( QK T\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax( QK T\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head 1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matricesW Q\\ni ∈ Rdmodel×dk, W K\\ni ∈ Rdmodel×dk, W V\\ni ∈ Rdmodel×dv\\nand W O ∈ Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0 , xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0 , xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndf f = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel)\\nP E(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10 −9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0 .1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0 .6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations , 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (V olume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12ebef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 119 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (119, 384)\n",
      "Adding 119 documents to vector store...\n",
      "Successfully added 119 documents to vector store\n",
      "Total documents in collection: 515\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b52d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f3dab4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1e4c494b7a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60c23bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_180a5cd0_86',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'subject': '',\n",
       "   'creationdate': '2023-08-03T00:07:29+00:00',\n",
       "   'trapped': '/False',\n",
       "   'content_length': 216,\n",
       "   'page_label': '3',\n",
       "   'page': 2,\n",
       "   'moddate': '2023-08-03T00:07:29+00:00',\n",
       "   'doc_index': 86,\n",
       "   'total_pages': 15,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'title': '',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'author': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.13995468616485596,\n",
       "  'distance': 0.860045313835144,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_5a4f7606_86',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'source_file': 'attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'author': '',\n",
       "   'moddate': '2023-08-03T00:07:29+00:00',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'creationdate': '2023-08-03T00:07:29+00:00',\n",
       "   'doc_index': 86,\n",
       "   'content_length': 216,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'trapped': '/False',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.13995468616485596,\n",
       "  'distance': 0.860045313835144,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_21d0a418_79',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'doc_index': 79,\n",
       "   'content_length': 216,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'creationdate': '2023-08-03T00:07:29+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'page': 2,\n",
       "   'keywords': '',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2023-08-03T00:07:29+00:00',\n",
       "   'title': '',\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'total_pages': 15},\n",
       "  'similarity_score': 0.13995468616485596,\n",
       "  'distance': 0.860045313835144,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21f9677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47569780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14c337ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"openai/gpt-oss-120b\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "226e3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: openai/gpt-oss-120b\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13c03fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_180a5cd0_86',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'doc_index': 86,\n",
       "   'content_length': 216,\n",
       "   'moddate': '2023-08-03T00:07:29+00:00',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'page_label': '3',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'creationdate': '2023-08-03T00:07:29+00:00',\n",
       "   'total_pages': 15,\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'trapped': '/False',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'page': 2,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'title': ''},\n",
       "  'similarity_score': 0.13995468616485596,\n",
       "  'distance': 0.860045313835144,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_5a4f7606_86',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'moddate': '2023-08-03T00:07:29+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2023-08-03T00:07:29+00:00',\n",
       "   'content_length': 216,\n",
       "   'subject': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'page': 2,\n",
       "   'doc_index': 86,\n",
       "   'trapped': '/False',\n",
       "   'title': '',\n",
       "   'page_label': '3',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'source_file': 'attention.pdf'},\n",
       "  'similarity_score': 0.13995468616485596,\n",
       "  'distance': 0.860045313835144,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_21d0a418_79',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'title': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'doc_index': 79,\n",
       "   'trapped': '/False',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'content_length': 216,\n",
       "   'keywords': '',\n",
       "   'total_pages': 15,\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'page': 2,\n",
       "   'source_file': 'attention.pdf',\n",
       "   'creationdate': '2023-08-03T00:07:29+00:00',\n",
       "   'page_label': '3',\n",
       "   'moddate': '2023-08-03T00:07:29+00:00',\n",
       "   'author': '',\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.13995468616485596,\n",
       "  'distance': 0.860045313835144,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07925ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"openai/gpt-oss-120b\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f825d991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 145.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism maps a **query** vector and a set of **key‑value** vectors to an output vector. It computes similarity scores between the query and each key, normalizes these scores into weights, and then returns a **weighted sum of the value vectors** using those weights. This allows the model to focus on the most relevant information for each query.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3754c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'The goal of reducing sequential computation'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 95.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"The goal of reducing sequential computation\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c7244cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is attention is all you need'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention functi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "Question: what is attention is all you need\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: “Attention Is All You Need” is the 2017 research paper that introduced the Transformer model. It shows that a neural network built solely on self‑attention mechanisms—without recurrent or convolutional layers—can effectively model sequences, achieving state‑of‑the‑art performance on tasks like machine translation.\n",
      "\n",
      "Citations:\n",
      "[1] attention.pdf (page 2)\n",
      "[2] attention.pdf (page 2)\n",
      "[3] attention.pdf (page 2)\n",
      "Summary: “Attention Is All You Need” (2017) introduced the Transformer, a neural architecture that relies exclusively on self‑attention mechanisms, eliminating the need for recurrent or convolutional layers. This design proved capable of modeling sequences effectively and achieved state‑of‑the‑art results on tasks such as machine translation.\n",
      "History: {'question': 'what is attention is all you need', 'answer': '“Attention\\u202fIs\\u202fAll\\u202fYou\\u202fNeed” is the 2017 research paper that introduced the Transformer model. It shows that a neural network built solely on self‑attention mechanisms—without recurrent or convolutional layers—can effectively model sequences, achieving state‑of‑the‑art performance on tasks like machine translation.', 'sources': [{'source': 'attention.pdf', 'page': 2, 'score': 0.13995468616485596, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.13995468616485596, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.13995468616485596, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}], 'summary': '“Attention\\u202fIs\\u202fAll\\u202fYou\\u202fNeed” (2017) introduced the Transformer, a neural architecture that relies exclusively on self‑attention mechanisms, eliminating the need for recurrent or convolutional layers. This design proved capable of modeling sequences effectively and achieved state‑of‑the‑art results on tasks such as machine translation.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG 1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
